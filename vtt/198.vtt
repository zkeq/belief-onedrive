WEBVTT

1
00:00:12.795 --> 00:00:14.391
Algorithms are everywhere.
算法无处不在。

2
00:00:15.931 --> 00:00:19.056
They sort and separate the winners from the losers.
他们把成功者和失败者区分开来。

3
00:00:19.839 --> 00:00:22.103
The winners get the job
成功者得到工作

4
00:00:22.127 --> 00:00:23.870
or a good credit card offer.
或是一个很好的信用卡优惠计划。

5
00:00:23.894 --> 00:00:26.545
The losers don't even get an interview
失败者甚至连面试机会都没有，

6
00:00:27.410 --> 00:00:29.187
or they pay more for insurance.
或者要为保险付更多的钱。

7
00:00:30.017 --> 00:00:33.566
We're being scored with secret formulas that we don't understand
我们被不理解的秘密公式打分，

8
00:00:34.495 --> 00:00:37.712
that often don't have systems of appeal.
却并没有上诉的渠道。

9
00:00:39.060 --> 00:00:40.356
That begs the question:
这引出了一个问题：

10
00:00:40.380 --> 00:00:43.293
What if the algorithms are wrong?
如果算法是错误的怎么办？

11
00:00:44.920 --> 00:00:46.960
To build an algorithm you need two things:
构建一个算法需要两个要素：

12
00:00:46.984 --> 00:00:48.965
you need data, what happened in the past,
需要数据，如过去发生的事情，

13
00:00:48.989 --> 00:00:50.550
and a definition of success,
和成功的定义，

14
00:00:50.574 --> 00:00:53.031
the thing you're looking for and often hoping for.
你正在寻找的，通常希望得到的东西。

15
00:00:53.055 --> 00:00:58.092
You train an algorithm by looking, figuring out.
你可以通过观察，理解来训练算法。

16
00:00:58.116 --> 00:01:01.535
The algorithm figures out what is associated with success.
这种算法能找出与成功相关的因素。

17
00:01:01.559 --> 00:01:04.022
What situation leads to success?
什么情况意味着成功？

18
00:01:04.701 --> 00:01:06.463
Actually, everyone uses algorithms.
其实，每个人都使用算法。

19
00:01:06.487 --> 00:01:09.205
They just don't formalize them in written code.
他们只是没有把它们写成书面代码。

20
00:01:09.229 --> 00:01:10.577
Let me give you an example.
举个例子。

21
00:01:10.601 --> 00:01:13.917
I use an algorithm every day to make a meal for my family.
我每天都用一种算法来 为我的家人做饭。

22
00:01:13.941 --> 00:01:15.417
The data I use
我使用的数据

23
00:01:16.214 --> 00:01:17.873
is the ingredients in my kitchen,
就是我厨房里的原料，

24
00:01:17.897 --> 00:01:19.424
the time I have,
我拥有的时间，

25
00:01:19.448 --> 00:01:20.681
the ambition I have,
我的热情，

26
00:01:20.705 --> 00:01:22.414
and I curate that data.
然后我整理了这些数据。

27
00:01:22.438 --> 00:01:26.689
I don't count those little packages of ramen noodles as food.
我不把那种小包拉面算作食物。

28
00:01:26.713 --> 00:01:28.582
(Laughter)
（笑声）

29
00:01:28.606 --> 00:01:30.451
My definition of success is:
我对成功的定义是：

30
00:01:30.475 --> 00:01:33.134
a meal is successful if my kids eat vegetables.
如果我的孩子们肯吃蔬菜， 这顿饭就是成功的。

31
00:01:34.001 --> 00:01:36.855
It's very different from if my youngest son were in charge.
这和我最小的儿子 负责做饭时的情况有所不同。

32
00:01:36.879 --> 00:01:39.667
He'd say success is if he gets to eat lots of Nutella.
他说，如果他能吃很多 Nutella巧克力榛子酱就是成功。

33
00:01:40.999 --> 00:01:43.225
But I get to choose success.
但我可以选择成功。

34
00:01:43.249 --> 00:01:45.956
I am in charge. My opinion matters.
我负责。我的意见就很重要。

35
00:01:45.980 --> 00:01:48.655
That's the first rule of algorithms.
这就是算法的第一个规则。

36
00:01:48.679 --> 00:01:51.859
Algorithms are opinions embedded in code.
算法是嵌入在代码中的观点。

37
00:01:53.382 --> 00:01:57.045
It's really different from what you think most people think of algorithms.
这和你认为大多数人对 算法的看法是不同的。

38
00:01:57.069 --> 00:02:01.573
They think algorithms are objective and true and scientific.
他们认为算法是客观、真实和科学的。

39
00:02:02.207 --> 00:02:03.906
That's a marketing trick.
那是一种营销技巧。

40
00:02:05.089 --> 00:02:07.214
It's also a marketing trick
这也是一种用算法来

41
00:02:07.238 --> 00:02:10.392
to intimidate you with algorithms,
恐吓你的营销手段，

42
00:02:10.416 --> 00:02:14.077
to make you trust and fear algorithms
为了让你信任和恐惧算法

43
00:02:14.101 --> 00:02:16.119
because you trust and fear mathematics.
因为你信任并害怕数学。

44
00:02:17.387 --> 00:02:22.217
A lot can go wrong when we put blind faith in big data.
当我们盲目信任大数据时， 很多人都可能犯错。

45
00:02:23.504 --> 00:02:26.877
This is Kiri Soares. She's a high school principal in Brooklyn.
这是凯丽·索尔斯。 她是布鲁克林的一名高中校长。

46
00:02:26.901 --> 00:02:29.487
In 2011, she told me her teachers were being scored
2011年，她告诉我， 她学校的老师们正在被一个复杂

47
00:02:29.511 --> 00:02:32.238
with a complex, secret algorithm
并且隐秘的算法进行打分，

48
00:02:32.262 --> 00:02:33.751
called the "value-added model."
这个算法被称为“增值模型"。

49
00:02:34.325 --> 00:02:37.417
I told her, "Well, figure out what the formula is, show it to me.
我告诉她，“先弄清楚这个 公式是什么，然后给我看看。

50
00:02:36.671 --> 00:02:38.682
I'm going to explain it to you."
我来给你解释一下。”

51
00:02:38.682 --> 00:02:40.631
She said, "Well, I tried to get the formula,
她说，“我寻求过这个公式，

52
00:02:40.631 --> 00:02:43.687
but my Department of Education contact told me it was math
但是教育部的负责人告诉我这是数学，

53
00:02:43.967 --> 00:02:45.513
and I wouldn't understand it."
给我我也看不懂。”

54
00:02:47.086 --> 00:02:48.424
It gets worse.
更糟的还在后面。

55
00:02:48.448 --> 00:02:51.978
The New York Post filed a Freedom of Information Act request,
纽约邮报提出了“信息自由法”的要求，

56
00:02:51.988 --> 00:02:54.975
got all the teachers' names and all their scores
来得到所有老师的名字与他们的分数，

57
00:02:54.985 --> 00:02:57.767
and they published them as an act of teacher-shaming.
并且他们以羞辱教师的方式 发表了这些数据。

58
00:02:58.904 --> 00:03:02.764
When I tried to get the formulas, the source code, through the same means,
当我试图用同样的方法来获取公式， 源代码的时候，

59
00:03:02.788 --> 00:03:04.937
I was told I couldn't.
我被告知我没有权力这么做。

60
00:03:04.961 --> 00:03:06.197
I was denied.
我被拒绝了。

61
00:03:06.221 --> 00:03:07.395
I later found out
后来我发现，

62
00:03:07.419 --> 00:03:10.285
that nobody in New York City had access to that formula.
纽约市压根儿没有人能接触到这个公式。

63
00:03:10.309 --> 00:03:11.614
No one understood it.
没有人能看懂。

64
00:03:13.749 --> 00:03:16.973
Then someone really smart got involved, Gary Rubinstein.
然后，一个非常聪明的人参与了， 加里·鲁宾斯坦。

65
00:03:16.997 --> 00:03:20.618
He found 665 teachers from that New York Post data
他从纽约邮报的数据中 找到了665名教师，

66
00:03:20.642 --> 00:03:22.508
that actually had two scores.
实际上他们有两个分数。

67
00:03:22.532 --> 00:03:24.413
That could happen if they were teaching
如果他们同时教七年级与八年级的数学，

68
00:03:24.437 --> 00:03:26.876
seventh grade math and eighth grade math.
就会得到两个评分。

69
00:03:26.900 --> 00:03:28.438
He decided to plot them.
他决定把这些数据绘成图表。

70
00:03:28.462 --> 00:03:30.455
Each dot represents a teacher.
每个点代表一个教师。

71
00:03:30.924 --> 00:03:33.303
(Laughter)
（笑声）

72
00:03:33.327 --> 00:03:34.848
What is that?
那是什么？

73
00:03:34.872 --> 00:03:36.149
(Laughter)
（笑声）

74
00:03:36.173 --> 00:03:39.619
That should never have been used for individual assessment.
它永远不应该被用于个人评估。

75
00:03:39.643 --> 00:03:41.569
It's almost a random number generator.
它几乎是一个随机数生成器。

76
00:03:41.593 --> 00:03:44.539
(Applause)
（掌声）

77
00:03:44.563 --> 00:03:45.725
But it was.
但它确实被使用了。

78
00:03:45.749 --> 00:03:46.925
This is Sarah Wysocki.
这是莎拉·维索斯基。

79
00:03:46.949 --> 00:03:49.124
She got fired, along with 205 other teachers,
她连同另外205名教师被解雇了，

80
00:03:49.148 --> 00:03:51.810
from the Washington, DC school district,
都是来自华盛顿特区的学区，

81
00:03:51.834 --> 00:03:54.743
even though she had great recommendations from her principal
尽管她的校长还有学生的

82
00:03:54.767 --> 00:03:56.195
and the parents of her kids.
父母都非常推荐她。

83
00:03:57.210 --> 00:03:59.242
I know what a lot of you guys are thinking,
我知道你们很多人在想什么，

84
00:03:58.792 --> 00:04:01.393
especially the data scientists, the AI experts here.
尤其是这里的数据科学家， 人工智能专家。

85
00:04:01.777 --> 00:04:06.003
You're thinking, "Well, I would never make an algorithm that inconsistent."
你在想，“我可永远不会做出 这样前后矛盾的算法。”

86
00:04:06.673 --> 00:04:08.356
But algorithms can go wrong,
但是算法可能会出错，

87
00:04:08.380 --> 00:04:12.978
even have deeply destructive effects with good intentions.
即使有良好的意图， 也会产生毁灭性的影响。

88
00:04:14.351 --> 00:04:16.730
And whereas an airplane that's designed badly
每个人都能看到一架设计的

89
00:04:16.754 --> 00:04:18.755
crashes to the earth and everyone sees it,
很糟糕的飞机会坠毁在地，

90
00:04:18.779 --> 00:04:20.629
an algorithm designed badly
而一个设计糟糕的算法

91
00:04:22.065 --> 00:04:25.930
can go on for a long time, silently wreaking havoc.
可以持续很长一段时间， 并无声地造成破坏。

92
00:04:27.568 --> 00:04:29.138
This is Roger Ailes.
这是罗杰·艾尔斯。

93
00:04:29.162 --> 00:04:31.162
(Laughter)
（笑声）

94
00:04:32.344 --> 00:04:34.732
He founded Fox News in 1996.
他在1996年创办了福克斯新闻。

95
00:04:35.256 --> 00:04:37.837
More than 20 women complained about sexual harassment.
公司有超过20多名女性曾抱怨过性骚扰。

96
00:04:37.861 --> 00:04:41.096
They said they weren't allowed to succeed at Fox News.
她们说她们不被允许在 福克斯新闻有所成就。

97
00:04:41.120 --> 00:04:43.640
He was ousted last year, but we've seen recently
他去年被赶下台，但我们最近看到

98
00:04:43.664 --> 00:04:46.334
that the problems have persisted.
问题依然存在。

99
00:04:47.474 --> 00:04:48.874
That begs the question:
这引出了一个问题：

100
00:04:48.898 --> 00:04:51.782
What should Fox News do to turn over another leaf?
福克斯新闻应该做些什么改变？

101
00:04:53.065 --> 00:04:56.106
Well, what if they replaced their hiring process
如果他们用机器学习算法

102
00:04:56.130 --> 00:04:57.784
with a machine-learning algorithm?
取代传统的招聘流程呢？

103
00:04:57.808 --> 00:04:59.403
That sounds good, right?
听起来不错，对吧？

104
00:04:59.427 --> 00:05:00.727
Think about it.
想想看。

105
00:05:00.751 --> 00:05:02.856
The data, what would the data be?
数据，这些数据到底是什么？

106
00:05:02.880 --> 00:05:07.827
A reasonable choice would be the last 21 years of applications to Fox News.
福克斯新闻在过去21年的申请函 是一个合理的选择。

107
00:05:07.851 --> 00:05:09.353
Reasonable.
很合理。

108
00:05:09.377 --> 00:05:11.315
What about the definition of success?
那么成功的定义呢？

109
00:05:11.741 --> 00:05:13.065
Reasonable choice would be,
合理的选择将是，

110
00:05:13.089 --> 00:05:14.867
well, who is successful at Fox News?
谁在福克斯新闻取得了成功？

111
00:05:14.891 --> 00:05:18.471
I guess someone who, say, stayed there for four years
我猜的是，比如在那里呆了四年，

112
00:05:18.495 --> 00:05:20.149
and was promoted at least once.
至少得到过一次晋升的人。

113
00:05:20.636 --> 00:05:22.197
Sounds reasonable.
听起来很合理。

114
00:05:22.221 --> 00:05:24.575
And then the algorithm would be trained.
然后这个算法将会被训练。

115
00:05:24.599 --> 00:05:28.476
It would be trained to look for people to learn what led to success,
它会被训练去向人们 学习是什么造就了成功，

116
00:05:29.039 --> 00:05:33.357
what kind of applications historically led to success
什么样的申请函在过去拥有

117
00:05:33.381 --> 00:05:34.675
by that definition.
这种成功的定义。

118
00:05:36.020 --> 00:05:37.795
Now think about what would happen
现在想想如果我们把它

119
00:05:37.819 --> 00:05:40.374
if we applied that to a current pool of applicants.
应用到目前的申请者中会发生什么。

120
00:05:40.939 --> 00:05:42.568
It would filter out women
它会过滤掉女性，

121
00:05:43.483 --> 00:05:47.413
because they do not look like people who were successful in the past.
因为她们看起来不像 在过去取得成功的人。

122
00:05:51.572 --> 00:05:54.109
Algorithms don't make things fair
算法不会让事情变得公平，

123
00:05:53.939 --> 00:05:56.587
if you just blithely, blindly apply algorithms.
如果你只是轻率地， 盲目地应用算法。

124
00:05:56.851 --> 00:05:58.333
They don't make things fair.
它们不会让事情变得公平。

125
00:05:58.357 --> 00:06:00.485
They repeat our past practices,
它们只是重复我们过去的做法，

126
00:06:00.509 --> 00:06:01.692
our patterns.
我们的规律。

127
00:06:01.716 --> 00:06:03.655
They automate the status quo.
它们使现状自动化。

128
00:06:04.538 --> 00:06:06.927
That would be great if we had a perfect world,
如果我们有一个 完美的世界那就太好了，

129
00:06:07.725 --> 00:06:09.037
but we don't.
但是我们没有。

130
00:06:08.991 --> 00:06:13.163
And I'll add that most companies don't have embarrassing lawsuits,
我还要补充一点， 大多数公司都没有令人尴尬的诉讼，

131
00:06:14.266 --> 00:06:16.854
but the data scientists in those companies
但是这些公司的数据科学家

132
00:06:16.878 --> 00:06:19.067
are told to follow the data,
被告知要跟随数据，

133
00:06:19.091 --> 00:06:21.234
to focus on accuracy.
关注它的准确性。

134
00:06:22.093 --> 00:06:23.474
Think about what that means.
想想这意味着什么。

135
00:06:23.498 --> 00:06:27.525
Because we all have bias, it means they could be codifying sexism
因为我们都有偏见， 这意味着他们可以编纂性别歧视

136
00:06:27.549 --> 00:06:29.385
or any other kind of bigotry.
或者任何其他的偏见。

137
00:06:31.308 --> 00:06:32.729
Thought experiment,
思维实验，

138
00:06:32.753 --> 00:06:34.262
because I like them:
因为我喜欢它们：

139
00:06:35.394 --> 00:06:38.369
an entirely segregated society --
一个完全隔离的社会——

140
00:06:40.067 --> 00:06:43.395
racially segregated, all towns, all neighborhoods
种族隔离存在于所有的城镇， 所有的社区，

141
00:06:43.419 --> 00:06:46.456
and where we send the police only to the minority neighborhoods
我们把警察只送到少数族裔的社区

142
00:06:46.480 --> 00:06:47.673
to look for crime.
去寻找犯罪。

143
00:06:48.271 --> 00:06:50.490
The arrest data would be very biased.
逮捕数据将会是十分有偏见的。

144
00:06:51.671 --> 00:06:54.246
What if, on top of that, we found the data scientists
除此之外，我们还会寻找数据科学家

145
00:06:54.270 --> 00:06:58.431
and paid the data scientists to predict where the next crime would occur?
并付钱给他们来预测 下一起犯罪会发生在哪里？

146
00:06:59.095 --> 00:07:00.582
Minority neighborhood.
少数族裔的社区。

147
00:07:01.105 --> 00:07:04.230
Or to predict who the next criminal would be?
或者预测下一个罪犯会是谁？

148
00:07:04.708 --> 00:07:06.103
A minority.
少数族裔。

149
00:07:07.769 --> 00:07:11.310
The data scientists would brag about how great and how accurate
这些数据科学家们 会吹嘘他们的模型有多好，

150
00:07:11.334 --> 00:07:12.631
their model would be,
多精确，

151
00:07:12.655 --> 00:07:13.954
and they'd be right.
当然他们是对的。

152
00:07:15.771 --> 00:07:20.386
Now, reality isn't that drastic, but we do have severe segregations
不过现实并没有那么极端， 但我们确实在许多城市里

153
00:07:20.410 --> 00:07:21.697
in many cities and towns,
有严重的种族隔离，

154
00:07:21.721 --> 00:07:23.614
and we have plenty of evidence
并且我们有大量的证据表明

155
00:07:23.638 --> 00:07:26.326
of biased policing and justice system data.
警察和司法系统的数据存有偏见。

156
00:07:27.452 --> 00:07:30.267
And we actually do predict hotspots,
而且我们确实预测过热点，

157
00:07:30.291 --> 00:07:31.821
places where crimes will occur.
那些犯罪会发生的地方。

158
00:07:32.221 --> 00:07:36.087
And we do predict, in fact, the individual criminality,
我们确实会预测个人犯罪，

159
00:07:36.111 --> 00:07:37.881
the criminality of individuals.
个人的犯罪行为。

160
00:07:38.792 --> 00:07:42.755
The news organization ProPublica recently looked into
新闻机构“人民 (ProPublica)”最近调查了，

161
00:07:42.779 --> 00:07:44.803
one of those "recidivism risk" algorithms,
其中一个称为

162
00:07:44.827 --> 00:07:45.990
as they're called,
“累犯风险”的算法。

163
00:07:45.240 --> 00:07:49.208
being used in Florida during sentencing by judges.
并在佛罗里达州的 宣判期间被法官采用。

164
00:07:50.231 --> 00:07:53.816
Bernard, on the left, the black man, was scored a 10 out of 10.
伯纳德，左边的那个黑人， 10分中得了满分。

165
00:07:54.999 --> 00:07:57.006
Dylan, on the right, 3 out of 10.
在右边的迪伦， 10分中得了3分。

166
00:07:56.980 --> 00:07:59.531
10 out of 10, high risk. 3 out of 10, low risk.
10分代表高风险。 3分代表低风险。

167
00:08:00.418 --> 00:08:02.803
They were both brought in for drug possession.
他们都因为持有毒品 而被带进了监狱。

168
00:08:02.827 --> 00:08:03.981
They both had records,
他们都有犯罪记录，

169
00:08:04.005 --> 00:08:06.811
but Dylan had a felony
但是迪伦有一个重罪

170
00:08:06.835 --> 00:08:08.011
but Bernard didn't.
但伯纳德没有。

171
00:08:09.638 --> 00:08:12.704
This matters, because the higher score you are,
这很重要，因为你的分数越高，

172
00:08:12.728 --> 00:08:16.201
the more likely you're being given a longer sentence.
你被判长期服刑的可能性就越大。

173
00:08:18.114 --> 00:08:19.408
What's going on?
到底发生了什么？

174
00:08:20.346 --> 00:08:21.678
Data laundering.
数据洗钱。

175
00:08:22.750 --> 00:08:27.177
It's a process by which technologists hide ugly truths
这是一个技术人员 把丑陋真相隐藏在

176
00:08:27.201 --> 00:08:29.022
inside black box algorithms
算法黑盒子中的过程，

177
00:08:28.902 --> 00:08:30.336
and call them objective;
并称之为客观；

178
00:08:31.140 --> 00:08:32.708
call them meritocratic.
称之为精英模式。

179
00:08:34.938 --> 00:08:37.323
When they're secret, important and destructive,
当它们是秘密的， 重要的并具有破坏性的，

180
00:08:37.347 --> 00:08:39.834
I've coined a term for these algorithms:
我为这些算法创造了一个术语：

181
00:08:39.858 --> 00:08:41.857
"weapons of math destruction."
“杀伤性数学武器”。

182
00:08:41.881 --> 00:08:43.445
(Laughter)
（笑声）

183
00:08:43.469 --> 00:08:46.523
(Applause)
（鼓掌）

184
00:08:46.547 --> 00:08:48.901
They're everywhere, and it's not a mistake.
它们无处不在，也不是一个错误。

185
00:08:49.515 --> 00:08:53.238
These are private companies building private algorithms
这些是私有公司为了私人目的

186
00:08:53.262 --> 00:08:54.654
for private ends.
建立的私有算法。

187
00:08:55.034 --> 00:08:58.248
Even the ones I talked about for teachers and the public police,
甚至是我谈到的老师 与公共警察使用的（算法），

188
00:08:58.272 --> 00:09:00.141
those were built by private companies
也都是由私人公司所打造的，

189
00:09:00.165 --> 00:09:02.396
and sold to the government institutions.
然后卖给政府机构。

190
00:09:02.420 --> 00:09:04.293
They call it their "secret sauce" --
他们称之为“秘密配方（来源）”——

191
00:09:04.317 --> 00:09:06.445
that's why they can't tell us about it.
这就是他们不能告诉我们的原因。

192
00:09:06.469 --> 00:09:08.689
It's also private power.
这也是私人权力。

193
00:09:09.744 --> 00:09:14.439
They are profiting for wielding the authority of the inscrutable.
他们利用神秘莫测的权威来获利。

194
00:09:16.934 --> 00:09:19.868
Now you might think, since all this stuff is private
你可能会想，既然所有这些都是私有的

195
00:09:19.892 --> 00:09:21.050
and there's competition,
而且会有竞争，

196
00:09:20.790 --> 00:09:23.320
maybe the free market will solve this problem.
也许自由市场会解决这个问题。

197
00:09:23.404 --> 00:09:24.653
It won't.
然而并不会。

198
00:09:24.677 --> 00:09:27.797
There's a lot of money to be made in unfairness.
在不公平的情况下， 有很多钱可以赚。

199
00:09:28.947 --> 00:09:32.316
Also, we're not economic rational agents.
而且，我们不是经济理性的代理人。

200
00:09:32.851 --> 00:09:34.143
We all are biased.
我们都是有偏见的。

201
00:09:34.780 --> 00:09:38.157
We're all racist and bigoted in ways that we wish we weren't,
我们都是固执的种族主义者， 虽然我们希望我们不是，

202
00:09:38.181 --> 00:09:40.200
in ways that we don't even know.
虽然我们甚至没有意识到。

203
00:09:41.172 --> 00:09:44.253
We know this, though, in aggregate,
总的来说，我们知道这一点，

204
00:09:44.277 --> 00:09:47.497
because sociologists have consistently demonstrated this
因为社会学家会一直通过这些实验

205
00:09:47.521 --> 00:09:49.186
with these experiments they build,
来证明这一点，

206
00:09:48.950 --> 00:09:51.782
where they send a bunch of applications to jobs out,
他们发送了大量的工作申请，

207
00:09:51.802 --> 00:09:54.303
equally qualified but some have white-sounding names
都是有同样资格的候选人， 有些用白人人名，

208
00:09:54.327 --> 00:09:56.033
and some have black-sounding names,
有些用黑人人名，

209
00:09:56.057 --> 00:09:58.751
and it's always disappointing, the results -- always.
然而结果总是令人失望的。

210
00:09:59.330 --> 00:10:01.101
So we are the ones that are biased,
所以我们是有偏见的，

211
00:10:00.971 --> 00:10:04.584
and we are injecting those biases into the algorithms
我们还通过选择收集到的数据

212
00:10:04.578 --> 00:10:06.390
by choosing what data to collect,
来把偏见注入到算法中，

213
00:10:06.414 --> 00:10:09.157
like I chose not to think about ramen noodles --
就像我不选择去想拉面一样——

214
00:10:09.181 --> 00:10:10.806
I decided it was irrelevant.
我自认为这无关紧要。

215
00:10:10.830 --> 00:10:16.514
But by trusting the data that's actually picking up on past practices
但是，通过信任那些 在过去的实践中获得的数据

216
00:10:16.538 --> 00:10:18.552
and by choosing the definition of success,
以及通过选择成功的定义，

217
00:10:18.576 --> 00:10:22.559
how can we expect the algorithms to emerge unscathed?
我们怎么能指望算法 会是毫无瑕疵的呢？

218
00:10:22.583 --> 00:10:24.939
We can't. We have to check them.
我们不能。我们必须检查。

219
00:10:25.985 --> 00:10:27.694
We have to check them for fairness.
我们必须检查它们是否公平。

220
00:10:27.718 --> 00:10:30.429
The good news is, we can check them for fairness.
好消息是，我们可以做到这一点。

221
00:10:30.453 --> 00:10:33.805
Algorithms can be interrogated,
算法是可以被审问的，

222
00:10:33.829 --> 00:10:35.863
and they will tell us the truth every time.
而且每次都能告诉我们真相。

223
00:10:35.887 --> 00:10:38.380
And we can fix them. We can make them better.
然后我们可以修复它们。 我们可以让他们变得更好。

224
00:10:38.404 --> 00:10:40.779
I call this an algorithmic audit,
我把它叫做算法审计，

225
00:10:40.803 --> 00:10:42.482
and I'll walk you through it.
接下来我会为你们解释。

226
00:10:42.506 --> 00:10:44.702
First, data integrity check.
首先，数据的完整性检查。

227
00:10:45.952 --> 00:10:48.609
For the recidivism risk algorithm I talked about,
对于刚才提到过的累犯风险算法，

228
00:10:49.402 --> 00:10:52.975
a data integrity check would mean we'd have to come to terms with the fact
数据的完整性检查将意味着 我们不得不接受这个事实，

229
00:10:52.999 --> 00:10:56.525
that in the US, whites and blacks smoke pot at the same rate
在美国，白人和黑人 吸毒的比例是一样的，

230
00:10:56.549 --> 00:10:59.034
but blacks are far more likely to be arrested --
但是黑人更有可能被逮捕——

231
00:10:59.058 --> 00:11:02.242
four or five times more likely, depending on the area.
取决于区域，可能性是白人的4到5倍。

232
00:11:03.137 --> 00:11:05.963
What is that bias looking like in other crime categories,
这种偏见在其他犯罪类别中 是什么样子的，

233
00:11:05.987 --> 00:11:07.438
and how do we account for it?
我们又该如何解释呢？

234
00:11:07.982 --> 00:11:11.021
Second, we should think about the definition of success,
其次，我们应该考虑成功的定义，

235
00:11:11.045 --> 00:11:12.426
audit that.
审计它。

236
00:11:12.450 --> 00:11:15.202
Remember -- with the hiring algorithm? We talked about it.
还记得我们谈论的雇佣算法吗？

237
00:11:15.226 --> 00:11:18.391
Someone who stays for four years and is promoted once?
那个呆了四年的人， 然后被提升了一次？

238
00:11:18.415 --> 00:11:20.184
Well, that is a successful employee,
这的确是一个成功的员工，

239
00:11:19.704 --> 00:11:23.287
but it's also an employee that is supported by their culture.
但这也是一名受到公司文化支持的员工。

240
00:11:23.909 --> 00:11:25.835
That said, also it can be quite biased.
也就是说， 这可能会有很大的偏差。

241
00:11:25.859 --> 00:11:27.924
We need to separate those two things.
我们需要把这两件事分开。

242
00:11:27.948 --> 00:11:30.374
We should look to the blind orchestra audition
我们应该去看一下乐团盲选试奏，

243
00:11:30.398 --> 00:11:31.594
as an example.
举个例子。

244
00:11:31.618 --> 00:11:34.374
That's where the people auditioning are behind a sheet.
这就是人们在幕后选拔乐手的地方。

245
00:11:34.766 --> 00:11:36.697
What I want to think about there
我想要考虑的是

246
00:11:36.721 --> 00:11:40.138
is the people who are listening have decided what's important
倾听的人已经 决定了什么是重要的，

247
00:11:40.162 --> 00:11:42.191
and they've decided what's not important,
同时他们已经决定了 什么是不重要的，

248
00:11:41.961 --> 00:11:44.274
and they're not getting distracted by that.
他们也不会因此而分心。

249
00:11:44.781 --> 00:11:47.530
When the blind orchestra auditions started,
当乐团盲选开始时，

250
00:11:47.554 --> 00:11:50.998
the number of women in orchestras went up by a factor of five.
在管弦乐队中， 女性的数量上升了5倍。

251
00:11:52.073 --> 00:11:54.088
Next, we have to consider accuracy.
其次，我们必须考虑准确性。

252
00:11:55.053 --> 00:11:58.787
This is where the value-added model for teachers would fail immediately.
这就是针对教师的增值模型 立刻失效的地方。

253
00:11:59.398 --> 00:12:01.560
No algorithm is perfect, of course,
当然，没有一个算法是完美的，

254
00:12:02.440 --> 00:12:06.045
so we have to consider the errors of every algorithm.
所以我们要考虑每一个算法的误差。

255
00:12:06.656 --> 00:12:11.015
How often are there errors, and for whom does this model fail?
出现错误的频率有多高， 让这个模型失败的对象是谁？

256
00:12:11.670 --> 00:12:13.388
What is the cost of that failure?
失败的代价是什么？

257
00:12:14.254 --> 00:12:16.461
And finally, we have to consider
最后，我们必须考虑

258
00:12:17.793 --> 00:12:19.979
the long-term effects of algorithms,
这个算法的长期效果，

259
00:12:20.686 --> 00:12:22.893
the feedback loops that are engendering.
与正在产生的反馈循环。

260
00:12:23.406 --> 00:12:24.642
That sounds abstract,
这听起来很抽象，

261
00:12:24.666 --> 00:12:27.330
but imagine if Facebook engineers had considered that
但是想象一下 如果脸书的工程师们之前考虑过，

262
00:12:28.090 --> 00:12:32.945
before they decided to show us only things that our friends had posted.
并决定只向我们展示 我们朋友所发布的东西。

263
00:12:33.581 --> 00:12:36.815
I have two more messages, one for the data scientists out there.
我还有两条建议， 一条是给数据科学家的。

264
00:12:37.270 --> 00:12:40.679
Data scientists: we should not be the arbiters of truth.
数据科学家们：我们不应该 成为真相的仲裁者。

265
00:12:41.340 --> 00:12:45.123
We should be translators of ethical discussions that happen
我们应该成为大社会中 所发生的道德讨论的

266
00:12:45.147 --> 00:12:46.441
in larger society.
翻译者。

267
00:12:47.399 --> 00:12:49.532
(Applause)
（掌声）

268
00:12:49.556 --> 00:12:51.112
And the rest of you,
然后剩下的人，

269
00:12:51.831 --> 00:12:53.227
the non-data scientists:
非数据科学家们：

270
00:12:53.251 --> 00:12:54.749
this is not a math test.
这不是一个数学测试。

271
00:12:55.452 --> 00:12:56.800
This is a political fight.
这是一场政治斗争。

272
00:12:58.407 --> 00:13:02.314
We need to demand accountability for our algorithmic overlords.
我们应该要求我们的 算法霸主承担问责。

273
00:13:03.938 --> 00:13:05.437
(Applause)
（掌声）

274
00:13:05.461 --> 00:13:09.686
The era of blind faith in big data must end.
盲目信仰大数据的时代必须结束。

275
00:13:09.710 --> 00:13:10.877
Thank you very much.
非常感谢。

276
00:13:10.901 --> 00:13:16.204
(Applause)
（掌声）