WEBVTT

1
00:00:12.580 --> 00:00:16.116
So when people voice fears of artificial intelligence,
当人们谈论起对于 人工智能的恐惧时

2
00:00:16.140 --> 00:00:20.116
very often, they invoke images of humanoid robots run amok.
浮现在脑海里的 往往是失控的机器人

3
00:00:20.140 --> 00:00:21.380
You know? Terminator?
就像终结者一样

4
00:00:22.220 --> 00:00:24.556
You know, that might be something to consider,
这种担心固然有一定道理

5
00:00:24.580 --> 00:00:26.436
but that's a distant threat.
但目前和我们相隔甚远

6
00:00:26.460 --> 00:00:29.916
Or, we fret about digital surveillance
我们也会对数字监控心生恐惧

7
00:00:29.940 --> 00:00:31.716
with metaphors from the past.
这从过去的隐喻中就可以初见端倪

8
00:00:31.740 --> 00:00:34.396
"1984," George Orwell's "1984,"
例如乔治·奥威尔的著作 1984

9
00:00:34.420 --> 00:00:36.700
it's hitting the bestseller lists again.
最近再次登上热销榜

10
00:00:37.780 --> 00:00:39.196
It's a great book,
这是一本很好的书

11
00:00:39.220 --> 00:00:43.100
but it's not the correct dystopia for the 21st century.
但是书中的反乌托邦社会 并不是21世纪的正确缩影

12
00:00:43.900 --> 00:00:45.316
What we need to fear most
我们最应该担心的

13
00:00:45.340 --> 00:00:50.116
is not what artificial intelligence will do to us on its own,
并不是人工智能本身 对我们的影响

14
00:00:50.140 --> 00:00:54.876
but how the people in power will use artificial intelligence
而是掌权的人会怎样 利用人工智能

15
00:00:54.900 --> 00:00:57.716
to control us and to manipulate us
来控制并摆布我们

16
00:00:57.740 --> 00:01:00.876
in novel, sometimes hidden,
通过新奇 有时是隐蔽的

17
00:01:00.900 --> 00:01:03.916
subtle and unexpected ways.
微妙以及不可预料的手段

18
00:01:03.940 --> 00:01:05.796
Much of the technology
很多对我们的

19
00:01:05.820 --> 00:01:10.156
that threatens our freedom and our dignity in the near-term future
自由和尊严有潜在威胁的科技

20
00:01:10.180 --> 00:01:12.036
is being developed by companies
正在被那些收集

21
00:01:12.060 --> 00:01:16.996
in the business of capturing and selling our data and our attention
并贩卖我们的私人信息给广告商的

22
00:01:17.020 --> 00:01:19.276
to advertisers and others:
公司开发出来

23
00:01:19.300 --> 00:01:22.716
Facebook, Google, Amazon,
例如脸书 谷歌 亚马逊

24
00:01:22.740 --> 00:01:24.620
Alibaba, Tencent.
以及阿里巴巴和腾讯

25
00:01:25.860 --> 00:01:31.356
Now, artificial intelligence has started bolstering their business as well.
现在 人工智能也开始强化 他们自身的业务

26
00:01:31.380 --> 00:01:33.476
And it may seem like artificial intelligence
看起来好像人工智能只不过

27
00:01:33.500 --> 00:01:36.356
is just the next thing after online ads.
是网络广告的下一步

28
00:01:36.380 --> 00:01:37.596
It's not.
但并非如此

29
00:01:37.620 --> 00:01:40.076
It's a jump in category.
它是一个全新的类别

30
00:01:40.100 --> 00:01:42.676
It's a whole different world,
是一个完全不同的世界

31
00:01:42.700 --> 00:01:45.316
and it has great potential.
并且有着极高的潜力

32
00:01:45.340 --> 00:01:52.260
It could accelerate our understanding of many areas of study and research.
它可以加快人们在很多 领域的学习与研究速度

33
00:01:52.940 --> 00:01:56.436
But to paraphrase a famous Hollywood philosopher,
但就如好莱坞一名著名哲学家所言

34
00:01:56.460 --> 00:02:00.100
"With prodigious potential comes prodigious risk."
惊人的潜力带来的是惊人的风险

35
00:02:00.940 --> 00:02:04.876
Now let's look at a basic fact of our digital lives, online ads.
我们得明白关于数字生活 以及网络广告的基本事实

36
00:02:04.900 --> 00:02:07.796
Right? We kind of dismiss them.
是吧 我们几乎把它们忽略了

37
00:02:07.820 --> 00:02:09.796
They seem crude, ineffective.
尽管它们看起来很粗糙 没什么说服力

38
00:02:09.820 --> 00:02:14.076
We've all had the experience of being followed on the web
我们都曾在上网时 被网上的一些广告追踪过

39
00:02:13.931 --> 00:02:16.605
by an ad based on something we searched or read.
它们是根据我们的浏览历史生成的

40
00:02:16.900 --> 00:02:18.756
You know, you look up a pair of boots
比如 你搜索了一双皮靴

41
00:02:18.780 --> 00:02:22.156
and for a week, those boots are following you around everywhere you go.
接下来的一周里 这双皮靴就 在网上如影随形的跟着你

42
00:02:22.180 --> 00:02:25.836
Even after you succumb and buy them, they're still following you around.
即使你屈服了 买下了它们 广告也不会消失

43
00:02:25.860 --> 00:02:28.876
We're kind of inured to that kind of basic, cheap manipulation.
我们已经习惯了这种 廉价粗暴的操纵

44
00:02:28.900 --> 00:02:32.300
We roll our eyes and we think, "You know what? These things don't work."
还不屑一顾的想着 这东西对我没用的

45
00:02:33.540 --> 00:02:35.636
Except, online,
但是别忘了 在网上

46
00:02:35.660 --> 00:02:39.260
the digital technologies are not just ads.
广告并不是数字科技的全部

47
00:02:40.060 --> 00:02:43.180
Now, to understand that, let's think of a physical world example.
为了便于理解 我们举几个 现实世界的例子

48
00:02:43.660 --> 00:02:48.316
You know how, at the checkout counters at supermarkets, near the cashier,
你知道为什么在超市收银台的旁边

49
00:02:48.340 --> 00:02:51.820
there's candy and gum at the eye level of kids?
要放一些小孩子 一眼就能看到的糖果吗

50
00:02:52.620 --> 00:02:56.116
That's designed to make them whine at their parents
那是为了让孩子在父母面前撒娇

51
00:02:56.140 --> 00:02:59.220
just as the parents are about to sort of check out.
就当他们马上要结账的时候

52
00:02:59.860 --> 00:03:02.500
Now, that's a persuasion architecture.
那是一种说服架构

53
00:03:02.980 --> 00:03:06.076
It's not nice, but it kind of works.
并不完美 但很管用

54
00:03:06.100 --> 00:03:08.140
That's why you see it in every supermarket.
这也是每家超市惯用的伎俩

55
00:03:08.540 --> 00:03:10.236
Now, in the physical world,
在现实世界里

56
00:03:10.260 --> 00:03:12.756
such persuasion architectures are kind of limited,
这种说服架构是有限制的

57
00:03:12.780 --> 00:03:17.596
because you can only put so many things by the cashier. Right?
因为能放在收银台旁边的 东西是有限的 对吧

58
00:03:17.620 --> 00:03:21.916
And the candy and gum, it's the same for everyone,
而且所有人看到的都是同样的糖果

59
00:03:21.940 --> 00:03:23.396
even though it mostly works
所以说大多数情况下

60
00:03:23.420 --> 00:03:27.460
only for people who have whiny little humans beside them.
只是针对那些带着小孩的买主

61
00:03:28.980 --> 00:03:32.900
In the physical world, we live with those limitations.
这些是现实世界的种种局限

62
00:03:34.100 --> 00:03:36.036
In the digital world, though,
但在网络世界里

63
00:03:35.952 --> 00:03:40.385
persuasion architectures can be built at the scale of billions
说服架构可以千变万化 因人而异

64
00:03:41.660 --> 00:03:45.516
and they can target, infer, understand
它们可以理解并推断个体用户的喜好

65
00:03:45.540 --> 00:03:48.436
and be deployed at individuals
然后被部署在用户周围

66
00:03:48.460 --> 00:03:49.676
one by one
一个接一个

67
00:03:49.700 --> 00:03:51.836
by figuring out your weaknesses,
通过对每个人弱点的了解

68
00:03:51.860 --> 00:03:57.476
and they can be sent to everyone's phone private screen,
出现在每个人的私人手机屏幕上

69
00:03:57.500 --> 00:03:59.756
so it's not visible to us.
而其他人却看不见

70
00:03:59.780 --> 00:04:01.036
And that's different.
这是（与物质世界）截然不同的地方

71
00:04:00.876 --> 00:04:03.722
And that's just one of the basic things that artificial intelligence can do.
而这仅仅是人工智能的基本功能之一

72
00:04:04.660 --> 00:04:05.996
Now, let's take an example.
再举个例子

73
00:04:05.426 --> 00:04:08.472
Let's say you want to sell plane tickets to Vegas. Right?
假如你要销售飞往 拉斯维加斯的机票

74
00:04:08.740 --> 00:04:12.236
So in the old world, you could think of some demographics to target
在过去 你也许需要一些 统计资料来确定销售对象

75
00:04:12.260 --> 00:04:14.780
based on experience and what you can guess.
然后根据你的个人经验和判断

76
00:04:15.380 --> 00:04:18.196
You might try to advertise to, oh,
你也许会把推广目标定为

77
00:04:17.821 --> 00:04:20.702
men between the ages of 25 and 35,
25岁到35岁的男性

78
00:04:20.740 --> 00:04:24.676
or people who have a high limit on their credit card,
或者是高信用卡额度人群

79
00:04:24.700 --> 00:04:26.076
or retired couples. Right?
或者是退休夫妇 对吧

80
00:04:25.988 --> 00:04:27.361
That's what you would do in the past.
那就是你以前采用的方法

81
00:04:27.940 --> 00:04:30.836
With big data and machine learning,
但在大数据和人工智能面前

82
00:04:30.860 --> 00:04:32.384
that's not how it works anymore.
一切都改变了

83
00:04:33.140 --> 00:04:35.316
So to imagine that,
请想象一下

84
00:04:35.340 --> 00:04:39.196
think of all the data that Facebook has on you:
你被Facebook掌握的所有信息

85
00:04:39.220 --> 00:04:41.756
every status update you ever typed,
你的每一次状态更新

86
00:04:41.780 --> 00:04:43.796
every Messenger conversation,
每一条对话内容

87
00:04:43.820 --> 00:04:45.700
every place you logged in from,
所有的登陆地点

88
00:04:48.220 --> 00:04:51.396
all your photographs that you uploaded there.
你上传的所有照片

89
00:04:51.420 --> 00:04:55.196
If you start typing something and change your mind and delete it,
还有你输入了一部分 后来又删掉的内容

90
00:04:55.220 --> 00:04:58.420
Facebook keeps those and analyzes them, too.
Facebook也会保存下来进行分析

91
00:04:58.980 --> 00:05:02.916
Increasingly, it tries to match you with your offline data.
它将越来越多的数据 和你的离线生活匹配

92
00:05:02.940 --> 00:05:06.116
It also purchases a lot of data from data brokers.
还有从网络信息商贩那里购买信息

93
00:05:06.140 --> 00:05:09.556
It could be everything from your financial records
从你的财务记录到 所有网页浏览记录

94
00:05:09.580 --> 00:05:11.700
to a good chunk of your browsing history.
各类信息无所不包

95
00:05:12.180 --> 00:05:17.596
Right? In the US, such data is routinely collected,
在美国 这种数据是经常被收集

96
00:05:17.620 --> 00:05:19.580
collated and sold.
被整理 然后被贩卖的

97
00:05:20.140 --> 00:05:22.580
In Europe, they have tougher rules.
而在欧洲 这是被明令禁止的

98
00:05:23.500 --> 00:05:25.700
So what happens then is,
所以接下来会发生的是

99
00:05:26.740 --> 00:05:30.756
by churning through all that data, these machine-learning algorithms --
电脑通过算法分析 所有收集到的数据

100
00:05:30.780 --> 00:05:33.676
that's why they're called learning algorithms --
这个算法之所以叫做学习算法

101
00:05:33.700 --> 00:05:37.796
they learn to understand the characteristics of people
因为它们能够学会分析所有之前买过

102
00:05:37.820 --> 00:05:40.340
who purchased tickets to Vegas before.
去维加斯机票的人的性格特征

103
00:05:41.580 --> 00:05:45.116
When they learn this from existing data,
而在学会分析已有数据的同时

104
00:05:45.140 --> 00:05:48.956
they also learn how to apply this to new people.
它们也在学习如何将其 应用在新的人群中

105
00:05:48.980 --> 00:05:52.036
So if they're presented with a new person,
如果有个新用户

106
00:05:52.060 --> 00:05:56.700
they can classify whether that person is likely to buy a ticket to Vegas or not.
它们可以迅速判断这个人 会不会买去维加斯的机票

107
00:05:57.540 --> 00:06:02.996
Fine. You're thinking, an offer to buy tickets to Vegas.
这倒还好 你也许会想 不就是一个卖机票的广告吗

108
00:06:03.020 --> 00:06:04.476
I can ignore that.
我不理它不就行了

109
00:06:04.500 --> 00:06:06.716
But the problem isn't that.
但问题不在这儿

110
00:06:06.740 --> 00:06:08.316
The problem is,
真正的问题是

111
00:06:08.340 --> 00:06:12.476
we no longer really understand how these complex algorithms work.
我们已经无法真正理解 这些复杂的算法究竟是怎样工作的了

112
00:06:12.500 --> 00:06:15.956
We don't understand how they're doing this categorization.
我们不知道它们是 如何进行这种分类的

113
00:06:15.980 --> 00:06:20.396
It's giant matrices, thousands of rows and columns,
那是庞大的数字矩阵 成千上万的行与列

114
00:06:20.420 --> 00:06:22.380
maybe millions of rows and columns,
也许是数百万的行与列

115
00:06:23.140 --> 00:06:25.780
and not the programmers
而没有程序员看管它们

116
00:06:26.580 --> 00:06:28.260
and not anybody who looks at it,
没有任何人看管它们

117
00:06:29.260 --> 00:06:30.756
even if you have all the data,
即使你拥有所有的数据

118
00:06:30.780 --> 00:06:35.396
understands anymore how exactly it's operating
也完全了解算法是如何运行的

119
00:06:35.420 --> 00:06:39.196
any more than you'd know what I was thinking right now
如果仅仅展示给你我的部分脑截面

120
00:06:39.220 --> 00:06:43.180
if you were shown a cross section of my brain.
你也不可能知道我的想法

121
00:06:44.180 --> 00:06:46.756
It's like we're not programming anymore,
就好像这已经不是我们在编程了

122
00:06:46.780 --> 00:06:51.180
we're growing intelligence that we don't truly understand.
我们是在创造一种 我们并不了解的智能

123
00:06:52.340 --> 00:06:56.316
And these things only work if there's an enormous amount of data,
这种智能只有在 庞大的数据支持下才能工作

124
00:06:56.340 --> 00:07:01.436
so they also encourage deep surveillance on all of us
所以它们才致力于对我们 所有人进行强力监控

125
00:07:01.460 --> 00:07:03.796
so that the machine learning algorithms can work.
以便学习算法的运行

126
00:07:03.820 --> 00:07:06.996
That's why Facebook wants to collect all the data it can about you.
这就是Facebook费尽心思 收集用户信息的原因

127
00:07:07.020 --> 00:07:08.596
The algorithms work better.
这样算法才能更好的运行

128
00:07:08.620 --> 00:07:11.316
So let's push that Vegas example a bit.
我们再将那个维加斯的 例子强化一下

129
00:07:11.340 --> 00:07:15.020
What if the system that we do not understand
如果那个我们并不了解的系统

130
00:07:16.020 --> 00:07:21.156
was picking up that it's easier to sell Vegas tickets
发现即将进入躁狂 阶段的躁郁症患者

131
00:07:21.180 --> 00:07:24.940
to people who are bipolar and about to enter the manic phase.
更有可能买去维加斯的机票

132
00:07:25.460 --> 00:07:30.380
Such people tend to become overspenders, compulsive gamblers.
这是一群有挥霍金钱 以及好赌倾向的人

133
00:07:30.873 --> 00:07:35.329
They could do this, and you'd have no clue that's what they were picking up on.
这些算法完全做得到 而你却对它们 是如何做到的毫不知情

134
00:07:35.580 --> 00:07:39.196
I gave this example to a bunch of computer scientists once
我曾把这个例子举给 一些计算机科学家

135
00:07:39.220 --> 00:07:41.276
and afterwards, one of them came up to me.
后来其中一个找到我

136
00:07:41.300 --> 00:07:44.820
He was troubled and he said, "That's why I couldn't publish it."
他很烦恼 并对我说 这就是我没办法发表它的原因

137
00:07:45.420 --> 00:07:47.135
I was like, "Couldn't publish what?"
我问 发表什么

138
00:07:47.620 --> 00:07:53.476
He had tried to see whether you can indeed figure out the onset of mania
他曾尝试在狂躁症病人 被确诊具有某些医疗症状前

139
00:07:53.500 --> 00:07:56.716
from social media posts before clinical symptoms,
是否可以从他们的社交媒体上 发现病情的端倪

140
00:07:56.740 --> 00:07:58.516
and it had worked,
他做到了

141
00:07:58.540 --> 00:08:00.596
and it had worked very well,
还做得相当不错

142
00:08:00.620 --> 00:08:05.500
and he had no idea how it worked or what it was picking up on.
但他不明白这是怎么做到的 或者说如何算出来的

143
00:08:06.660 --> 00:08:11.076
Now, the problem isn't solved if he doesn't publish it,
那么如果他不发表论文 这个问题就得不到解决

144
00:08:11.100 --> 00:08:12.996
because there are already companies
因为早就有其他的一些公司

145
00:08:13.020 --> 00:08:15.556
that are developing this kind of technology,
在发展这样的科技了

146
00:08:15.580 --> 00:08:18.380
and a lot of the stuff is just off the shelf.
很多类似的东西现在就摆在货架上

147
00:08:19.060 --> 00:08:21.636
This is not very difficult anymore.
这已经不是什么难事了

148
00:08:21.660 --> 00:08:25.116
Do you ever go on YouTube meaning to watch one video
你是否曾经想在YouTube 上看一个视频

149
00:08:24.988 --> 00:08:27.679
and an hour later you've watched 27?
结果不知不觉看了27个

150
00:08:28.580 --> 00:08:31.076
You know how YouTube has this column on the right
你知不知道YouTube的 网页右边有一个边栏

151
00:08:31.100 --> 00:08:33.316
that says, "Up next"
上面写着 即将播放

152
00:08:33.340 --> 00:08:35.156
and it autoplays something?
然后它往往会自动播放一些东西

153
00:08:35.180 --> 00:08:36.396
It's an algorithm
这就是算法

154
00:08:36.420 --> 00:08:40.036
picking what it thinks that you might be interested in
算出你的兴趣点

155
00:08:40.060 --> 00:08:41.596
and maybe not find on your own.
甚至连你自己都没想到

156
00:08:41.620 --> 00:08:42.876
It's not a human editor.
这可不是人工编辑

157
00:08:42.900 --> 00:08:44.316
It's what algorithms do.
这就是算法的本职工作

158
00:08:44.340 --> 00:08:49.076
It picks up on what you have watched and what people like you have watched,
它选出你以及和你 相似的人看过的视频

159
00:08:49.100 --> 00:08:53.316
and infers that that must be what you're interested in,
然后推断出你的大致兴趣圈

160
00:08:53.340 --> 00:08:54.595
what you want more of,
推断出你想看什么

161
00:08:54.619 --> 00:08:55.955
and just shows you more.
然后就那些东西展示给你

162
00:08:55.979 --> 00:08:58.180
It sounds like a benign and useful feature,
听起来像是一个无害且贴心的功能

163
00:08:58.878 --> 00:09:00.506
except when it isn't.
但有时候它并不是

164
00:09:01.460 --> 00:09:08.420
So in 2016, I attended rallies of then-candidate Donald Trump
2016年 我参加了当时的总统 候选人 唐纳德 特朗普 的系列集会

165
00:09:09.660 --> 00:09:12.996
to study as a scholar the movement supporting him.
以学者的身份研究 这个支持他的运动

166
00:09:13.020 --> 00:09:16.476
I study social movements, so I was studying it, too.
我当时正好在研究社会运动

167
00:09:16.500 --> 00:09:19.836
And then I wanted to write something about one of his rallies,
然后我想要写一些 有关其中一次集会的文章

168
00:09:19.860 --> 00:09:21.820
so I watched it a few times on YouTube.
所以我在YouTube上看了几遍 这个集会的视频

169
00:09:22.984 --> 00:09:25.762
YouTube started recommending to me
然后YouTube就开始 不断给我推荐

170
00:09:26.180 --> 00:09:30.436
and autoplaying to me white supremacist videos
并且自动播放一些 白人至上主义的视频

171
00:09:30.460 --> 00:09:33.116
in increasing order of extremism.
这些视频一个比一个更极端

172
00:09:33.140 --> 00:09:34.956
If I watched one,
如果我看了一个

173
00:09:34.980 --> 00:09:37.956
it served up one even more extreme
就会有另一个更加 极端的视频加入队列

174
00:09:37.980 --> 00:09:39.404
and autoplayed that one, too.
并自动播放

175
00:09:40.140 --> 00:09:44.676
If you watch Hillary Clinton or Bernie Sanders content,
如果你看有关 希拉里 克林顿 或者 伯尼 桑德斯 的内容

176
00:09:44.700 --> 00:09:49.396
YouTube recommends and autoplays conspiracy left,
YouTube就会开始推荐并 自动播放左翼阴谋内容

177
00:09:49.420 --> 00:09:51.180
and it goes downhill from there.
并且愈演愈烈

178
00:09:52.300 --> 00:09:55.356
Well, you might be thinking, this is politics, but it's not.
你也许觉得这和政治有关

179
00:09:55.380 --> 00:09:56.636
This isn't about politics.
但事实上并不是这样

180
00:09:56.660 --> 00:09:59.756
This is just the algorithm figuring out human behavior.
这只不过是算法在 学习人类行为而已

181
00:09:59.780 --> 00:10:04.556
I once watched a video about vegetarianism on YouTube
我曾在YouTube上观看过 一个有关素食主义的视频

182
00:10:04.580 --> 00:10:09.516
and YouTube recommended and autoplayed a video about being vegan.
然后YouTube就推送了 纯素主义的视频

183
00:10:09.540 --> 00:10:12.556
It's like you're never hardcore enough for YouTube.
在YouTube上你就 好像永远都不够决绝

184
00:10:12.580 --> 00:10:14.156
(Laughter)
（笑声）

185
00:10:14.180 --> 00:10:15.740
So what's going on?
这到底是怎么回事儿

186
00:10:16.340 --> 00:10:19.876
Now, YouTube's algorithm is proprietary,
现在YouTube有其专有的算法

187
00:10:19.900 --> 00:10:22.260
but here's what I think is going on.
但我认为事情是这样的

188
00:10:23.180 --> 00:10:25.276
The algorithm has figured out
这算法已经分析出了

189
00:10:25.300 --> 00:10:28.996
that if you can entice people
如果能展示出更加核心的内容

190
00:10:28.941 --> 00:10:32.559
into thinking that you can show them something more hardcore,
以此来诱惑网站用户

191
00:10:32.780 --> 00:10:35.196
they're more likely to stay on the site
那么人们就更有可能沉浸在网页里

192
00:10:35.220 --> 00:10:39.636
watching video after video going down that rabbit hole
一个接一个的观看推荐的视频

193
00:10:39.660 --> 00:10:41.340
while Google serves them ads.
同时Google给它们投放广告

194
00:10:43.580 --> 00:10:46.700
Now, with nobody minding the ethics of the store,
目前没有人在意网络的道德规范

195
00:10:47.540 --> 00:10:51.780
these sites can profile people
这些网站可以对用户进行划分

196
00:10:53.500 --> 00:10:55.420
who are Jew haters,
哪些人仇视犹太人

197
00:10:56.180 --> 00:10:58.660
who think that Jews are parasites
哪些人视犹太人为寄生虫

198
00:11:00.140 --> 00:11:05.060
and who have such explicit anti-Semitic content,
以及说过明显反犹太言论的人

199
00:11:05.900 --> 00:11:07.900
and let you target them with ads.
然后让你面向这些 目标人群投放广告

200
00:11:09.020 --> 00:11:12.556
They can also mobilize algorithms
他们也可以利用算法

201
00:11:12.580 --> 00:11:15.716
to find for you look-alike audiences,
来找到和你类似的观众

202
00:11:15.740 --> 00:11:21.316
people who do not have such explicit anti-Semitic content on their profile
那些个人账号中虽然没有过 明显的反犹太人言论

203
00:11:21.340 --> 00:11:27.516
but who the algorithm detects may be susceptible to such messages,
但却被算法检测出 可能被这种言论影响的人

204
00:11:27.540 --> 00:11:29.460
and lets you target them with ads, too.
然后也面向他们投放同样的广告

205
00:11:30.500 --> 00:11:33.236
Now, this may sound like an implausible example,
这听起来难以置信

206
00:11:33.260 --> 00:11:34.580
but this is real.
但确有其事

207
00:11:35.300 --> 00:11:37.436
ProPublica investigated this
ProPublica在这方面调查过

208
00:11:37.460 --> 00:11:41.076
and found that you can indeed do this on Facebook,
发现这的确可以在Facebook上实现

209
00:11:41.100 --> 00:11:43.516
and Facebook helpfully offered up suggestions
Facebook还积极的就 有关如何将算法的受众

210
00:11:43.540 --> 00:11:45.140
on how to broaden that audience.
再度扩大提出了建议

211
00:11:46.540 --> 00:11:49.556
BuzzFeed tried it for Google, and very quickly they found,
Buzzfeed曾在Google上 进行尝试 并很快发现

212
00:11:49.580 --> 00:11:51.316
yep, you can do it on Google, too.
没错 这也可在Google实现

213
00:11:51.340 --> 00:11:53.036
And it wasn't even expensive.
而这甚至花不了多少钱

214
00:11:53.060 --> 00:11:57.476
The ProPublica reporter spent about 30 dollars
ProPublica只花了大概30美元

215
00:11:57.500 --> 00:11:59.740
to target this category.
就找出了目标人群

216
00:12:02.420 --> 00:12:07.716
So last year, Donald Trump's social media manager disclosed
那么去年 特朗普的 社交媒体经理披露道

217
00:12:07.740 --> 00:12:13.076
that they were using Facebook dark posts to demobilize people,
他们使用Facebook的 隐藏发帖来动员大众退出

218
00:12:13.100 --> 00:12:14.476
not to persuade them,
不是劝告

219
00:12:14.500 --> 00:12:17.300
but to convince them not to vote at all.
而是说服他们根本就不要投票

220
00:12:18.340 --> 00:12:21.916
And to do that, they targeted specifically,
为了做到这一点 他们有 针对性的找到目标

221
00:12:21.940 --> 00:12:25.836
for example, African-American men in key cities like Philadelphia,
比如 在费城这种关键城市里 居住的非裔美国人

222
00:12:25.860 --> 00:12:28.316
and I'm going to read exactly what he said.
请注意接下来我要复述的

223
00:12:28.340 --> 00:12:29.556
I'm quoting.
都是他们的原话

224
00:12:29.580 --> 00:12:32.596
They were using "nonpublic posts
他们使用 以下是引用 由竞选者控制的

225
00:12:32.620 --> 00:12:34.796
whose viewership the campaign controls
非面向公众的贴文发帖

226
00:12:34.820 --> 00:12:38.596
so that only the people we want to see it see it.
这样就只有我们选定的人 可以看到其内容

227
00:12:38.620 --> 00:12:39.836
We modeled this.
我们估算了一下

228
00:12:39.860 --> 00:12:44.580
It will dramatically affect her ability to turn these people out."
这会极大程度的做到让这些人退出

229
00:12:45.540 --> 00:12:47.820
What's in those dark posts?
以上我引述的隐藏贴文说了些什么呢

230
00:12:48.300 --> 00:12:49.956
We have no idea.
我们无从知晓

231
00:12:49.980 --> 00:12:51.180
Facebook won't tell us.
Facebook不会告诉我们

232
00:12:52.300 --> 00:12:56.676
So Facebook also algorithmically arranges the posts
所以Facebook也利用 算法管理贴文

233
00:12:56.700 --> 00:13:00.436
that your friends put on Facebook, or the pages you follow.
不管是你朋友的发帖 还是你的跟帖

234
00:13:00.460 --> 00:13:02.676
It doesn't show you everything chronologically.
它不会把东西按时间顺序展现给你

235
00:13:02.700 --> 00:13:07.516
It puts the order in the way that the algorithm thinks will entice you
而是按算法计算的顺序展现给你

236
00:13:07.540 --> 00:13:09.380
to stay on the site longer.
以使你更长时间停留在页面上

237
00:13:10.860 --> 00:13:14.236
Now, so this has a lot of consequences.
而这一切都是有后果的

238
00:13:14.260 --> 00:13:18.060
You may be thinking somebody is snubbing you on Facebook.
你也许会觉得有人在 Facebook上对你不理不睬

239
00:13:18.620 --> 00:13:21.876
The algorithm may never be showing your post to them.
这是因为算法可能根本就 没有给他们展示你的发帖

240
00:13:21.900 --> 00:13:27.860
The algorithm is prioritizing some of them and burying the others.
算法会优先展示一些贴文 而把另一些埋没

241
00:13:28.972 --> 00:13:30.144
Experiments show
实验显示

242
00:13:30.460 --> 00:13:34.980
that what the algorithm picks to show you can affect your emotions.
算法决定展示给你的东西 会影响到你的情绪

243
00:13:36.420 --> 00:13:37.620
But that's not all.
还不止这样

244
00:13:38.100 --> 00:13:40.460
It also affects political behavior.
它也会影响到政治行为

245
00:13:41.180 --> 00:13:45.836
So in 2010, in the midterm elections,
在2010年的中期选举中

246
00:13:45.860 --> 00:13:51.756
Facebook did an experiment on 61 million people in the US
Facebook对美国6100万人 做了一个实验

247
00:13:51.780 --> 00:13:53.676
that was disclosed after the fact.
这是在事后被披露的

248
00:13:53.700 --> 00:13:57.116
So some people were shown, "Today is election day,"
当时有些人收到了 今天是选举日 的贴文

249
00:13:56.992 --> 00:13:58.243
the simpler one,
简单的版本

250
00:13:58.540 --> 00:14:02.436
and some people were shown the one with that tiny tweak
而有一些人则收到了 微调过的贴文

251
00:14:02.460 --> 00:14:04.556
with those little thumbnails
上面有一些小的缩略图

252
00:14:04.580 --> 00:14:07.420
of your friends who clicked on "I voted."
显示的是你的 哪些好友 已投票

253
00:14:08.820 --> 00:14:10.220
This simple tweak.
这小小的微调

254
00:14:11.340 --> 00:14:15.636
OK? So the pictures were the only change,
看到了吧 改变仅仅是 添加了缩略图而已

255
00:14:15.660 --> 00:14:18.916
and that post shown just once
并且那些贴文仅出现一次

256
00:14:18.940 --> 00:14:24.996
turned out an additional 340,000 voters
后来的调查结果显示

257
00:14:25.020 --> 00:14:26.716
in that election,
在那次选举中

258
00:14:26.740 --> 00:14:28.436
according to this research
根据选民登记册的确认

259
00:14:28.460 --> 00:14:30.980
as confirmed by the voter rolls.
多出了34万的投票者

260
00:14:32.740 --> 00:14:34.396
A fluke? No.
仅仅是意外吗 并非如此

261
00:14:34.420 --> 00:14:39.780
Because in 2012, they repeated the same experiment.
因为在2012年 他们再次进行了同样的实验

262
00:14:40.660 --> 00:14:42.396
And that time,
而那一次

263
00:14:42.420 --> 00:14:45.716
that civic message shown just once
类似贴文也只出现了一次

264
00:14:45.740 --> 00:14:50.180
turned out an additional 270,000 voters.
最后多出了28万投票者

265
00:14:50.980 --> 00:14:56.196
For reference, the 2016 US presidential election
作为参考 2016年总统大选的

266
00:14:56.220 --> 00:14:59.740
was decided by about 100,000 votes.
最终结果是由大概 十万张选票决定的

267
00:15:00.747 --> 00:15:05.793
Now, Facebook can also very easily infer what your politics are,
Facebook还可以轻易 推断出你的政治倾向

268
00:15:05.940 --> 00:15:08.196
even if you've never disclosed them on the site.
即使你从没有在网上披露过

269
00:15:08.220 --> 00:15:10.740
Right? These algorithms can do that quite easily.
这可难不倒算法

270
00:15:11.780 --> 00:15:15.676
What if a platform with that kind of power
而如果一个拥有 这样强大能力的平台

271
00:15:15.700 --> 00:15:20.740
decides to turn out supporters of one candidate over the other?
决定要让一个候选者胜利获选

272
00:15:21.500 --> 00:15:23.940
How would we even know about it?
我们根本无法察觉

273
00:15:25.380 --> 00:15:29.516
Now, we started from someplace seemingly innocuous --
现在我们从一个无伤大雅的方面 也就是如影随形的

274
00:15:29.540 --> 00:15:31.756
online adds following us around --
网络广告

275
00:15:31.780 --> 00:15:33.620
and we've landed someplace else.
转到了另一个方面

276
00:15:35.300 --> 00:15:37.756
As a public and as citizens,
作为一个普通大众和公民

277
00:15:37.780 --> 00:15:41.196
we no longer know if we're seeing the same information
我们已经无法确认 自己看到的信息

278
00:15:41.220 --> 00:15:42.700
or what anybody else is seeing,
和别人看到的信息是否一样

279
00:15:43.500 --> 00:15:46.076
and without a common basis of information,
而在没有一个共同的 基本信息的情况下

280
00:15:46.100 --> 00:15:47.716
little by little,
逐渐的

281
00:15:47.740 --> 00:15:50.956
public debate is becoming impossible,
公开辩论将变得不再可能

282
00:15:50.980 --> 00:15:53.956
and we're just at the beginning stages of this.
而我们已经开始走在这条路上了

283
00:15:53.980 --> 00:15:57.436
These algorithms can quite easily infer
这些算法可以轻易推断出

284
00:15:57.460 --> 00:16:00.716
things like your people's ethnicity,
任何一个用户的种族 宗教信仰

285
00:16:00.740 --> 00:16:03.076
religious and political views, personality traits,
包括政治倾向 还有个人喜好

286
00:16:03.100 --> 00:16:06.476
intelligence, happiness, use of addictive substances,
你的智力 心情 以及用药历史

287
00:16:06.500 --> 00:16:09.636
parental separation, age and genders,
父母是否离异 你的年龄和性别

288
00:16:09.660 --> 00:16:11.620
just from Facebook likes.
这些都可以从你的 Facebook关注里推算出来

289
00:16:12.849 --> 00:16:17.229
These algorithms can identify protesters
这些算法可以识别抗议人士

290
00:16:17.340 --> 00:16:20.100
even if their faces are partially concealed.
即使他们部分掩盖了面部特征

291
00:16:21.540 --> 00:16:28.156
These algorithms may be able to detect people's sexual orientation
这些算法可以测出人们的性取向

292
00:16:27.664 --> 00:16:31.483
just from their dating profile pictures.
只需要查看他们的约会账号头像

293
00:16:33.380 --> 00:16:35.996
Now, these are probabilistic guesses,
然而所有的一切都 只是概率性的推算

294
00:16:36.020 --> 00:16:38.916
so they're not going to be 100 percent right,
所以它们不会百分之百精确

295
00:16:38.940 --> 00:16:43.836
but I don't see the powerful resisting the temptation to use these technologies
这些算法有很多误报

296
00:16:41.031 --> 00:16:43.942
just because there are some false positives,
也必然会导致其他层次的种种问题

297
00:16:43.942 --> 00:16:48.600
which will of course create a whole other layer of problems.
但我没有看到对想要使用这些 科技的有力反抗

298
00:16:49.340 --> 00:16:52.276
Imagine what a state can do
想象一下 拥有了海量的市民数据

299
00:16:52.300 --> 00:16:55.860
with the immense amount of data it has on its citizens.
一个国家能做出什么

300
00:16:56.500 --> 00:17:01.276
China is already using face detection technology
中国已经在使用

301
00:17:01.300 --> 00:17:04.180
to identify and arrest people.
面部识别来抓捕犯人

302
00:17:05.100 --> 00:17:07.236
And here's the tragedy:
然而不幸的是

303
00:17:07.260 --> 00:17:12.796
we're building this infrastructure of surveillance authoritarianism
我们正在建造一个 监控独裁性质的设施

304
00:17:12.820 --> 00:17:15.780
merely to get people to click on ads.
目的仅是为了让人们点击广告

305
00:17:17.060 --> 00:17:19.636
And this won't be Orwell's authoritarianism.
而这和奥威尔笔下的独裁政府不同

306
00:17:19.659 --> 00:17:21.556
This isn't "1984."
不是 1984 里的情景

307
00:17:21.580 --> 00:17:26.156
Now, if authoritarianism is using overt fear to terrorize us,
现在如果独裁主义公开恐吓我们

308
00:17:25.826 --> 00:17:28.665
we'll all be scared, but we'll know it,
我们会惧怕 但我们也会察觉

309
00:17:29.100 --> 00:17:31.300
we'll hate it and we'll resist it.
我们会奋起抵抗并瓦解它

310
00:17:32.700 --> 00:17:37.116
But if the people in power are using these algorithms
但如果掌权的人使用这种算法

311
00:17:36.946 --> 00:17:40.243
to quietly watch us,
来安静的监视我们

312
00:17:40.540 --> 00:17:42.620
to judge us and to nudge us,
来评判我们 煽动我们

313
00:17:43.540 --> 00:17:47.716
to predict and identify the troublemakers and the rebels,
来预测和识别出那些 会给政府制造麻烦的家伙

314
00:17:47.740 --> 00:17:51.636
to deploy persuasion architectures at scale
并且大规模的布置说服性的架构

315
00:17:51.660 --> 00:17:55.796
and to manipulate individuals one by one
利用每个人自身的

316
00:17:55.820 --> 00:18:01.260
using their personal, individual weaknesses and vulnerabilities,
弱点和漏洞来把我们逐个击破

317
00:18:02.540 --> 00:18:04.740
and if they're doing it at scale
假如他们的做法受众面很广

318
00:18:05.900 --> 00:18:07.636
through our private screens
就会给每个手机都推送不同的信息

319
00:18:07.660 --> 00:18:09.316
so that we don't even know
这样我们甚至都不会知道

320
00:18:09.340 --> 00:18:12.100
what our fellow citizens and neighbors are seeing,
我们周围的人看到的是什么

321
00:18:13.380 --> 00:18:18.196
that authoritarianism will envelop us like a spider's web
独裁主义会像蜘蛛网 一样把我们困住

322
00:18:18.220 --> 00:18:20.700
and we may not even know we're in it.
而我们并不会意识到 自己已深陷其中

323
00:18:21.879 --> 00:18:25.152
So Facebook's market capitalization
Facebook现在的市值

324
00:18:25.220 --> 00:18:28.516
is approaching half a trillion dollars.
已经接近了5000亿美元

325
00:18:28.540 --> 00:18:31.660
It's because it works great as a persuasion architecture.
只因为它作为一个说服架构 完美的运作着

326
00:18:33.580 --> 00:18:36.396
But the structure of that architecture
但不管你是要卖鞋子

327
00:18:36.420 --> 00:18:39.636
is the same whether you're selling shoes
还是要卖政治思想

328
00:18:39.660 --> 00:18:42.156
or whether you're selling politics.
这个架构的结构都是固定的

329
00:18:42.180 --> 00:18:45.300
The algorithms do not know the difference.
算法并不知道其中的差异

330
00:18:45.969 --> 00:18:48.922
The same algorithms set loose upon us
同样的算法也被使用在我们身上

331
00:18:49.380 --> 00:18:52.556
to make us more pliable for ads
它让我们更易受广告诱导

332
00:18:52.580 --> 00:18:59.316
are also organizing our political, personal and social information flows,
也管控着我们的政治 个人 以及社会信息的流向

333
00:18:59.340 --> 00:19:01.180
and that's what's got to change.
而那正是需要改变的部分

334
00:19:01.911 --> 00:19:04.163
Now, don't get me wrong,
我还需要澄清一下

335
00:19:04.380 --> 00:19:08.060
we use digital platforms because they provide us with great value.
我们使用数字平台 因为它们带给我们便利

336
00:19:08.940 --> 00:19:12.500
I use Facebook to keep in touch with friends and family around the world.
我和世界各地的朋友和家人 通过 Facebook 联系

337
00:19:13.820 --> 00:19:19.596
I've written about how crucial social media is for social movements.
我也曾撰文谈过社交媒体 在社会运动中的重要地位

338
00:19:19.620 --> 00:19:22.636
I have studied how these technologies can be used
我也曾研究过如何使用这些技术

339
00:19:22.660 --> 00:19:25.140
to circumvent censorship around the world.
来绕开世界范围内的审查制度

340
00:19:26.979 --> 00:19:32.849
But it's not that the people who run, you know, Facebook or Google
但并不是那些管理Facebook 或者Google的人

341
00:19:33.540 --> 00:19:36.236
are maliciously and deliberately trying
在意图不轨的尝试

342
00:19:36.260 --> 00:19:40.716
to make the country or the world more polarized
如何使世界走向极端化

343
00:19:40.740 --> 00:19:42.420
and encourage extremism.
并且推广极端主义

344
00:19:43.260 --> 00:19:47.236
I read the many well-intentioned statements
我曾读到过很多由这些人写的

345
00:19:47.260 --> 00:19:50.580
that these people put out.
十分善意的言论

346
00:19:51.420 --> 00:19:57.476
But it's not the intent or the statements people in technology make that matter,
但重要的并不是 这些科技人员说的话

347
00:19:57.500 --> 00:20:01.060
it's the structures and business models they're building.
而是他们正在建造的 架构体系和商业模式

348
00:20:02.180 --> 00:20:04.276
And that's the core of the problem.
那才是问题的关键所在

349
00:20:04.300 --> 00:20:09.020
Either Facebook is a giant con of half a trillion dollars
要么Facebook是个 5000亿市值的弥天大谎

350
00:20:10.020 --> 00:20:11.916
and ads don't work on the site,
那些广告根本就不奏效

351
00:20:11.940 --> 00:20:14.636
it doesn't work as a persuasion architecture,
它并不是以一个 说服架构的模式成功运作

352
00:20:14.660 --> 00:20:18.780
or its power of influence is of great concern.
要么Facebook的影响力 就是令人担忧的

353
00:20:20.380 --> 00:20:22.156
It's either one or the other.
只有这两种可能

354
00:20:22.180 --> 00:20:23.780
It's similar for Google, too.
Google也是一样

355
00:20:24.700 --> 00:20:27.156
So what can we do?
那么我们能做什么呢

356
00:20:26.855 --> 00:20:28.414
This needs to change.
我们必须改变现状

357
00:20:28.946 --> 00:20:31.508
Now, I can't offer a simple recipe,
现在我还无法给出 一个简单的方法

358
00:20:31.740 --> 00:20:33.996
because we need to restructure
因为我们必须重新调整

359
00:20:34.020 --> 00:20:37.036
the whole way our digital technology operates.
整个数字科技的运行结构

360
00:20:37.060 --> 00:20:41.156
Everything from the way technology is developed
一切科技从发展到激励的方式

361
00:20:41.180 --> 00:20:45.036
to the way the incentives, economic and otherwise,
不论是在经济 还是在其他领域

362
00:20:45.060 --> 00:20:47.340
are built into the system.
都是建立在这种结构之上

363
00:20:47.861 --> 00:20:51.562
We have to face and try to deal with
我们必须得面对并尝试解决

364
00:20:51.780 --> 00:20:56.436
the lack of transparency created by the proprietary algorithms,
由专有算法制造出来的 透明度过低问题

365
00:20:56.460 --> 00:21:00.276
the structural challenge of machine learning's opacity,
还有由机器学习的 不透明带来的结构挑战

366
00:21:00.300 --> 00:21:03.700
all this indiscriminate data that's being collected about us.
以及所有这些不加选择 收集到的我们的信息

367
00:21:04.820 --> 00:21:07.340
We have a big task in front of us.
我们的任务艰巨

368
00:21:07.980 --> 00:21:10.660
We have to mobilize our technology,
必须调整我们的科技

369
00:21:11.580 --> 00:21:13.156
our creativity
我们的创造力

370
00:21:13.180 --> 00:21:15.060
and yes, our politics
以及我们的政治

371
00:21:15.816 --> 00:21:18.694
so that we can build artificial intelligence
这样我们才能够制造出

372
00:21:18.740 --> 00:21:21.860
that supports us in our human goals
真正为人类服务的人工智能

373
00:21:22.620 --> 00:21:26.540
but that is also constrained by our human values.
但这也会受到人类价值观的阻碍

374
00:21:27.420 --> 00:21:29.580
And I understand this won't be easy.
我也明白这不会轻松

375
00:21:30.180 --> 00:21:33.780
We might not even easily agree on what those terms mean.
我们甚至都无法在这些 理论上达成一致

376
00:21:34.740 --> 00:21:37.140
But if we take seriously
但如果我们每个人都认真对待

377
00:21:37.741 --> 00:21:43.301
how these systems that we depend on for so much operate,
这些我们一直以来 都在依赖的操作系统

378
00:21:43.936 --> 00:21:48.217
I don't see how we can postpone this conversation anymore.
我认为我们也 没有理由再拖延下去了

379
00:21:49.020 --> 00:21:51.556
These structures
这些结构

380
00:21:51.580 --> 00:21:55.676
are organizing how we function
在影响着我们的工作方式

381
00:21:55.700 --> 00:21:57.996
and they're controlling
它们同时也在控制

382
00:21:57.934 --> 00:21:59.899
what we can and we cannot do.
我们能做与不能做什么事情

383
00:22:00.660 --> 00:22:03.116
And many of these ad-financed platforms,
而许许多多的 这种以广告为生的平台

384
00:22:03.140 --> 00:22:04.716
they boast that they're free.
他们夸下海口 对大众分文不取

385
00:22:04.740 --> 00:22:09.300
In this context, it means that we are the product that's being sold.
而事实上 我们却是他们销售的产品

386
00:22:10.660 --> 00:22:13.396
We need a digital economy
我们需要一种数字经济

387
00:22:13.420 --> 00:22:16.916
where our data and our attention
一种我们的数据以及我们专注的信息

388
00:22:16.940 --> 00:22:22.020
is not for sale to the highest-bidding authoritarian or demagogue.
不会如竞拍一样被售卖给 出价最高的独裁者和煽动者

389
00:22:22.980 --> 00:22:26.780
(Applause)
（掌声）

390
00:22:30.300 --> 00:22:33.556
So to go back to that Hollywood paraphrase,
回到那句好莱坞名人说的话

391
00:22:33.580 --> 00:22:37.316
we do want the prodigious potential
我们的确想要

392
00:22:37.340 --> 00:22:40.540
of artificial intelligence and digital technology to blossom,
由人工智能与数字科技发展 带来的惊人潜力

393
00:22:41.220 --> 00:22:46.156
but for that, we must face this prodigious menace,
但与此同时 我们也要 做好面对惊人风险的准备

394
00:22:46.180 --> 00:22:48.116
open-eyed and now.
睁大双眼 就在此时此刻

395
00:22:48.140 --> 00:22:49.356
Thank you.
谢谢

396
00:22:49.380 --> 00:22:54.020
(Applause)
（掌声）