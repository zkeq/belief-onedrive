WEBVTT

1
00:00:12.820 --> 00:00:14.838
Around five years ago,
大约五年前，

2
00:00:14.862 --> 00:00:17.254
it struck me that I was losing the ability
我突然发现我在丧失

3
00:00:17.278 --> 00:00:19.778
to engage with people who aren't like-minded.
与立场不同的人交流的能力。

4
00:00:20.737 --> 00:00:24.379
The idea of discussing hot-button issues with my fellow Americans
单是跟美国同胞讨论争议性话题这个想法

5
00:00:24.403 --> 00:00:26.796
was starting to give me more heartburn
让我感到心累的程度，

6
00:00:26.820 --> 00:00:30.945
than the times that I engaged with suspected extremists overseas.
甚至就超过了我和海外可疑的极端主义者有过的交流。

7
00:00:31.570 --> 00:00:35.171
It was starting to leave me feeling more embittered and frustrated.
这开始让我感到更加痛苦和沮丧。

8
00:00:35.195 --> 00:00:36.629
And so just like that,
于是就这样，

9
00:00:36.653 --> 00:00:38.463
I shifted my entire focus
我把我的焦点

10
00:00:38.487 --> 00:00:40.921
from global national security threats
从影响全球国家安全的威胁，

11
00:00:40.945 --> 00:00:44.171
to trying to understand what was causing this push
转移到了寻找那个导致

12
00:00:44.195 --> 00:00:47.421
towards extreme polarization at home.
国内民众两极化的原因上。

13
00:00:47.445 --> 00:00:49.796
As a former CIA officer and diplomat
作为一名前中情局警官和外交官，

14
00:00:49.820 --> 00:00:53.254
who spent years working on counterextremism issues,
我在解决反极端主义的问题上有着多年的工作经验。

15
00:00:53.278 --> 00:00:57.129
I started to fear that this was becoming a far greater threat to our democracy
我开始担心，这对我们国家的民主造成的威胁

16
00:00:57.153 --> 00:00:59.671
than any foreign adversary.
会超过来自国外对手的威胁。

17
00:00:59.695 --> 00:01:01.504
And so I started digging in,
于是我开始深究，

18
00:01:01.528 --> 00:01:03.046
and I started speaking out,
开始发声，

19
00:01:03.070 --> 00:01:05.879
which eventually led me to being hired at Facebook
后来脸谱网聘请了我，

20
00:01:05.903 --> 00:01:08.588
and ultimately brought me here today
也最终引领我来到了这里，

21
00:01:08.612 --> 00:01:15.463
to continue warning you about how these platforms are manipulating and radicalizing so many of us
使我能继续警告各位，这些平台在如何操控和极端化我们的思想，

22
00:01:15.487 --> 00:01:18.445
and to talk about how to reclaim our public square.
并谈谈我们应该如何夺回自己的公共空间。

23
00:01:19.445 --> 00:01:21.421
I was a foreign service officer in Kenya
就在 911 事件发生的几年后，

24
00:01:21.445 --> 00:01:24.504
just a few years after the September 11 attacks,
我曾担任过肯尼亚外交官，

25
00:01:24.528 --> 00:01:27.088
and I led what some call "hearts and minds" campaigns
那时我在索马里的边境领导了一个叫

26
00:01:27.112 --> 00:01:29.046
along the Somalia border.
“心灵与精神”的项目。

27
00:01:29.070 --> 00:01:32.379
A big part of my job was to build trust with communities
我的主要工作是与那些最容易

28
00:01:32.403 --> 00:01:35.112
deemed the most susceptible to extremist messaging.
被极端主义思想影响的群体建立信任。

29
00:01:35.945 --> 00:01:40.129
I spent hours drinking tea with outspoken anti-Western clerics
我花了大量的时间与直言不讳的反西方教士喝茶，

30
00:01:40.153 --> 00:01:43.379
and even dialogued with some suspected terrorists,
甚至跟疑似恐怖份子的人进行过谈话，

31
00:01:43.403 --> 00:01:46.796
and while many of these engagements began with mutual suspicion,
虽然这些交谈都是从互相怀疑开始的，

32
00:01:46.820 --> 00:01:50.629
I don't recall any of them resulting in shouting or insults,
他们却从来没有大喊或说过任何辱骂的话，

33
00:01:50.653 --> 00:01:54.737
and in some case we even worked together on areas of mutual interest.
有些时候，我们甚至能在共同感兴趣的领域一起合作。

34
00:01:55.820 --> 00:01:59.963
The most powerful tools we had were to simply listen, learn
我们拥有的最有效的工具就是聆听、学习

35
00:01:59.987 --> 00:02:01.713
and build empathy.
还有建立同理心。

36
00:02:01.737 --> 00:02:04.796
This is the essence of hearts and minds work,
这就是“心灵与精神” 这份工作的核心，

37
00:02:04.820 --> 00:02:08.088
because what I found again and again is that what most people wanted
因为我多次发现，他们大多数人想要的

38
00:02:08.112 --> 00:02:11.879
was to feel heard, validated and respected.
无非是被聆听、被认可，还有被尊重。

39
00:02:11.903 --> 00:02:14.546
And I believe that's what most of us want.
我相信这也是我们大多数人想要的。

40
00:02:14.570 --> 00:02:17.879
So what I see happening online today is especially heartbreaking
可如今，网络上发生的一切却令我心碎，

41
00:02:17.903 --> 00:02:19.987
and a much harder problem to tackle.
也让我明白，这是一个很难解决的问题。

42
00:02:20.737 --> 00:02:24.504
We are being manipulated by the current information ecosystem
我们正被目前的信息网络操纵，

43
00:02:24.528 --> 00:02:28.296
entrenching so many of us so far into absolutism
陷入了绝对主义中，

44
00:02:28.320 --> 00:02:31.296
that compromise has become a dirty word.
在这里，“妥协”已经成为一个肮脏的字眼。

45
00:02:31.320 --> 00:02:32.629
Because right now,
因为当今，

46
00:02:32.653 --> 00:02:34.796
social media companies like Facebook
类似于脸谱网的社交媒体公司

47
00:02:34.820 --> 00:02:38.671
profit off of segmenting us and feeding us personalized content
通过为我们提供个性化的内容来获利，

48
00:02:38.695 --> 00:02:42.546
that both validates and exploits our biases.
而这些内容既认可，也利用了我们的偏见。

49
00:02:42.570 --> 00:02:46.004
Their bottom line depends on provoking a strong emotion
他们的底线就是，通过激起强烈的情绪

50
00:02:46.028 --> 00:02:47.713
to keep us engaged,
来让我们沉迷，

51
00:02:47.737 --> 00:02:52.004
often incentivizing the most inflammatory and polarizing voices,
持续强调那些煽性的，十分两极化的声音，

52
00:02:52.028 --> 00:02:56.463
to the point where finding common ground no longer feels possible.
直到找到共同点变得不再可能。

53
00:02:56.487 --> 00:03:00.713
And despite a growing chorus of people crying out for the platforms to change,
虽然越来越多的人开始要求这些平台做出改变，

54
00:03:00.737 --> 00:03:04.046
it's clear they will not do enough on their own.
但单靠它们是明显不够的。

55
00:03:04.070 --> 00:03:06.963
So governments must define the responsibility
所以，政府必须定义

56
00:03:06.987 --> 00:03:10.796
for the real-world harms being caused by these business models
这些对社会造成危害的商业模式需承担什么样的责任，

57
00:03:10.820 --> 00:03:18.921
and impose real costs on the damaging effects they're having to our public health, our public square and our democracy.
并让它们因严重损害我们的公共健康、公共空间，还有我们的民主付出代价。

58
00:03:18.945 --> 00:03:23.588
But unfortunately, this won't happen in time for the US presidential election,
但不幸的是，在美国大选之前这都不可能发生，

59
00:03:23.612 --> 00:03:26.379
so I am continuing to raise this alarm,
所以我只能继续发出警告，

60
00:03:26.403 --> 00:03:29.463
because even if one day we do have strong rules in place,
因为即使有一天我们有了强有力的法律，

61
00:03:29.487 --> 00:03:32.237
it will take all of us to fix this.
还是需要大家一起来解决问题。

62
00:03:33.237 --> 00:03:35.838
When I started shifting my focus from threats abroad
当我的注意力从海外威胁

63
00:03:35.862 --> 00:03:38.004
to the breakdown in civil discourse at home,
转移到已然破碎的国内民众对话时，

64
00:03:38.028 --> 00:03:44.213
I wondered if we could repurpose some of these hearts and minds campaigns to help heal our divides.
我曾想过，是否可以重新利用“心灵与精神”这个项目来缓解国内的分歧。

65
00:03:44.237 --> 00:03:48.046
Our more than 200-year experiment with democracy works
我们过去 200 多年的民主实验之所以有效，

66
00:03:48.070 --> 00:03:54.796
in large part because we are able to openly and passionately debate our ideas for the best solutions.
很大程度是因为我们可以公开并且热情地讨论想法，以找到最佳解决方案。

67
00:03:54.820 --> 00:03:56.629
But while I still deeply believe
我始终坚信，

68
00:03:56.653 --> 00:03:58.921
in the power of face-to-face civil discourse,
面对面的民间对话是有效的，

69
00:03:58.945 --> 00:04:00.588
it just cannot compete
但它确实难以对抗

70
00:04:00.612 --> 00:04:04.421
with the polarizing effects and scale of social media right now.
如今社交媒体带来的两极分化的效应和规模。

71
00:04:04.445 --> 00:04:06.796
The people who are sucked down these rabbit holes
与我曾经合作过的那些弱势群体相比，

72
00:04:06.820 --> 00:04:11.796
of social media outrage often feel far harder to break of their ideological mindsets
那些沉陷于社交媒体上激进内容的人

73
00:04:11.820 --> 00:04:15.296
than those vulnerable communities I worked with ever were.
更难脱离他们的固有观念。

74
00:04:15.320 --> 00:04:17.629
So when Facebook called me in 2018
所以在 2018 年，当脸谱网找到我，

75
00:04:17.653 --> 00:04:22.963
and offered me this role heading its elections integrity operations for political advertising,
并给我提供了这份负责政治广告管理的工作时，

76
00:04:22.987 --> 00:04:25.004
I felt I had to say yes.
我觉得我必须答应。

77
00:04:25.028 --> 00:04:27.754
I had no illusions that I would fix it all,
我知道我可能无法解决所有的问题，

78
00:04:27.778 --> 00:04:31.629
but when offered the opportunity to help steer the ship in a better direction,
但有了这个可以引导民众的机会，

79
00:04:31.653 --> 00:04:33.195
I had to at least try.
我必须至少尝试一下。

80
00:04:34.487 --> 00:04:36.754
I didn't work directly on polarization,
虽然我们没有研究过两极分化，

81
00:04:36.778 --> 00:04:41.213
but I did look at which issues were the most divisive in our society
但是我确实研究过哪些问题最容易引起社会分裂，

82
00:04:41.237 --> 00:04:45.046
and therefore the most exploitable in elections interference efforts,
在选举干扰中也最易被利用，

83
00:04:45.070 --> 00:04:47.653
which was Russia's tactic ahead of 2016.
这也是俄罗斯在 2016 年前的策略。

84
00:04:48.403 --> 00:04:50.754
So I started by asking questions.
所以我从提问开始。

85
00:04:50.778 --> 00:04:56.129
I wanted to understand the underlying systemic issues that were allowing all of this to happen,
我想找到导致这一切发生的潜在的系统性问题，

86
00:04:56.153 --> 00:04:58.237
in order to figure out how to fix it.
以便找到解决的方法。

87
00:04:59.445 --> 00:05:04.546
Now I still do believe in the power of the internet to bring more voices to the table,
现在，我仍然相信互联网的力量能让更多的声音参与进来，

88
00:05:04.570 --> 00:05:07.671
but despite their stated goal of building community,
尽管它们宣称的目标是建立社区，

89
00:05:07.695 --> 00:05:11.004
the largest social media companies as currently constructed
目前最大的社交媒体公司的构建方式

90
00:05:11.028 --> 00:05:14.629
are antithetical to the concept of reasoned discourse.
与理性言论的概念却是背道而驰的。

91
00:05:14.653 --> 00:05:18.796
There's no way to reward listening, to encourage civil debate
在一个把优化用户参与度和用户增长量

92
00:05:18.820 --> 00:05:22.463
and to protect people who sincerely want to ask questions
作为衡量成功的两大指标的行业里，

93
00:05:22.487 --> 00:05:25.838
in a business where optimizing engagement and user growth
你没有办法奖励倾听、鼓励民众讨论，

94
00:05:25.862 --> 00:05:29.088
are the two most important metrics for success.
你也没有办法去保护那些诚恳提问的人。

95
00:05:29.112 --> 00:05:32.546
There's no incentive to help people slow down,
没有任何激励因素去帮助人们慢下来，

96
00:05:32.570 --> 00:05:35.796
to build in enough friction that people have to stop,
没有去建立足够的摩擦让人们不得不停下来，

97
00:05:35.820 --> 00:05:38.421
recognize their emotional reaction to something,
认识到自己对某事的情绪化反应，

98
00:05:38.445 --> 00:05:41.112
and question their own assumptions before engaging.
并且在参与之前质疑自己的假设。

99
00:05:42.445 --> 00:05:44.421
The unfortunate reality is:
令人遗憾的事实是：

100
00:05:44.445 --> 00:05:51.088
lies are more engaging online than truth, and salaciousness beats out wonky, fact-based reasoning
在一个优化无摩擦的，病毒式传播的世界里，网络上的谎言比真相更吸引人，

101
00:05:51.112 --> 00:05:54.153
in a world optimized for frictionless virality.
淫秽信息胜过了基于事实的推理。

102
00:05:54.987 --> 00:05:58.421
As long as algorithms' goals are to keep us engaged,
只要算法的目标是保持我们的参与，

103
00:05:58.445 --> 00:06:02.504
they will continue to feed us the poison that plays to our worst instincts
它们就会继续“喂”迎合我们最糟糕的本能

104
00:06:02.528 --> 00:06:03.987
and human weaknesses.
和人性弱点的毒药。

105
00:06:04.778 --> 00:06:07.796
And yes, anger, mistrust,
当然，愤怒、不信任、

106
00:06:07.820 --> 00:06:09.546
the culture of fear, hatred:
恐惧和仇恨的文化：

107
00:06:09.570 --> 00:06:12.421
none of this is new in America.
这些在美国都不是新事物。

108
00:06:12.445 --> 00:06:15.796
But in recent years, social media has harnessed all of that
但近年来，社交软件充分利用了这一切，

109
00:06:15.820 --> 00:06:19.421
and, as I see it, dramatically tipped the scales.
并且在我看来，还起到了决定性的作用。

110
00:06:19.445 --> 00:06:21.838
And Facebook knows it.
脸谱网也深谙这一点。

111
00:06:21.862 --> 00:06:23.754
A recent "Wall Street Journal" article
《华尔街日报》最近的一篇文章

112
00:06:23.778 --> 00:06:27.921
exposed an internal Facebook presentation from 2018
披露的一项脸谱网 2018 年的内部演示中，

113
00:06:27.945 --> 00:06:31.713
that specifically points to the companies' own algorithms
特别提到了该公司自己的算法

114
00:06:31.737 --> 00:06:35.254
for growing extremist groups' presence on their platform
让越来越多的极端组织信息出现在其平台上，

115
00:06:35.278 --> 00:06:37.445
and for polarizing their users.
并导致了用户两极分化。

116
00:06:38.528 --> 00:06:42.213
But keeping us engaged is how they make their money.
但让我们参与其中正是它们赚钱的方式。

117
00:06:42.237 --> 00:06:46.463
The modern information environment is crystallized around profiling us
现代信息环境是通过我们的信息画像而构建的，

118
00:06:46.487 --> 00:06:49.463
and then segmenting us into more and more narrow categories
再将我们分割成越来越细小的类别，

119
00:06:49.487 --> 00:06:52.796
to perfect this personalization process.
从而完善这个个性化的过程。

120
00:06:52.820 --> 00:06:56.629
We're then bombarded with information confirming our views,
然后，我们被可以证实我们观点的信息狂轰滥炸，

121
00:06:56.653 --> 00:06:58.504
reinforcing our biases,
强化我们的偏见，

122
00:06:58.528 --> 00:07:02.046
and making us feel like we belong to something.
并且让我们感觉属于某类群体。

123
00:07:02.070 --> 00:07:07.671
These are the same tactics we would see terrorist recruiters using on vulnerable youth,
这些策略和那些恐怖分子用于招募弱势青少年的伎俩如出一辙，

124
00:07:07.695 --> 00:07:11.546
albeit in smaller, more localized ways before social media,
尽管是用比社交媒体更小、更本土化的方式，

125
00:07:11.570 --> 00:07:14.838
with the ultimate goal of persuading their behavior.
但最终目标都是灌输某种行为。

126
00:07:14.862 --> 00:07:20.129
Unfortunately, I was never empowered by Facebook to have an actual impact.
不幸的是，我从未被脸谱网授权去产生实质影响。

127
00:07:20.153 --> 00:07:23.921
In fact, on my second day, my title and job description were changed
事实上，在我工作的第二天，我的头衔和具体工作就发生了变化，

128
00:07:23.945 --> 00:07:26.921
and I was cut out of decision-making meetings.
并且被排除在决策会议外。

129
00:07:26.945 --> 00:07:28.254
My biggest efforts,
我最大的努力，

130
00:07:28.278 --> 00:07:34.879
trying to build plans to combat disinformation and voter suppression in political ads, were rejected.
试图制定计划，以打击政治广告中的虚假信息和投票权压制被拒绝了。

131
00:07:34.903 --> 00:07:37.403
And so I lasted just shy of six months.
所以我只坚持了不到 6 个月就辞职了。

132
00:07:37.945 --> 00:07:41.546
But here is my biggest takeaway from my time there.
但这是我在这段期间最大的收获。

133
00:07:41.570 --> 00:07:44.046
There are thousands of people at Facebook
成千上万的人在脸谱网工作，

134
00:07:44.070 --> 00:07:49.796
who are passionately working on a product that they truly believe makes the world a better place,
充满热情的投入于这个他们相信能将世界变得更美好的产品，

135
00:07:49.820 --> 00:07:56.254
but as long as the company continues to merely tinker around the margins of content policy and moderation,
但是，只要公司依然仅仅只是在内容政策和合理性的边缘试探，

136
00:07:56.278 --> 00:08:00.838
as opposed to considering how the entire machine is designed and monetized,
而不考虑整个平台的设计和赚钱方式，

137
00:08:00.862 --> 00:08:08.421
they will never truly address how the platform is contributing to hatred, division and radicalization.
那它们永远不会真正解决平台是如何助长仇恨、分裂和激进这些问题的。

138
00:08:08.445 --> 00:08:12.713
And that's the one conversation I never heard happen during my time there,
而这也正是我在那里从未听到过的对话，

139
00:08:12.737 --> 00:08:15.796
because that would require fundamentally accepting
因为那将会需要你从根本上接受

140
00:08:15.820 --> 00:08:19.963
that the thing you built might not be the best thing for society
你所创造的东西可能不是对社会最有益的东西，

141
00:08:19.987 --> 00:08:23.112
and agreeing to alter the entire product and profit model.
并同意改变整个产品和盈利模式。

142
00:08:24.320 --> 00:08:26.153
So what can we do about this?
那么我们能做些什么呢？

143
00:08:26.862 --> 00:08:32.629
I'm not saying that social media bears the sole responsibility for the state that we're in today.
我并不是说社交媒体要为我们国家的现状承担唯一的责任。

144
00:08:32.653 --> 00:08:38.088
Clearly, we have deep-seated societal issues that we need to solve.
显然，我们有一些根深蒂固的社会问题需要解决。

145
00:08:38.112 --> 00:08:42.254
But Facebook's response, that it is just a mirror to society,
但脸谱网的回应只是社会的一面镜子，

146
00:08:42.278 --> 00:08:49.588
is a convenient attempt to deflect any responsibility from the way their platform is amplifying harmful content
这是一种转移其平台放大有害言论，

147
00:08:47.619 --> 00:08:52.653
and pushing some users towards extreme views.
并将一些用户推向极端观点的责任的方便说辞。

148
00:08:53.403 --> 00:08:58.129
And Facebook could, if they wanted to, fix some of this.
当然，如果脸谱网的确有这方面的主观意愿，是可以解决其中一些问题的。

149
00:08:58.153 --> 00:09:02.004
They could stop amplifying and recommending the conspiracy theorists,
他们可以停止推荐和放大阴谋理论家、

150
00:09:02.028 --> 00:09:04.713
the hate groups, the purveyors of disinformation
仇恨组织和虚假信息散布者的影响，

151
00:09:04.737 --> 00:09:08.588
and, yes, in some cases even our president.
是的，在某些情况下，甚至包括我们的总统。

152
00:09:08.612 --> 00:09:16.254
They could stop using the same personalization techniques to deliver political rhetoric that they use to sell us sneakers.
它们可以停止使用与推销跑鞋同样的个性化推荐服务来传递政治说辞。

153
00:09:16.278 --> 00:09:20.213
They could retrain their algorithms to focus on a metric other than engagement,
它们可以重新修改算法，去专注于一个非用户互动类的度量指标，

154
00:09:20.237 --> 00:09:24.546
and they could build in guardrails to stop certain content from going viral
也可以建立信息护栏，防止某些内容在被审核之前

155
00:09:24.570 --> 00:09:26.237
before being reviewed.
像病毒一样被传播。

156
00:09:26.695 --> 00:09:28.963
And they could do all of this
他们可以做到所有这一切。

157
00:09:28.987 --> 00:09:33.171
without becoming what they call the arbiters of truth.
而不必成为他们所谓的 “真理仲裁者”。

158
00:09:33.195 --> 00:09:36.004
But they've made it clear that they will not go far enough
但是他们已经明确表示，除非被强制这样做，

159
00:09:36.028 --> 00:09:38.838
to do the right thing without being forced to,
否则他们就不会积极采取正确的行动，

160
00:09:38.862 --> 00:09:41.796
and, to be frank, why should they?
毕竟，坦白说，他们为什么要这么做呢？

161
00:09:41.820 --> 00:09:45.754
The markets keep rewarding them, and they're not breaking the law.
市场一直在奖励他们，他们也没有违法。

162
00:09:45.778 --> 00:09:47.088
Because as it stands,
因为就现在的情况而言，

163
00:09:47.112 --> 00:09:52.004
there are no US laws compelling Facebook, or any social media company,
美国并没有法律强制脸谱网或者其他社交媒体公司

164
00:09:52.028 --> 00:09:53.796
to protect our public square,
去保护我们的公共广场、

165
00:09:53.820 --> 00:09:55.129
our democracy
民主，

166
00:09:55.153 --> 00:09:57.588
and even our elections.
甚至是我们的选举。

167
00:09:57.612 --> 00:10:04.820
We have ceded the decision-making on what rules to write and what to enforce to the CEOs of for-profit internet companies.
我们已经将制定和执行哪些计划的决定权交给了盈利性互联网公司的首席执行官们。

168
00:10:05.570 --> 00:10:07.588
Is this what we want?
这是我们想要的吗？

169
00:10:07.612 --> 00:10:10.838
A post-truth world where toxicity and tribalism
一个有毒言论和部落主义

170
00:10:10.862 --> 00:10:13.487
trump bridge-building and consensus-seeking?
彻底压倒打造沟通桥梁和追求共识的后真相世界？

171
00:10:14.487 --> 00:10:22.254
I do remain optimistic that we still have more in common with each other than the current media and online environment portray,
我仍然乐观的认为，我们要比现存的媒体和网络环境所描绘的彼此拥有更多的共同点，

172
00:10:22.278 --> 00:10:25.379
and I do believe that having more perspective surface
我也坚信相信，拥有更多视角

173
00:10:25.403 --> 00:10:29.004
makes for a more robust and inclusive democracy.
会让民主更加健康和包容，

174
00:10:29.028 --> 00:10:31.838
But not the way it's happening right now.
但不是以目前的方式。

175
00:10:31.862 --> 00:10:36.088
And it bears emphasizing, I do not want to kill off these companies.
需要强调的是，我并不想搞垮这些公司。

176
00:10:36.112 --> 00:10:39.338
I just want them held to a certain level of accountability,
我只想让它们承担一定程度的责任，

177
00:10:39.362 --> 00:10:41.278
just like the rest of society.
就像社会中的其他人一样。

178
00:10:42.362 --> 00:10:48.379
It is time for our governments to step up and do their jobs of protecting our citizenry.
我们的政府是时候站出来做好保护我们公民的工作了。

179
00:10:48.403 --> 00:10:52.963
And while there isn't one magical piece of legislation that will fix this all,
虽然还没有一项神奇的立法可以解决所有的问题，

180
00:10:52.987 --> 00:10:59.838
I do believe that governments can and must find the balance between protecting free speech
但我的确相信，政府可以，而且必须在保护自由言论和让这些平台

181
00:10:59.862 --> 00:11:04.171
and holding these platforms accountable for their effects on society.
为它们的社会影响负责之间找到平衡。

182
00:11:04.195 --> 00:11:08.504
And they could do so in part by insisting on actual transparency
他们可以通过保证这些推荐引擎如何工作，

183
00:11:08.528 --> 00:11:16.338
around how these recommendation engines are working, around how the curation, amplification and targeting are happening.
如何进行管理、放大和定位的透明度来做到这一点。

184
00:11:16.362 --> 00:11:18.796
You see, I want these companies held accountable
我希望这些公司

185
00:11:18.820 --> 00:11:21.921
not for if an individual posts misinformation
不仅是对个人发布虚假信息

186
00:11:21.945 --> 00:11:23.421
or extreme rhetoric,
或者极端言论负责，

187
00:11:23.445 --> 00:11:27.296
but for how their recommendation engines spread it,
还要对他们的推荐引擎如何传播这些信息，

188
00:11:27.320 --> 00:11:30.546
how their algorithms are steering people towards it,
他们的算法如何引导人们使用这些信息，

189
00:11:30.570 --> 00:11:33.862
and how their tools are used to target people with it.
以及他们的工具如何被用来针对这些人负责。

190
00:11:35.237 --> 00:11:38.921
I tried to make change from within Facebook and failed,
我试图从脸谱网内部做出改变，但是我失败了，

191
00:11:38.945 --> 00:11:44.338
and so I've been using my voice again for the past few years to continue sounding this alarm
所以在过去几年，我一直在用自己的声音不断发出警告，

192
00:11:44.362 --> 00:11:48.754
and hopefully inspire more people to demand this accountability.
希望能激发更多的人来要求这种责任。

193
00:11:48.778 --> 00:11:51.421
My message to you is simple:
我要传递给你们的信息很简单：

194
00:11:51.445 --> 00:11:53.629
pressure your government representatives
对你们的政府代表施压，让他们站出来，

195
00:11:53.653 --> 00:11:59.088
to step up and stop ceding our public square to for-profit interests.
停止将我们的公共空间拱手让给以营利为目的获利行为。

196
00:11:59.112 --> 00:12:00.963
Help educate your friends and family
帮助你的朋友和家人了解

197
00:12:00.987 --> 00:12:04.129
about how they're being manipulated online.
他们在网上是如何被操纵的。

198
00:12:04.153 --> 00:12:07.671
Push yourselves to engage with people who aren't like-minded.
推动自己和没有共同想法的人交流。

199
00:12:07.695 --> 00:12:10.338
Make this issue a priority.
把这个问题当作优先事项。

200
00:12:10.362 --> 00:12:13.778
We need a whole-society approach to fix this.
我们需要一个能动员全社会的方法来解决它。

201
00:12:15.070 --> 00:12:20.629
And my message to the leaders of my former employer Facebook is this:
我对我的前雇主，脸谱网的领导们想要说的是：

202
00:12:20.653 --> 00:12:26.463
right now, people are using your tools exactly as they were designed
现在，人们正在使用你们设计好的工具

203
00:12:26.487 --> 00:12:29.088
to sow hatred, division and distrust,
传播仇恨、分裂和不信任，

204
00:12:29.112 --> 00:12:33.254
and you're not just allowing it, you are enabling it.
而你们不仅允许了，而且还在加剧这种情况。

205
00:12:33.278 --> 00:12:35.754
And yes, there are lots of great stories
没错，在全球范围内

206
00:12:35.778 --> 00:12:39.463
of positive things happening on your platform around the globe,
你们的平台上发生了很多积极的事，

207
00:12:39.487 --> 00:12:42.713
but that doesn't make any of this OK.
但这并不意味着这一切都是可以接受的。

208
00:12:42.737 --> 00:12:45.713
And it's only getting worse as we're heading into our election,
随着选举的临近，情况越来越糟，

209
00:12:45.737 --> 00:12:47.504
and even more concerning,
更值得担忧的是，

210
00:12:47.528 --> 00:12:50.004
face our biggest potential crisis yet,
如果选举结果不被信任，如果暴力事件爆发，

211
00:12:50.028 --> 00:12:54.171
if the results aren't trusted, and if violence breaks out.
那么迄今为止最大的潜在危机将不可避免。

212
00:12:54.195 --> 00:12:59.213
So when in 2021 you once again say, "We know we have to do better,"
所以到 2021 年，当你们再说，“我们知道我们必须做得更好”时，

213
00:12:59.237 --> 00:13:01.921
I want you to remember this moment,
我想要你们记住现在这一刻，

214
00:13:01.945 --> 00:13:05.171
because it's no longer just a few outlier voices.
因为这再也不只是一些局外人的声音。

215
00:13:05.195 --> 00:13:07.463
Civil rights leaders, academics,
民权领袖、学者、

216
00:13:07.487 --> 00:13:10.463
journalists, advertisers, your own employees,
记者、广告商和你们自己的员工

217
00:13:10.487 --> 00:13:12.546
are shouting from the rooftops
都会在屋顶上大声呼吁，

218
00:13:12.570 --> 00:13:15.171
that your policies and your business practices
你们的政策和商业行为

219
00:13:15.195 --> 00:13:17.612
are harming people and democracy.
一直都在危害人民和民主。

220
00:13:18.695 --> 00:13:21.046
You own your decisions,
决定权在你们手中，

221
00:13:21.070 --> 00:13:24.737
but you can no longer say that you couldn't have seen it coming.
但是你们再也不能说，你们不可能预见这样的后果。

222
00:13:25.987 --> 00:13:27.237
Thank you.
谢谢。