WEBVTT

1
00:00:12.751 --> 00:00:15.088
Chris Anderson: Christiane, great to have you here.
克里斯·安德森：克莉丝蒂安， 很高兴你今天能来。

2
00:00:14.622 --> 00:00:16.805
So you've had this amazing viewpoint,
你有个很棒的观点，

3
00:00:16.979 --> 00:00:20.039
and perhaps it's fair to say that in the last few years,
也许可以这么说， 在过去的几年间，

4
00:00:19.993 --> 00:00:23.216
there have been some alarming developments that you're seeing.
你目睹了一些令人惊讶的发展。

5
00:00:23.840 --> 00:00:25.404
What's alarmed you most?
你最惊讶的是什么？

6
00:00:25.428 --> 00:00:28.620
Christiane Amanpour: Well, just listening to the earlier speakers,
克莉丝蒂安·阿曼普： 听了前几位讲者的演说，

7
00:00:28.644 --> 00:00:31.116
I can frame it in what they've been saying:
我可以用他们的话来说：

8
00:00:31.140 --> 00:00:34.562
climate change, for instance -- cities, the threat to our environment
气候改变，比如说—— 城市，对我们环境的威胁

9
00:00:34.586 --> 00:00:35.846
and our lives.
以及对我们的生命的威胁。

10
00:00:36.260 --> 00:00:40.154
It basically also boils down to understanding the truth
这基本上是说如果我们想要真正解决它，

11
00:00:40.178 --> 00:00:43.189
and to be able to get to the truth of what we're talking about
我们就要去理解事实，

12
00:00:43.213 --> 00:00:45.305
in order to really be able to solve it.
要能够从我们的 谈话内容中探究出真相。

13
00:00:45.329 --> 00:00:49.256
So if 99.9 percent of the science on climate
因此假如在气候科学上，99.9%的证据

14
00:00:49.280 --> 00:00:52.337
is empirical, scientific evidence,
是实证性的，科学的证据，

15
00:00:52.361 --> 00:00:57.256
but it's competing almost equally with a handful of deniers,
但是仍充斥着等量的反面否认信息，

16
00:00:57.280 --> 00:00:58.507
that is not the truth;
但那不是事实；

17
00:00:58.531 --> 00:01:01.029
that is the epitome of fake news.
那是虚假信息的缩影。

18
00:01:00.963 --> 00:01:05.905
And so for me, the last few years -- certainly this last year --
因此在我看来，过去的几年—— 尤其是去年——

19
00:01:06.179 --> 00:01:10.439
has crystallized the notion of fake news in a way that's truly alarming
正以一种真正令人震惊的方式 将虚假新闻的定义清晰化，

20
00:01:10.463 --> 00:01:13.122
and not just some slogan to be thrown around.
而不仅仅是漫天抛出 一些空洞的标语。

21
00:01:13.146 --> 00:01:16.957
Because when you can't distinguish between the truth and fake news,
因为当你不能区分 事实和虚假新闻的时候，

22
00:01:16.981 --> 00:01:20.872
you have a very much more difficult time trying to solve
你会在尝试去解决 一些我们所面临的重大问题时，

23
00:01:20.896 --> 00:01:23.347
some of the great issues that we face.
经历更大的困难。

24
00:01:24.332 --> 00:01:27.753
CA: Well, you've been involved in this question of,
CA：你参与到一些问题，

25
00:01:27.777 --> 00:01:30.689
what is balance, what is truth, what is impartiality,
例如什么是平衡，什么是真实， 什么是公正，

26
00:01:30.713 --> 00:01:31.968
for a long time.
有很长时间了。

27
00:01:31.992 --> 00:01:37.862
You were on the front lines reporting the Balkan Wars 25 years ago.
25年前你在巴尔干战争的前线做报道。

28
00:01:37.886 --> 00:01:41.298
And back then, you famously said,
那个时候，通过呼吁对人权侵犯的关注，

29
00:01:41.322 --> 00:01:43.943
by calling out human right abuses,
你说了一句很有名的话，

30
00:01:43.967 --> 00:01:48.296
you said, "Look, there are some situations one simply cannot be neutral about,
你说：“看，有一些情况人们 无法对其持中立态度，

31
00:01:48.320 --> 00:01:49.700
because when you're neutral,
因为当你中立的时候，

32
00:01:49.724 --> 00:01:51.611
you are an accomplice."
你便成为帮凶。”

33
00:01:53.063 --> 00:01:57.960
So, do you feel that today's journalists aren't heeding that advice
那么，你觉得如今的记者 没有在留意

34
00:01:57.984 --> 00:01:59.456
about balance?
那个关于平衡的建议？

35
00:01:59.480 --> 00:02:03.586
CA: Well, look, I think for journalists, objectivity is the golden rule.
CA：是这样的， 我觉得身为记者，客观是黄金法则。

36
00:02:03.610 --> 00:02:08.026
But I think sometimes we don't understand what objectivity means.
但是我认为有时候 我们不明白客观意味着什么。

37
00:02:07.990 --> 00:02:10.859
And I actually learned this very, very young in my career,
事实上在我事业的初期， 我就认识到了，

38
00:02:10.988 --> 00:02:12.244
which was during the Balkan Wars.
也就是在巴尔干战争期间。

39
00:02:12.664 --> 00:02:13.880
I was young then.
那是我很年轻。

40
00:02:13.904 --> 00:02:16.443
It was about 25 years ago.
大约25年前。

41
00:02:16.467 --> 00:02:22.248
And what we faced was the wholesale violation, not just of human rights,
我们所面临的是全面的侵犯， 不仅是人权侵犯，

42
00:02:22.272 --> 00:02:25.251
but all the way to ethnic cleansing and genocide,
而是种族清洗和种族灭绝。

43
00:02:25.275 --> 00:02:29.281
and that has been adjudicated in the highest war crimes court
那已经在世界的

44
00:02:29.305 --> 00:02:30.469
in the world.
最高战争犯罪法庭做了裁决。

45
00:02:29.893 --> 00:02:31.960
So, we know what we were seeing.
所以我们了解我们所看到的。

46
00:02:31.960 --> 00:02:34.671
Trying to tell the world what we were seeing
试着告诉全世界 我们所看到的，

47
00:02:34.731 --> 00:02:37.506
brought us accusations of bias,
给我们带来了偏见的指责，

48
00:02:37.530 --> 00:02:39.419
of siding with one side,
比如片面性，

49
00:02:39.443 --> 00:02:41.305
of not seeing the whole side,
没有看到整体，

50
00:02:41.329 --> 00:02:43.626
and just, you know, trying to tell one story.
仅仅是尝试着 去述说一个故事。

51
00:02:43.650 --> 00:02:47.957
I particularly and personally was accused of siding with,
我尤其是被个人地指责站在，

52
00:02:47.981 --> 00:02:49.963
for instance, the citizens of Sarajevo --
比如就像，塞拉耶佛的人们——

53
00:02:49.987 --> 00:02:51.414
"siding with the Muslims,"
"站在穆斯林这一边，"

54
00:02:51.438 --> 00:02:54.490
because they were the minority who were being attacked
因为他们是被塞尔维亚

55
00:02:54.514 --> 00:02:58.252
by Christians on the Serb side
那边的基督徒

56
00:02:58.276 --> 00:02:59.995
in this area.
攻击的少数群体。

57
00:03:00.019 --> 00:03:01.361
And it worried me.
这让我很担心。

58
00:03:01.385 --> 00:03:03.576
It worried me that I was being accused of this.
我担心我因为这个受到指责。

59
00:03:03.600 --> 00:03:04.942
I thought maybe I was wrong,
我想过也许我错了，

60
00:03:04.966 --> 00:03:07.314
maybe I'd forgotten what objectivity was.
也许我忘记了什么是客观。

61
00:03:07.338 --> 00:03:10.345
But then I started to understand that what people wanted
然后我开始明白人们想要的，

62
00:03:10.369 --> 00:03:12.167
was actually not to do anything --
实际上是不采取 任何行动——

63
00:03:12.191 --> 00:03:13.608
not to step in,
不要深入涉及，

64
00:03:13.632 --> 00:03:15.202
not to change the situation,
不要去改变局势，

65
00:03:14.966 --> 00:03:16.495
not to find a solution.
不要去找到解决方法。

66
00:03:16.699 --> 00:03:19.052
And so, their fake news at that time,
如此一来，在那时候的虚假新闻，

67
00:03:18.816 --> 00:03:20.148
their lie at that time --
那时候人们的谎言——

68
00:03:20.482 --> 00:03:24.012
including our government's, our democratically elected government's,
包括我们政府的， 我们民主选举出的政府，

69
00:03:24.036 --> 00:03:26.308
with values and principles of human rights --
有着人权价值观 和原则的政府——

70
00:03:26.332 --> 00:03:29.839
their lie was to say that all sides are equally guilty,
他们的谎言是说 所有的立场都同等地有罪，

71
00:03:29.863 --> 00:03:32.656
that this has been centuries of ethnic hatred,
种族的仇恨延续了数百年，

72
00:03:32.680 --> 00:03:34.562
whereas we knew that wasn't true,
然而我们知道那是不真实的，

73
00:03:34.586 --> 00:03:38.233
that one side had decided to kill, slaughter and ethnically cleanse
即一方决定去杀害 屠杀和种族清洗

74
00:03:38.257 --> 00:03:39.414
another side.
另一方。

75
00:03:39.438 --> 00:03:40.934
So that is where, for me,
对于我来说，那个时候，

76
00:03:40.958 --> 00:03:46.264
I understood that objectivity means giving all sides an equal hearing
我明白了客观意味着 给予所有的立场同等的听证机会，

77
00:03:46.288 --> 00:03:48.393
and talking to all sides,
和所有立场方交谈，

78
00:03:48.417 --> 00:03:52.039
but not treating all sides equally,
而不是同等地对待所有立场，

79
00:03:51.863 --> 00:03:56.775
not creating a forced moral equivalence or a factual equivalence.
不是去创造一个强迫的道德平等性， 或是一个事实的平等性。

80
00:03:56.875 --> 00:04:01.354
And when you come up against that crisis point
当你站在那个危机点的 反对立场，

81
00:04:01.378 --> 00:04:07.049
in situations of grave violations of international and humanitarian law,
站在对国际法和人权法的 严重侵犯的立场的时候，

82
00:04:07.073 --> 00:04:09.415
if you don't understand what you're seeing,
如果你不明白你所看到的，

83
00:04:09.439 --> 00:04:11.599
if you don't understand the truth
如果你不明白事实的真相，

84
00:04:11.623 --> 00:04:15.136
and if you get trapped in the fake news paradigm,
当你陷入在虚假新闻的 模式里面的时候，

85
00:04:15.160 --> 00:04:16.750
then you are an accomplice.
那么你成为了一个同谋。

86
00:04:17.478 --> 00:04:20.475
And I refuse to be an accomplice to genocide.
我拒绝成为种族灭绝的同谋。

87
00:04:20.499 --> 00:04:23.782
(Applause)
（观众掌声）

88
00:04:26.222 --> 00:04:29.000
CH: So there have always been these propaganda battles,
CH: 这种宣传战争的行为总在进行着，

89
00:04:28.774 --> 00:04:33.050
and you were courageous in taking the stand you took back then.
而你在是很勇敢地 站在你选择的立场。

90
00:04:33.472 --> 00:04:37.199
Today, there's a whole new way, though,
然而现今，我们有全新的方式，

91
00:04:37.223 --> 00:04:39.427
in which news seems to be becoming fake.
新闻似乎在变得虚假。

92
00:04:39.451 --> 00:04:41.085
How would you characterize that?
你如何将它们特征化？

93
00:04:40.769 --> 00:04:43.117
CA: Well, look -- I am really alarmed.
CA: 首先——我是非常警觉的。

94
00:04:43.217 --> 00:04:45.419
And everywhere I look,
我所看到的每处，

95
00:04:45.443 --> 00:04:47.280
you know, we're buffeted by it.
都受到了一定的打击。

96
00:04:47.304 --> 00:04:49.506
Obviously, when the leader of the free world,
显然，当自由世界的领导，

97
00:04:49.530 --> 00:04:52.003
when the most powerful person in the entire world,
当整个世界最有力量的人，

98
00:04:51.957 --> 00:04:53.877
which is the president of the United States --
那是美国的总统——

99
00:04:53.877 --> 00:04:58.960
this is the most important, most powerful country in the whole world,
而这是全世界最重要的， 最有力量的国家，

100
00:04:58.960 --> 00:05:03.380
economically, militarily, politically in every which way --
在经济上，军事上，政治上， 在每一方面——

101
00:05:04.235 --> 00:05:09.252
and it seeks to, obviously, promote its values and power around the world.
而且它很显然地在世界范围内 不断推行它的价值观。

102
00:05:09.276 --> 00:05:13.252
So we journalists, who only seek the truth --
那么我们作为记者， 只是寻求真相——

103
00:05:13.276 --> 00:05:14.797
I mean, that is our mission --
我是说，那是我们的使命——

104
00:05:14.821 --> 00:05:16.938
we go around the world looking for the truth
我们在全世界范围内寻找真相，

105
00:05:16.962 --> 00:05:18.935
in order to be everybody's eyes and ears,
以此来成为每个人的 目击者和聆听者，

106
00:05:18.959 --> 00:05:21.478
people who can't go out in various parts of the world
为那些不能够去到世界不同地方的人，

107
00:05:20.932 --> 00:05:24.285
to figure out what's going on about things that are vitally important
去找寻对每个人的健康和安全

108
00:05:24.895 --> 00:05:26.851
to everybody's health and security.
极其重要的真相。

109
00:05:26.875 --> 00:05:33.561
So when you have a major world leader accusing you of fake news,
所以当一个重要的世界领导人 指责你制造虚假新闻的时候，

110
00:05:33.585 --> 00:05:37.428
it has an exponential ripple effect.
这有着巨大的连锁反应。

111
00:05:37.452 --> 00:05:41.724
And what it does is, it starts to chip away
它开始削减，

112
00:05:42.292 --> 00:05:45.180
at not just our credibility,
不仅仅是对我们的信任度，

113
00:05:45.204 --> 00:05:47.233
but at people's minds --
而且还有人们的大脑——

114
00:05:47.752 --> 00:05:49.890
people who look at us, and maybe they're thinking,
当人们看着我们的时候， 也许他们在想，

115
00:05:49.890 --> 00:05:52.993
"Well, if the president of the United States says that,
“如果美国总统那样说了，

116
00:05:52.993 --> 00:05:55.088
maybe somewhere there's a truth in there."
也许某处存在着几分事实。”

117
00:05:55.968 --> 00:06:00.152
CH: Presidents have always been critical of the media --
CH: 总统们总是对 媒体存在批判的——

118
00:06:00.176 --> 00:06:01.777
CA: Not in this way.
CA: 不是以这个方式。

119
00:06:01.801 --> 00:06:03.306
CH: So, to what extent --
CH: 那么到什么程度上——

120
00:06:03.330 --> 00:06:04.394
(Laughter)
（观众笑声）

121
00:06:04.418 --> 00:06:07.538
(Applause)
（观众掌声）

122
00:06:07.562 --> 00:06:14.458
CH: I mean, someone a couple years ago looking at the avalanche of information
CH: 我是说，几年前看着这雪崩般的信息

123
00:06:14.482 --> 00:06:17.718
pouring through Twitter and Facebook and so forth,
涌现在推特和脸书上的时候，

124
00:06:17.742 --> 00:06:18.900
might have said,
人们也许会说，

125
00:06:18.924 --> 00:06:21.765
"Look, our democracies are healthier than they've ever been.
“看，我们的民主变得前所未有的健康。

126
00:06:20.899 --> 00:06:23.014
There's more news than ever.
我们有前所未有的大量的信息。

127
00:06:23.334 --> 00:06:25.545
Of course presidents will say what they'll say,
当然总统会说他们想要说的，

128
00:06:25.569 --> 00:06:27.802
but everyone else can say what they will say.
但其他人可以说他们想要说的。

129
00:06:27.826 --> 00:06:31.981
What's not to like? How is there an extra danger?"
那么区别是什么？ 为什么这就有了额外的危险？”

130
00:06:31.925 --> 00:06:33.547
CA: So, I wish that was true.
CA: 我希望那是真实的。

131
00:06:34.812 --> 00:06:40.905
I wish that the proliferation of platforms upon which we get our information
我希望我们获取信息的平台的增加

132
00:06:40.929 --> 00:06:44.807
meant that there was a proliferation of truth and transparency
意味着事实和透明度的增加，

133
00:06:44.831 --> 00:06:46.699
and depth and accuracy.
以及深度和准确性的增加。

134
00:06:46.723 --> 00:06:49.178
But I think the opposite has happened.
但我认为相反的情况发生了。

135
00:06:49.202 --> 00:06:51.292
You know, I'm a little bit of a Luddite,
你知道，我有一点路德主义，

136
00:06:51.316 --> 00:06:52.512
I will confess.
我承认这一点。

137
00:06:52.967 --> 00:06:56.351
Even when we started to talk about the information superhighway,
即使当我们很久之前开始谈论

138
00:06:56.375 --> 00:06:58.003
which was a long time ago,
信息高速公路的时候，

139
00:06:57.947 --> 00:07:00.312
before social media, Twitter and all the rest of it,
那在社交网络之前，推特和 所有其它这些东西之前，

140
00:07:00.702 --> 00:07:02.526
I was actually really afraid
我实际上是真的很害怕

141
00:07:02.550 --> 00:07:06.571
that that would put people into certain lanes and tunnels
那将会把人们置身于 特定的道路和隧道里面，

142
00:07:06.595 --> 00:07:10.937
and have them just focusing on areas of their own interest
使得他们仅仅专注在 他们自己感兴趣的的领域，

143
00:07:10.961 --> 00:07:13.294
instead of seeing the broad picture.
而看不到更广的画面。

144
00:07:13.318 --> 00:07:17.904
And I'm afraid to say that with algorithms, with logarithms,
我害怕地说 按照那种算法，

145
00:07:17.928 --> 00:07:19.576
with whatever the "-ithms" are
无论是以什么方式，

146
00:07:19.600 --> 00:07:23.866
that direct us into all these particular channels of information,
那将领导我们去到所有这些 特定的信息渠道，

147
00:07:23.890 --> 00:07:25.760
that seems to be happening right now.
那似乎是现在正在发生的。

148
00:07:25.784 --> 00:07:28.328
I mean, people have written about this phenomenon.
我是说，人们描写了这些现象。

149
00:07:28.352 --> 00:07:30.550
People have said that yes, the internet came,
人们说，是的， 网络世纪来临，

150
00:07:30.574 --> 00:07:36.317
its promise was to exponentially explode our access to more democracy,
它带来的是大量的对于更加 民主的获取，

151
00:07:36.341 --> 00:07:38.055
more information,
更多的信息，

152
00:07:38.079 --> 00:07:39.971
less bias,
更少的偏见，

153
00:07:39.995 --> 00:07:42.384
more varied information.
更多样化的信息。

154
00:07:42.408 --> 00:07:44.733
And, in fact, the opposite has happened.
实际上，相反的情况发生了。

155
00:07:44.757 --> 00:07:48.775
And so that, for me, is incredibly dangerous.
那对于我来说是相当危险的。

156
00:07:48.799 --> 00:07:53.314
And again, when you are the president of this country and you say things,
回到那一点上，当你是这个国家的总统， 你在说一些事情的时候，

157
00:07:53.338 --> 00:07:58.763
it also gives leaders in other undemocratic countries the cover
它同时也在为其它 不民主的国家做掩护，

158
00:07:59.829 --> 00:08:02.135
to affront us even worse,
去更加地冒犯我们，

159
00:08:02.159 --> 00:08:05.019
and to really whack us -- and their own journalists --
而且用这个虚假新闻的棍棒， 真正打击我们——

160
00:08:04.933 --> 00:08:06.686
with this bludgeon of fake news.
和他们的记者。

161
00:08:07.820 --> 00:08:10.004
CH: To what extent is what happened, though,
CH: 这在某种程度上已经发生，

162
00:08:09.988 --> 00:08:11.638
in part, just an unintended consequence,
部分上，仅仅是无心的后果，

163
00:08:11.638 --> 00:08:14.814
that the traditional media that you worked in
你曾经工作的传统意义上的媒体，

164
00:08:14.944 --> 00:08:17.024
had this curation-mediation role,
有这个策划调节的角色，

165
00:08:17.048 --> 00:08:19.074
where certain norms were observed,
一些特定的规定是必须的，

166
00:08:19.098 --> 00:08:22.251
certain stories would be rejected because they weren't credible,
一些故事会被否决， 因为它们不可信，

167
00:08:22.275 --> 00:08:28.774
but now that the standard for publication and for amplification
但是现在发表和传播的标准

168
00:08:28.798 --> 00:08:32.126
is just interest, attention, excitement, click,
仅仅是兴趣，关注， 兴奋，点击。

169
00:08:32.150 --> 00:08:33.313
"Did it get clicked on?"
“它有没有被点击？”

170
00:08:33.337 --> 00:08:34.492
"Send it out there!"
“发布出去！”

171
00:08:34.516 --> 00:08:38.020
and that's what's -- is that part of what's caused the problem?
那是所谓的—— 那是所引起的问题的一部分吗？

172
00:08:37.684 --> 00:08:41.253
CA: I think it's a big problem, and we saw this in the election of 2016,
CA: 我认为这是一个大问题， 我们看到了2016年的大选，

173
00:08:41.663 --> 00:08:46.770
where the idea of "clickbait" was very sexy and very attractive,
关于“标题党”这个概念， 是非常性感和吸引人的，

174
00:08:46.794 --> 00:08:51.100
and so all these fake news sites and fake news items
那么所有这些虚假新闻网站和虚假的内容

175
00:08:51.124 --> 00:08:55.246
were not just haphazardly and by happenstance being put out there,
不仅仅是胡乱地， 而且是偶然地被发布出去，

176
00:08:54.920 --> 00:08:59.395
there's been a whole industry in the creation of fake news
有整个行业都在制造虚假新闻，

177
00:08:59.745 --> 00:09:02.735
in parts of Eastern Europe, wherever,
在东欧的一部分，或者其它地方，

178
00:09:02.759 --> 00:09:06.019
and you know, it's planted in real space and in cyberspace.
都在真实的空间以及网络空间中生根。

179
00:09:06.043 --> 00:09:08.402
So I think that, also,
所以我也在想，

180
00:09:07.366 --> 00:09:13.541
the ability of our technology to proliferate this stuff
我们科技以声速或者光速

181
00:09:13.571 --> 00:09:17.082
at the speed of sound or light, just about --
扩散这样东西的能力——

182
00:09:17.106 --> 00:09:19.089
we've never faced that before.
这种事我们从来没有面对过。

183
00:09:19.113 --> 00:09:23.980
And we've never faced such a massive amount of information
而且我们从来没有面对过 这样庞大的信息量，

184
00:09:23.964 --> 00:09:25.563
which is not curated
而这些信息不是被

185
00:09:25.593 --> 00:09:30.889
by those whose profession leads them to abide by the truth,
那些职业记者去捍卫真相，

186
00:09:30.913 --> 00:09:32.115
to fact-check
去做事实调查，

187
00:09:31.769 --> 00:09:36.353
and to maintain a code of conduct and a code of professional ethics.
去维护一个操守准则 和一个职业道德的守则。

188
00:09:36.997 --> 00:09:40.340
CH: Many people here may know people who work at Facebook
CH: 在座的很多人认识那些在脸书

189
00:09:40.364 --> 00:09:42.688
or Twitter and Google and so on.
或者推特以及谷歌工作的人。

190
00:09:42.712 --> 00:09:45.844
They all seem like great people with good intention --
他们似乎都是很棒的人， 有着好的意图——

191
00:09:45.868 --> 00:09:47.248
let's assume that.
让我们姑且这样说。

192
00:09:47.272 --> 00:09:50.947
If you could speak with the leaders of those companies,
如果你能够和这些 公司的领导者对话，

193
00:09:50.971 --> 00:09:52.262
what would you say to them?
你会对他们说些什么？

194
00:09:52.286 --> 00:09:54.055
CA: Well, you know what --
CA: 你知道吗——

195
00:09:53.959 --> 00:09:56.337
I'm sure they are incredibly well-intentioned,
我确定他们都 有着极度良好的意图，

196
00:09:56.447 --> 00:10:01.665
and they certainly developed an unbelievable, game-changing system,
而且他们绝对是发展了 一个令人难以置信的，有突破性的系统，

197
00:10:01.689 --> 00:10:04.900
where everybody's connected on this thing called Facebook.
在那个系统里每个人都通过脸书相关联。

198
00:10:04.924 --> 00:10:08.725
And they've created a massive economy for themselves
他们为他们自己 创造了一个巨大的经济体，

199
00:10:08.749 --> 00:10:11.429
and an amazing amount of income.
以及令人惊叹的收入。

200
00:10:11.453 --> 00:10:12.633
I would just say,
我会说，

201
00:10:12.657 --> 00:10:16.891
"Guys, you know, it's time to wake up and smell the coffee
“大伙们，你们知道，是时候醒来认清事实了。

202
00:10:16.915 --> 00:10:19.617
and look at what's happening to us right now."
然后看看现在正在发生的事情。”

203
00:10:19.641 --> 00:10:22.573
Mark Zuckerberg wants to create a global community.
马克·扎克伯格想要创造 一个全球性的社区。

204
00:10:22.597 --> 00:10:25.816
I want to know: What is that global community going to look like?
我想知道：那个全球社区 将会看起来是什么样子的？

205
00:10:25.840 --> 00:10:29.907
I want to know where the codes of conduct actually are.
我想知道职业守则 实际上是在哪里。

206
00:10:29.931 --> 00:10:31.756
Mark Zuckerberg said --
马克·扎克伯格说——

207
00:10:31.780 --> 00:10:34.498
and I don't blame him, he probably believed this --
我不是责怪他， 他估计相信这个——

208
00:10:34.522 --> 00:10:36.878
that it was crazy to think
这个很疯狂的想法是

209
00:10:36.902 --> 00:10:41.011
that the Russians or anybody else could be tinkering and messing around
俄国或者任何其他人 都可以用这种方式

210
00:10:40.965 --> 00:10:42.242
with this avenue.
摆弄或者是玩弄我们。

211
00:10:42.302 --> 00:10:44.784
And what have we just learned in the last few weeks?
我们在过去的几周中 学到了什么？

212
00:10:44.808 --> 00:10:47.766
That, actually, there has been a major problem in that regard,
实际上，有一个主要的问题，

213
00:10:47.790 --> 00:10:50.908
and now they're having to investigate it and figure it out.
现在他们在进行调查。

214
00:10:50.932 --> 00:10:54.211
Yes, they're trying to do what they can now
是的，他们在做 他们现在力所能及的

215
00:10:54.235 --> 00:10:56.393
to prevent the rise of fake news,
去阻止虚假新闻的增加，

216
00:10:56.417 --> 00:10:57.800
but, you know,
但是，你知道，

217
00:10:57.824 --> 00:11:02.915
it went pretty unrestricted for a long, long time.
很长时间以来 这都是很不严格的。

218
00:11:02.939 --> 00:11:04.839
So I guess I would say, you know,
所以我会说，你们知道，

219
00:11:04.863 --> 00:11:06.962
you guys are brilliant at technology;
你们很精通科技；

220
00:11:06.986 --> 00:11:08.877
let's figure out another algorithm.
让我们用另一套算法吧。

221
00:11:08.901 --> 00:11:10.072
Can we not?
我们可以吗？

222
00:11:10.096 --> 00:11:12.983
CH: An algorithm that includes journalistic investigation --
CH: 一个包括 记者性调查的算法——

223
00:11:12.527 --> 00:11:16.357
CA: I don't really know how they do it, but somehow, you know --
CA: 我不是很明白 他们是怎么做的，但你知道——

224
00:11:16.387 --> 00:11:18.206
filter out the crap!
过滤掉垃圾！

225
00:11:18.230 --> 00:11:19.380
(Laughter)
（笑声）

226
00:11:19.404 --> 00:11:21.406
And not just the unintentional --
不仅仅是不经意间——

227
00:11:21.430 --> 00:11:24.684
(Applause)
（观众掌声）

228
00:11:24.708 --> 00:11:26.914
but the deliberate lies that are planted
而是那种几十年来

229
00:11:26.938 --> 00:11:31.263
by people who've been doing this as a matter of warfare
被视为战争手段的人们

230
00:11:31.287 --> 00:11:32.589
for decades.
刻意植下的谎言。

231
00:11:32.613 --> 00:11:34.546
The Soviets, the Russians --
苏维埃，俄国——

232
00:11:34.570 --> 00:11:39.814
they are the masters of war by other means, of hybrid warfare.
他们是战争大师， 换句话说，混合战争大师。

233
00:11:40.438 --> 00:11:41.882
And this is a --
这是一个——

234
00:11:42.509 --> 00:11:45.493
this is what they've decided to do.
这是他们决定去做的事情。

235
00:11:45.517 --> 00:11:47.122
It worked in the United States,
在美国行得通，

236
00:11:46.836 --> 00:11:48.351
it didn't work in France,
在法国行不通，

237
00:11:48.491 --> 00:11:50.164
it hasn't worked in Germany.
在德国还没有成功。

238
00:11:50.188 --> 00:11:53.129
During the elections there, where they've tried to interfere,
在选举期间， 他们尝试过干涉过，

239
00:11:53.153 --> 00:11:55.755
the president of France right now, Emmanuel Macron,
现任的法国总统 埃马纽埃尔·马克龙，

240
00:11:55.779 --> 00:11:58.302
took a very tough stand and confronted it head on,
站在很强硬的立场上， 持续与其作战，

241
00:11:58.326 --> 00:11:59.484
as did Angela Merkel.
安格拉·默克尔也是这样做的。

242
00:11:59.508 --> 00:12:02.493
CH: There's some hope to be had from some of this, isn't there?
CH: 这其中多少有一些希望不是吗？

243
00:12:01.737 --> 00:12:02.952
That the world learns.
世人会学习的。

244
00:12:02.952 --> 00:12:04.924
We get fooled once,
我们被愚弄过一次，

245
00:12:04.924 --> 00:12:06.350
maybe we get fooled again,
也许我们会再次被愚弄，

246
00:12:06.390 --> 00:12:07.845
but maybe not the third time.
但也许不会有第三次。

247
00:12:07.869 --> 00:12:09.037
Is that true?
是那样吗？

248
00:12:08.371 --> 00:12:09.441
CA: I mean, let's hope.
CA: 我想说，希望吧。

249
00:12:09.441 --> 00:12:13.352
But I think in this regard that so much of it is also about technology,
但我认为就这而言， 这也事关科技，

250
00:12:13.652 --> 00:12:17.097
that the technology has to also be given some kind of moral compass.
科技也被给予了一些 道德上的罗盘。

251
00:12:17.121 --> 00:12:19.937
I know I'm talking nonsense, but you know what I mean.
我知道我在乱说， 但你懂我的意思。

252
00:12:19.961 --> 00:12:23.669
CH: We need a filter-the-crap algorithm with a moral compass --
CH: 我们需要有着道德罗盘的 可以过滤垃圾的算法——

253
00:12:23.693 --> 00:12:24.850
CA: There you go.
CA: 正是。

254
00:12:24.874 --> 00:12:26.026
CH: I think that's good.
CH: 我认为那是好的。

255
00:12:25.656 --> 00:12:27.605
CA: No -- "moral technology."
CA: 不——“道德科技”。

256
00:12:27.745 --> 00:12:30.851
We all have moral compasses -- moral technology.
我们都有道德罗盘—— 道德科技。

257
00:12:30.875 --> 00:12:33.854
CH: I think that's a great challenge. CA: You know what I mean.
CH: 我认为那是一个很大的挑战。 CA: 你知道我的意思。

258
00:12:32.668 --> 00:12:35.196
CH: Talk just a minute about leadership.
CH: 花一分钟谈论领导力。

259
00:12:35.846 --> 00:12:38.982
You've had a chance to speak with so many people across the world.
你有一个机会和全世界 如此多的人谈话。

260
00:12:38.736 --> 00:12:40.029
I think for some of us --
我想对于我们 其中某些人来说——

261
00:12:40.269 --> 00:12:42.961
I speak for myself, I don't know if others feel this --
我是在说我自己的观点， 我不知道是否其他人也这样认为——

262
00:12:42.985 --> 00:12:44.981
there's kind of been a disappointment of:
一直以来有一种失望：

263
00:12:44.595 --> 00:12:46.448
Where are the leaders?
领导者们都在哪里？

264
00:12:46.888 --> 00:12:49.202
So many of us have been disappointed --
我们其中的许多人感到失望——

265
00:12:49.226 --> 00:12:51.242
Aung San Suu Kyi, what's happened recently,
翁山苏姬， 那是最近发生的，

266
00:12:50.676 --> 00:12:53.035
it's like, "No! Another one bites the dust."
就好像是，“不！另一个倒下了。"

267
00:12:53.375 --> 00:12:54.974
You know, it's heartbreaking.
你知道，很令人心碎。

268
00:12:54.998 --> 00:12:56.233
(Laughter)
（观众笑声）

269
00:12:55.947 --> 00:12:57.902
Who have you met
你遇见了谁？

270
00:12:57.902 --> 00:13:01.116
who you have been impressed by, inspired by?
你对谁印象深刻？ 你被谁鼓舞？

271
00:13:01.196 --> 00:13:03.700
CA: Well, you talk about the world in crisis,
CA: 你要谈论在危机中的世界，

272
00:13:03.724 --> 00:13:05.078
which is absolutely true,
这绝对是真实的，

273
00:13:05.102 --> 00:13:09.589
and those of us who spend our whole lives immersed in this crisis --
对于我们这些将整个生命 沉浸在危机中的人来说——

274
00:13:09.613 --> 00:13:12.606
I mean, we're all on the verge of a nervous breakdown.
我是说，我们都在神经崩溃的边缘上。

275
00:13:12.630 --> 00:13:15.306
So it's pretty stressful right now.
所以现在是一个 压力非常大的时期。

276
00:13:15.330 --> 00:13:16.489
And you're right --
而且你是对的——

277
00:13:16.513 --> 00:13:19.623
there is this perceived and actual vacuum of leadership,
这里有一个感知的和实际上的 领导力的真空，

278
00:13:19.647 --> 00:13:22.497
and it's not me saying it, I ask all these --
不是我在说， 我在问这些——

279
00:13:22.521 --> 00:13:24.974
whoever I'm talking to, I ask about leadership.
所有和我谈话的人， 我和他们谈领导力。

280
00:13:24.998 --> 00:13:29.508
I was speaking to the outgoing president of Liberia today,
我今天在和十分友好的 利比里亚总统谈话，

281
00:13:29.532 --> 00:13:31.342
[Ellen Johnson Sirleaf,]
埃伦·约翰逊·瑟利夫，

282
00:13:31.366 --> 00:13:32.520
who --
她——

283
00:13:32.544 --> 00:13:34.759
(Applause)
（观众掌声）

284
00:13:34.783 --> 00:13:36.325
in three weeks' time,
在三个星期以内，

285
00:13:36.349 --> 00:13:40.293
will be one of the very rare heads of an African country
将会成为非洲国家 十分罕见的领导者之一，

286
00:13:40.317 --> 00:13:42.495
who actually abides by the constitution
少有的遵守宪法的

287
00:13:42.519 --> 00:13:46.131
and gives up power after her prescribed term.
而且在任期结束之后放弃权力的领导人。

288
00:13:46.155 --> 00:13:50.012
She has said she wants to do that as a lesson.
她说她做这个是想要 建立一个先例。

289
00:13:49.836 --> 00:13:51.922
But when I asked her about leadership,
但当我和她探讨领导力的时候，

290
00:13:51.922 --> 00:13:54.689
and I gave a quick-fire round of certain names,
我快速提及了一些名字，

291
00:13:54.799 --> 00:13:57.776
I presented her with the name of the new French president,
我提及了法国新总统的名字，

292
00:13:57.800 --> 00:13:59.233
Emmanuel Macron.
埃马纽埃尔·马克龙。

293
00:13:59.257 --> 00:14:00.593
And she said --
然后她说——

294
00:14:00.617 --> 00:14:03.123
I said, "So what do you think when I say his name?"
我说，“当我提到他名字的时候 你想到了什么？”

295
00:14:03.147 --> 00:14:04.420
And she said,
她说，

296
00:14:05.398 --> 00:14:07.723
"Shaping up potentially to be
“他是可以成为潜在的

297
00:14:07.747 --> 00:14:11.813
a leader to fill our current leadership vacuum."
来填充我们现今领导真空的一个领导人。”

298
00:14:11.837 --> 00:14:13.670
I thought that was really interesting.
我认为那是很有趣的。

299
00:14:13.694 --> 00:14:16.150
Yesterday, I happened to have an interview with him.
昨天，我刚巧和他做了访问。

300
00:14:14.914 --> 00:14:16.126
I'm very proud to say,
我很自豪地说，

301
00:14:16.126 --> 00:14:19.609
I got his first international interview. It was great. It was yesterday.
我拿到了他的第一个国际专访 这很棒。是在昨天。

302
00:14:19.609 --> 00:14:20.975
And I was really impressed.
我印象很深。

303
00:14:20.975 --> 00:14:23.817
I don't know whether I should be saying that in an open forum,
我不知道我是否应该在 一个公开的论坛里谈论这个，

304
00:14:23.817 --> 00:14:26.086
but I was really impressed.
但我真的有很深的印象。

305
00:14:26.546 --> 00:14:27.764
(Laughter)
（笑声）

306
00:14:27.377 --> 00:14:30.916
And it could be just because it was his first interview,
这可能是因为 这是他的第一个专访，

307
00:14:30.916 --> 00:14:33.385
but -- I asked questions, and you know what?
但是——我提了问， 你们知道吗？

308
00:14:33.505 --> 00:14:34.713
He answered them!
他回答了他们！

309
00:14:34.737 --> 00:14:36.670
(Laughter)
（观众笑声）

310
00:14:36.694 --> 00:14:39.963
(Applause)
（观众掌声）

311
00:14:39.987 --> 00:14:41.580
There was no spin,
没有回旋，

312
00:14:41.604 --> 00:14:43.995
there was no wiggle and waggle,
没有来来回回，

313
00:14:43.969 --> 00:14:46.412
there was no spend-five-minutes- to-come-back-to-the-point.
没有五分钟之后 回到话题点上来。

314
00:14:46.872 --> 00:14:48.540
I didn't have to keep interrupting,
我不用一直打断他，

315
00:14:48.564 --> 00:14:50.647
which I've become rather renowned for doing,
我以一直做这个闻名，

316
00:14:50.671 --> 00:14:53.203
because I want people to answer the question.
因为我想要人们回答问题。

317
00:14:53.227 --> 00:14:55.278
And he answered me,
而他回答了我的问题，

318
00:14:55.302 --> 00:14:57.916
and it was pretty interesting.
这就有趣了。

319
00:14:57.940 --> 00:14:59.371
And he said --
然后他说——

320
00:14:59.395 --> 00:15:01.173
CH: Tell me what he said.
CH: 告诉我们他说的。

321
00:15:01.197 --> 00:15:02.417
CA: No, no, you go ahead.
CA: 不， 你先说。

322
00:15:02.441 --> 00:15:04.669
CH: You're the interrupter, I'm the listener.
CH: 你是打断者， 我是聆听者。

323
00:15:03.843 --> 00:15:05.145
CA: No, no, go ahead.
CA: 不，你先说。

324
00:15:05.875 --> 00:15:07.030
CH: What'd he say?
CH: 他说了什么？

325
00:15:05.974 --> 00:15:08.586
CA: OK. You've talked about nationalism and tribalism here today.
CA: 我们今天在这里谈论了 民族主义和部落主义。

326
00:15:08.586 --> 00:15:13.812
I asked him, "How did you have the guts to confront the prevailing winds
我问他，“你是如何有勇气 去面对现在涌行的

327
00:15:13.942 --> 00:15:18.477
of anti-globalization, nationalism, populism
关于反全球化， 民族主义，民粹主义的潮势，

328
00:15:18.501 --> 00:15:20.463
when you can see what happened in Brexit,
当你在2017年初看到

329
00:15:20.487 --> 00:15:23.042
when you could see what happened in the United States
英国退出欧盟所发生的，

330
00:15:21.986 --> 00:15:24.875
and what might have happened in many European elections
当你看到可能在美国发生的，

331
00:15:24.875 --> 00:15:27.356
at the beginning of 2017?"
在很多欧洲选举中可能发生的？”

332
00:15:27.426 --> 00:15:28.745
And he said,
然后他说，

333
00:15:29.417 --> 00:15:32.691
"For me, nationalism means war.
“对于我来说，民族主义意味着战争。

334
00:15:33.306 --> 00:15:34.979
We have seen it before,
我们曾经见证过，

335
00:15:34.913 --> 00:15:37.225
we have lived through it before on my continent,
在我生活的大陆上我们经历过，

336
00:15:37.285 --> 00:15:39.971
and I am very clear about that."
所以我对于这点很明确。”

337
00:15:39.995 --> 00:15:43.956
So he was not going to, just for political expediency,
所以他不会只是 为了政治上的便利，

338
00:15:43.980 --> 00:15:47.422
embrace the, kind of, lowest common denominator
迎合大部分的选票，

339
00:15:47.446 --> 00:15:51.451
that had been embraced in other political elections.
这种情况常发生在其它政治选举中。

340
00:15:51.475 --> 00:15:55.916
And he stood against Marine Le Pen, who is a very dangerous woman.
他选择和玛丽娜·勒龙， 一个很危险的女人对战。

341
00:15:56.748 --> 00:15:58.780
CH: Last question for you, Christiane.
CH: 最后一个问题，克莉丝蒂安。

342
00:15:59.913 --> 00:16:01.911
TED is about ideas worth spreading.
跟我们讲讲关于 值得广布流传的观念。

343
00:16:01.935 --> 00:16:06.582
If you could plant one idea into the minds of everyone here,
如果你要给在座的每位 植入一个观念，

344
00:16:06.606 --> 00:16:07.803
what would that be?
那会是什么？

345
00:16:07.827 --> 00:16:12.941
CA: I would say really be careful where you get your information from;
CA: 我会说，对于你从哪里获取 信息，要十分小心谨慎；

346
00:16:12.965 --> 00:16:18.287
really take responsibility for what you read, listen to and watch;
真的要对于你所阅读的， 听到的和看到的，承担起责任来；

347
00:16:18.311 --> 00:16:23.198
make sure that you go to the trusted brands to get your main information,
确保你去到你信任的渠道 去获取你的主要信息，

348
00:16:22.952 --> 00:16:27.765
no matter whether you have a wide, eclectic intake,
无论你是否有一个广泛的 不拘一格的抓取信息的方式，

349
00:16:27.935 --> 00:16:30.930
really stick with the brand names that you know,
真正的和你信任的渠道 保持连接，

350
00:16:30.954 --> 00:16:34.546
because in this world right now, at this moment right now,
因为在现今这个世界， 就在此时此刻，

351
00:16:34.570 --> 00:16:38.909
our crises, our challenges, our problems are so severe,
我们的危机，我们的挑战， 我们的问题如此严重，

352
00:16:38.933 --> 00:16:42.484
that unless we are all engaged as global citizens
除非我们都以全球公民的 身份来共同参与，

353
00:16:42.508 --> 00:16:44.411
who appreciate the truth,
我们都珍惜和推崇真相，

354
00:16:44.435 --> 00:16:48.780
who understand science, empirical evidence and facts,
我们都明白科学证据和事实，

355
00:16:48.804 --> 00:16:52.303
then we are just simply going to be wandering along
否则我们就只会是随波逐流

356
00:16:52.327 --> 00:16:54.288
to a potential catastrophe.
到一个潜在的巨大危机里面去。

357
00:16:54.312 --> 00:16:55.676
So I would say, the truth,
所以，我会说，真相，

358
00:16:55.700 --> 00:16:57.956
and then I would come back to Emmanuel Macron
然后我会回到 埃马纽埃尔·马克龙上，

359
00:16:57.980 --> 00:16:59.280
and talk about love.
谈论爱。

360
00:16:59.842 --> 00:17:04.311
I would say that there's not enough love going around.
我认为世间没有足够的爱。

361
00:17:04.335 --> 00:17:07.027
And I asked him to tell me about love.
我要他和我说说爱。

362
00:17:06.861 --> 00:17:10.567
I said, "You know, your marriage is the subject of global obsession."
我说，“你知道的，你的婚姻 是一个全球都沉迷其中的话题。”

363
00:17:10.667 --> 00:17:12.302
(Laughter)
（观众笑声）

364
00:17:12.326 --> 00:17:13.739
"Can you tell me about love?
“你可以跟我们说说爱情吗？”

365
00:17:13.763 --> 00:17:15.077
What does it mean to you?"
它对你来说意味着什么？”

366
00:17:14.941 --> 00:17:16.956
I've never asked a president or an elected leader about love.
我从来没有向一个总统或是 一个被选举的领导人提出关于爱的问题。

367
00:17:16.956 --> 00:17:18.344
I thought I'd try it.
我当时在想，我要试试。

368
00:17:19.248 --> 00:17:23.163
And he said -- you know, he actually answered it.
然后他说——你知道的， 他实际上回答了这个问题。

369
00:17:22.977 --> 00:17:27.142
And he said, "I love my wife, she is part of me,
他说，“我爱我的妻子 她是我的一部分，

370
00:17:27.372 --> 00:17:28.999
we've been together for decades."
我们在一起超过十年了。”

371
00:17:28.893 --> 00:17:30.342
But here's where it really counted,
但这里是关键点，

372
00:17:30.732 --> 00:17:32.235
what really stuck with me.
这才是真正抓住我的地方，

373
00:17:32.259 --> 00:17:33.500
He said,
他说，

374
00:17:33.524 --> 00:17:37.044
"It is so important for me to have somebody at home
“对于我来说，有一个人在家里 告诉我真相，

375
00:17:36.898 --> 00:17:38.967
who tells me the truth."
是如此地重要。”

376
00:17:40.438 --> 00:17:43.150
So you see, I brought it home. It's all about the truth.
你看，我把它带回了家里。 都是关于真相的。

377
00:17:42.754 --> 00:17:43.914
(Laughter)
（观众笑声）

378
00:17:43.914 --> 00:17:46.195
CH: So there you go. Truth and love. Ideas worth spreading.
CH: 真相和爱情。值得推广的理念。

379
00:17:46.195 --> 00:17:48.532
Christiane Amanpour, thank you so much. That was great.
克莉丝蒂安·阿曼普，十分感谢你今天的到来。

380
00:17:48.532 --> 00:17:49.834
(Applause)
（观众掌声）

381
00:17:49.834 --> 00:17:51.992
CA: Thank you. CH: That was really lovely.
CA: 谢谢。 CH: 那真的是很棒的经历。

382
00:17:51.992 --> 00:17:54.091
(Applause)
（观众掌声）

383
00:17:54.411 --> 00:17:55.576
CA: Thank you.
CA: 谢谢。