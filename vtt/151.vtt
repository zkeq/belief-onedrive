WEBVTT

1
00:00:12.780 --> 00:00:13.980
I want you to imagine
我想让你们想象

2
00:00:14.820 --> 00:00:16.020
walking into a room,
走进一个房间，

3
00:00:17.300 --> 00:00:19.436
a control room with a bunch of people,
这个房间是个控制室， 里面有一群人，大概上百人，

4
00:00:19.460 --> 00:00:22.300
a hundred people, hunched over a desk with little dials,
都在有控制器的桌子旁伏案工作，

5
00:00:23.100 --> 00:00:24.620
and that that control room
而那间控制室

6
00:00:25.500 --> 00:00:29.196
will shape the thoughts and feelings
会决定上亿人的

7
00:00:29.220 --> 00:00:30.460
of a billion people.
思想和情感。

8
00:00:32.380 --> 00:00:34.260
This might sound like science fiction,
这听上去像是科幻小说，

9
00:00:35.140 --> 00:00:37.356
but this actually exists
但是今天，此刻，

10
00:00:37.380 --> 00:00:38.580
right now, today.
它真真切切地发生着。

11
00:00:39.860 --> 00:00:43.220
I know because I used to be in one of those control rooms.
因为我就在这种控制室工作过。

12
00:00:43.979 --> 00:00:46.276
I was a design ethicist at Google,
我曾是谷歌的设计伦理学家，

13
00:00:46.300 --> 00:00:49.660
where I studied how do you ethically steer people's thoughts?
在那里我曾研究过如何 名正言顺地左右人们的想法。

14
00:00:50.380 --> 00:00:53.276
Because what we don't talk about is how the handful of people
因为我们不常讨论屈指可数的

15
00:00:53.300 --> 00:00:55.836
working at a handful of technology companies
几家科技公司的那么几个人，

16
00:00:55.860 --> 00:01:00.900
through their choices will steer what a billion people are thinking today.
是如何通过他们的选择 来影响现今上亿人的思想。

17
00:01:02.220 --> 00:01:03.956
Because when you pull out your phone
因为当你拿出手机，

18
00:01:03.980 --> 00:01:07.076
and they design how this works or what's on the feed,
他们将设计下一步会如何 以及应该推送哪些新闻，

19
00:01:07.100 --> 00:01:10.316
it's scheduling little blocks of time in our minds.
就像在我们的脑子里设定出了小小的模块。

20
00:01:10.340 --> 00:01:13.476
If you see a notification, it schedules you to have thoughts
如果你看到一个消息， 这会促使你形成

21
00:01:13.500 --> 00:01:15.540
that maybe you didn't intend to have.
或许原本没有的想法。

22
00:01:16.220 --> 00:01:18.836
If you swipe over that notification,
如果你忽略了这个消息，

23
00:01:18.860 --> 00:01:21.241
it schedules you into spending a little bit of time
他们还会设计让你在原本

24
00:01:21.265 --> 00:01:22.646
getting sucked into something
不关注的东西上

25
00:01:22.670 --> 00:01:25.625
that maybe you didn't intend to get sucked into.
多花些时间。

26
00:01:27.140 --> 00:01:28.660
When we talk about technology,
当我们谈到科技时，

27
00:01:29.860 --> 00:01:32.556
we tend to talk about it as this blue sky opportunity.
我们倾向于把它描绘成 充满机遇的蓝图。

28
00:01:32.580 --> 00:01:34.060
It could go any direction.
天马行空，任你想象。

29
00:01:35.220 --> 00:01:37.076
And I want to get serious for a moment
而我想严肃地告诉你们，

30
00:01:36.970 --> 00:01:39.790
and tell you why it's going in a very specific direction.
为什么它选择了某个特定的方向。

31
00:01:40.660 --> 00:01:42.860
Because it's not evolving randomly.
因为科技的发展不是随机的。

32
00:01:43.820 --> 00:01:45.836
There's a hidden goal driving the direction
在我们创造的所有科技背后

33
00:01:45.860 --> 00:01:47.996
of all of the technology we make,
都隐藏着明确的目标，

34
00:01:47.990 --> 00:01:51.950
and that goal is the race for our attention.
那个目标就是追求我们的注意力。

35
00:01:52.660 --> 00:01:55.396
Because every news site,
因为所有新的网站——

36
00:01:55.420 --> 00:01:58.156
TED, elections, politicians,
TED演讲，选举，政客，

37
00:01:58.180 --> 00:02:00.156
games, even meditation apps
游戏，甚至是冥想软件——

38
00:02:00.180 --> 00:02:02.140
have to compete for one thing,
都需要为同一种东西彼此竞争，

39
00:02:02.980 --> 00:02:04.716
which is our attention,
那就是我们的注意力，

40
00:02:04.740 --> 00:02:06.340
and there's only so much of it.
而注意力是有限的。

41
00:02:08.260 --> 00:02:10.676
And the best way to get people's attention
获得人们注意力的最佳办法

42
00:02:10.700 --> 00:02:13.140
is to know how someone's mind works.
就是了解我们的大脑是如何运作的。

43
00:02:13.620 --> 00:02:15.956
And there's a whole bunch of persuasive techniques
大学时我曾在一个叫做 说服性技术实验室的地方

44
00:02:15.980 --> 00:02:19.476
that I learned in college at a lab called the Persuasive Technology Lab
学到了许多说服别人的技巧，

45
00:02:19.500 --> 00:02:21.100
to get people's attention.
来获取人们的注意力。

46
00:02:21.700 --> 00:02:23.180
A simple example is YouTube.
一个简单的例子就是YouTube。

47
00:02:23.820 --> 00:02:26.756
YouTube wants to maximize how much time you spend.
YouTube希望你在 它上面花的时间越多越好。

48
00:02:26.780 --> 00:02:27.980
And so what do they do?
那么他们做了什么呢？

49
00:02:28.660 --> 00:02:30.940
They autoplay the next video.
他们增加了自动播放下一个视频的功能。

50
00:02:31.580 --> 00:02:33.396
And let's say that works really well.
效果应该说很不错。

51
00:02:33.420 --> 00:02:35.836
They're getting a little bit more of people's time.
人们在上面花费了更多的时间。

52
00:02:35.860 --> 00:02:38.236
Well, if you're Netflix, you look at that and say,
如果你在Netflix工作， 看到他们这么做之后会觉得，

53
00:02:37.480 --> 00:02:39.188
well, that's shrinking my market share,
这会减少我的市场份额，

54
00:02:39.188 --> 00:02:42.422
so I'm going to autoplay the next episode.
我也要自动播放下一集。

55
00:02:43.140 --> 00:02:44.516
But then if you're Facebook,
但如果你是Facebook的员工，

56
00:02:44.540 --> 00:02:46.876
you say, that's shrinking all of my market share,
你会说，这减少了我的市场份额，

57
00:02:46.900 --> 00:02:49.556
so now I have to autoplay all the videos in the newsfeed
所以我会在你点击播放按键前

58
00:02:48.936 --> 00:02:51.902
before waiting for you to click play.
自动播放新闻推送中的所有视频。

59
00:02:52.140 --> 00:02:55.300
So the internet is not evolving at random.
所以网络不是随机发展的。

60
00:02:56.140 --> 00:03:00.556
The reason it feels like it's sucking us in the way it is
我们感觉上瘾的原因在于

61
00:03:00.580 --> 00:03:02.956
is because of this race for attention.
我们的注意力成了被竞争的对象。

62
00:03:02.980 --> 00:03:04.396
We know where this is going.
我们知道这样发展下去会如何。

63
00:03:04.420 --> 00:03:05.940
Technology is not neutral,
科技并非中性的，

64
00:03:07.140 --> 00:03:10.556
and it becomes this race to the bottom of the brain stem
它变成了那些可以深入了解

65
00:03:10.580 --> 00:03:12.780
of who can go lower to get it.
人脑运作的人之间的竞争。

66
00:03:13.740 --> 00:03:16.076
Let me give you an example of Snapchat.
给你们举个Snapchat的例子。

67
00:03:16.100 --> 00:03:19.796
If you didn't know, Snapchat is the number one way
有人可能不太了解， Snapchat是在美国青少年中

68
00:03:19.820 --> 00:03:22.076
that teenagers in the United States communicate.
排名第一的交流方式。

69
00:03:21.840 --> 00:03:26.260
So if you're like me, and you use text messages to communicate,
如果你们和我一样用短信交流，

70
00:03:26.300 --> 00:03:28.076
Snapchat is that for teenagers,
那么Snapchat就是青少年的短信，

71
00:03:28.100 --> 00:03:30.796
and there's, like, a hundred million of them that use it.
有近一亿人在使用。

72
00:03:30.820 --> 00:03:33.036
And they invented a feature called Snapstreaks,
他们发明了一种叫Snapstreaks的功能，

73
00:03:32.496 --> 00:03:34.506
which shows the number of days in a row
它会展示两个人

74
00:03:34.980 --> 00:03:37.596
that two people have communicated with each other.
持续互动的天数。

75
00:03:37.620 --> 00:03:39.476
In other words, what they just did
换句话说，他们所做的

76
00:03:39.500 --> 00:03:42.460
is they gave two people something they don't want to lose.
就是给了两个人不想失去的记录。

77
00:03:43.820 --> 00:03:47.276
Because if you're a teenager, and you have 150 days in a row,
因为如果你是一个青少年， 连续交流了150天，

78
00:03:47.300 --> 00:03:49.276
you don't want that to go away.
你不想失去这些东西。

79
00:03:49.300 --> 00:03:53.460
And so think of the little blocks of time that that schedules in kids' minds.
试想一下孩子脑子里 已被设定好的时间模块。

80
00:03:53.980 --> 00:03:56.316
This isn't theoretical: when kids go on vacation,
这不是理论空谈，而是事实； 当孩子们去度假时，

81
00:03:56.340 --> 00:03:59.596
it's been shown they give their passwords to up to five other friends
即使他们不能使用 Snapstreaks，也会把密码

82
00:03:59.620 --> 00:04:01.836
to keep their Snapstreaks going,
告诉至多五个朋友，

83
00:04:01.860 --> 00:04:03.876
even when they can't do it.
来帮助他把Snapstreaks接力下去。

84
00:04:03.900 --> 00:04:05.836
And they have, like, 30 of these things,
他们要同时关注大约30件这种事情，

85
00:04:05.860 --> 00:04:09.236
and so they have to get through taking photos of just pictures or walls
所以每天他们就忙于拍拍画，拍拍墙，

86
00:04:09.260 --> 00:04:11.740
or ceilings just to get through their day.
拍拍天花板来消磨时间。

87
00:04:13.020 --> 00:04:15.716
So it's not even like they're having real conversations.
这甚至不能被称为真正的交流。

88
00:04:15.740 --> 00:04:17.676
We have a temptation to think about this
我们可能会这么想，

89
00:04:17.700 --> 00:04:20.396
as, oh, they're just using Snapchat
他们用Snapchat的方式

90
00:04:20.420 --> 00:04:22.436
the way we used to gossip on the telephone.
就像我们曾经用电话聊八卦一样，

91
00:04:22.460 --> 00:04:23.660
It's probably OK.
应该是没问题的。

92
00:04:24.300 --> 00:04:26.556
Well, what this misses is that in the 1970s,
但人们忽略了在上世纪七十年代，

93
00:04:26.580 --> 00:04:29.195
when you were just gossiping on the telephone,
当你们用电话聊八卦时，

94
00:04:29.219 --> 00:04:32.236
there wasn't a hundred engineers on the other side of the screen
在另一头并没有数百名工程师

95
00:04:31.920 --> 00:04:34.260
who knew exactly how your psychology worked
准确知道你们的心理活动，

96
00:04:34.340 --> 00:04:36.980
and orchestrated you into a double bind with each other.
并精心策划着如何让你们加强联络。

97
00:04:38.260 --> 00:04:41.660
Now, if this is making you feel a little bit of outrage,
如果这让你有些愤怒了，

98
00:04:42.500 --> 00:04:45.076
notice that that thought just comes over you.
请注意，你刚刚才意识到这点，

99
00:04:45.100 --> 00:04:48.420
Outrage is a really good way also of getting your attention,
愤怒也是获取你注意力的一种好方式。

100
00:04:49.700 --> 00:04:51.276
because we don't choose outrage.
因为我们不会选择愤怒。

101
00:04:51.300 --> 00:04:52.716
It happens to us.
它会自动发生在我们身上。

102
00:04:52.740 --> 00:04:54.596
And if you're the Facebook newsfeed,
如果你负责Facebook的新闻推送，

103
00:04:54.620 --> 00:04:56.036
whether you'd want to or not,
信不信由你，

104
00:04:55.810 --> 00:04:58.740
you actually benefit when there's outrage.
人们愤怒的时候你会受益。

105
00:04:58.820 --> 00:05:01.756
Because outrage doesn't just schedule a reaction
因为愤怒不仅仅是人们

106
00:05:01.780 --> 00:05:04.660
in emotional time, space, for you.
在情感和空间上的一个反应。

107
00:05:05.260 --> 00:05:07.676
We want to share that outrage with other people.
想要与人分享这份愤怒，

108
00:05:07.700 --> 00:05:09.276
So we want to hit share and say,
所以我们会按下分享键，然后说，

109
00:05:08.900 --> 00:05:11.340
"Can you believe the thing that they said?"
“你能相信他们说的吗？”

110
00:05:12.340 --> 00:05:15.716
And so outrage works really well at getting attention,
而这种愤怒真的特别容易获得注意力，

111
00:05:15.740 --> 00:05:19.636
such that if Facebook had a choice between showing you the outrage feed
甚至于如果Facebook 可以在展示惹人愤怒的消息

112
00:05:19.660 --> 00:05:20.980
and a calm newsfeed,
和一般的消息之间进行选择的话，

113
00:05:21.940 --> 00:05:24.076
they would want to show you the outrage feed,
他们会选择向你发布让你愤怒的消息，

114
00:05:23.800 --> 00:05:26.236
not because someone consciously chose that,
不是因为刻意的原因，

115
00:05:26.180 --> 00:05:28.860
but because that worked better at getting your attention.
只是那会更好的获得你的注意。

116
00:05:30.940 --> 00:05:36.420
And the newsfeed control room is not accountable to us.
消息控制室不用对我们负责，

117
00:05:36.860 --> 00:05:39.156
It's only accountable to maximizing attention.
它只对最大化获得人们的注意力负责，

118
00:05:39.180 --> 00:05:40.396
It's also accountable,
鉴于广告的商业模型，

119
00:05:40.420 --> 00:05:42.796
because of the business model of advertising,
它也对可以花钱进入

120
00:05:42.820 --> 00:05:46.156
for anybody who can pay the most to actually walk into the control room
控制室的人负责，他们可以要求说

121
00:05:46.180 --> 00:05:47.756
and say, "That group over there,
“那一组人，我想把

122
00:05:47.780 --> 00:05:50.420
I want to schedule these thoughts into their minds."
这些东西灌输给他们。”

123
00:05:51.580 --> 00:05:52.780
So you can target,
所以你可以定位，

124
00:05:53.860 --> 00:05:55.796
you can precisely target a lie
你可以准确定位到

125
00:05:55.820 --> 00:05:58.740
directly to the people who are most susceptible.
那些最易受到影响的人。

126
00:05:59.900 --> 00:06:02.780
And because this is profitable, it's only going to get worse.
因为这有利可图， 所以只会变得越来越糟。

127
00:06:04.860 --> 00:06:06.660
So I'm here today
今天我在这里（跟大家讲）

128
00:06:07.980 --> 00:06:09.980
because the costs are so obvious.
是因为代价已经太高了。

129
00:06:12.100 --> 00:06:14.236
I don't know a more urgent problem than this,
我不知道还有什么问题比这更紧急的，

130
00:06:14.260 --> 00:06:17.380
because this problem is underneath all other problems.
因为这个问题隐藏在所有问题之中。

131
00:06:18.540 --> 00:06:21.716
It's not just taking away our agency
它并不仅仅是剥夺了我们的注意力

132
00:06:21.740 --> 00:06:24.340
to spend our attention and live the lives that we want,
和选择生活方式的自主权，

133
00:06:25.540 --> 00:06:29.076
it's changing the way that we have our conversations,
更是改变了我们进行交流的方式，

134
00:06:29.100 --> 00:06:30.836
it's changing our democracy,
改变了我们的民主意识，

135
00:06:30.860 --> 00:06:33.476
and it's changing our ability to have the conversations
改变了我们与他人交流，

136
00:06:33.500 --> 00:06:35.500
and relationships we want with each other.
维系关系的能力。

137
00:06:36.980 --> 00:06:38.756
And it affects everyone,
这影响到了每个人，

138
00:06:38.780 --> 00:06:42.140
because a billion people have one of these in their pocket.
因为亿万人的口袋里 都装着这个东西（手机）。

139
00:06:45.180 --> 00:06:46.580
So how do we fix this?
那么我们要如何解决这个问题呢？

140
00:06:48.900 --> 00:06:51.836
We need to make three radical changes
我们需要对科技和社会

141
00:06:51.860 --> 00:06:53.660
to technology and to our society.
做三个大胆的突破。

142
00:06:55.540 --> 00:06:59.340
The first is we need to acknowledge that we are persuadable.
首先，我们需要承认我们可以被说服。

143
00:07:00.660 --> 00:07:02.036
Once you start understanding
一旦了解

144
00:07:01.990 --> 00:07:04.470
that your mind can be scheduled into having little thoughts
我们是可以安排大脑去想一点其他事情，

145
00:07:04.860 --> 00:07:07.436
or little blocks of time that you didn't choose,
或无计划地占用一些时间，

146
00:07:07.460 --> 00:07:09.516
wouldn't we want to use that understanding
我们难道不能利用这点认识

147
00:07:09.540 --> 00:07:11.700
and protect against the way that that happens?
来改变现状吗？

148
00:07:12.420 --> 00:07:15.716
I think we need to see ourselves fundamentally in a new way.
我认为我们需要 以全新的方式审视自己。

149
00:07:15.740 --> 00:07:17.956
It's almost like a new period of human history,
就像是人类历史上的新篇章，

150
00:07:17.980 --> 00:07:19.196
like the Enlightenment,
就像启蒙运动，

151
00:07:18.946 --> 00:07:21.380
but almost a kind of self-aware Enlightenment,
但这是自省式的启蒙运动，

152
00:07:21.460 --> 00:07:23.540
that we can be persuaded,
意识到我们可以被说服影响，

153
00:07:24.140 --> 00:07:26.380
and there might be something we want to protect.
意识到有些东西需要我们去保护。

154
00:07:27.220 --> 00:07:31.796
The second is we need new models and accountability systems
第二点是，我们需要新的 模型和责任系统，

155
00:07:31.820 --> 00:07:35.316
so that as the world gets better and more and more persuasive over time --
以便让世界变得越来越好， 也越来越有影响力时——

156
00:07:35.340 --> 00:07:37.676
because it's only going to get more persuasive --
也就是更能说服我们时——

157
00:07:37.700 --> 00:07:39.556
that the people in those control rooms
那些控制室里的人们

158
00:07:38.886 --> 00:07:41.990
are accountable and transparent to what we want.
才会对我们负责，且行为对我们透明化。

159
00:07:41.990 --> 00:07:44.730
The only form of ethical persuasion that exists
道德说服只有当

160
00:07:44.780 --> 00:07:46.716
is when the goals of the persuader
说服和被说服者的

161
00:07:46.740 --> 00:07:48.940
are aligned with the goals of the persuadee.
目标一致时才存在。

162
00:07:49.460 --> 00:07:53.300
And that involves questioning big things, like the business model of advertising.
这就涉及到对热门举措的怀疑， 比如广告的商业模型。

163
00:07:54.540 --> 00:07:56.116
Lastly,
最后，

164
00:07:56.140 --> 00:07:57.820
we need a design renaissance,
我们需要一次科技重塑，

165
00:07:58.900 --> 00:08:01.956
because once you have this view of human nature,
因为一旦你开始了解人的这种本性，

166
00:08:01.980 --> 00:08:04.956
that you can steer the timelines of a billion people --
你就可以控制上亿人的时间——

167
00:08:04.980 --> 00:08:07.716
just imagine, there's people who have some desire
想象一下，有些人有这样的欲望，

168
00:08:07.740 --> 00:08:10.596
about what they want to do and what they want to be thinking
他们想做什么， 他们想思考什么，

169
00:08:09.776 --> 00:08:12.566
and what they want to be feeling and how they want to be informed,
他们想感受什么， 他们想了解什么，

170
00:08:12.566 --> 00:08:14.696
and we're all just tugged into these other directions.
而我们被吸引到这些不同的方向。

171
00:08:14.696 --> 00:08:18.876
And you have a billion people just tugged into all these different directions.
数亿人会跟随着这些不同的方向。

172
00:08:18.876 --> 00:08:22.146
Well, imagine an entire design renaissance
试想一下一个完整的科技复兴

173
00:08:22.140 --> 00:08:25.236
that tried to orchestrate the exact and most empowering
将会指导我们准确有效的

174
00:08:25.260 --> 00:08:28.396
time-well-spent way for those timelines to happen.
分配时间。

175
00:08:28.420 --> 00:08:30.076
And that would involve two things:
这包含两个方面：

176
00:08:29.686 --> 00:08:32.306
one would be protecting against the timelines
一是防止我们把时间花在

177
00:08:32.260 --> 00:08:34.116
that we don't want to be experiencing,
在不想花的地方，

178
00:08:34.140 --> 00:08:36.556
the thoughts that we wouldn't want to be happening,
阻止我们产生不想形成的想法，

179
00:08:36.580 --> 00:08:39.916
so that when that ding happens, not having the ding that sends us away;
即便提示音响了， 我们也不会被牵着鼻子走；

180
00:08:39.940 --> 00:08:43.556
and the second would be empowering us to live out the timeline that we want.
二是让我们按照所期待的时间轨迹生活。

181
00:08:43.580 --> 00:08:45.460
So let me give you a concrete example.
给你们举个实际的例子。

182
00:08:46.100 --> 00:08:48.556
Today, let's say your friend cancels dinner on you,
今天，比如你的朋友 取消了和你共进晚餐，

183
00:08:48.580 --> 00:08:52.355
and you are feeling a little bit lonely.
你感到有些寂寞。

184
00:08:52.379 --> 00:08:54.196
And so what do you do in that moment?
这一刻你会做什么呢？

185
00:08:53.690 --> 00:08:55.499
You open up Facebook.
你打开了Facebook。

186
00:08:56.780 --> 00:08:58.476
And in that moment,
在那一刻，

187
00:08:58.500 --> 00:09:01.876
the designers in the control room want to schedule exactly one thing,
控制室里的设计者要做一件事，

188
00:09:01.900 --> 00:09:04.940
which is to maximize how much time you spend on the screen.
那就是要你盯着屏幕，时间越长越好。

189
00:09:06.460 --> 00:09:10.356
Now, instead, imagine if those designers created a different timeline
现在想象一下， 如果设计者规划了另一条时间轴，

190
00:09:10.380 --> 00:09:13.876
that was the easiest way, using all of their data,
利用他们所有的数据， 用最简单的方法

191
00:09:13.900 --> 00:09:16.996
to actually help you get out with the people that you care about?
帮你约出你关心的人呢？

192
00:09:17.020 --> 00:09:22.436
Just think, alleviating all loneliness in society,
试想如果消除社会中所有的孤单，

193
00:09:22.460 --> 00:09:25.956
if that was the timeline that Facebook wanted to make possible for people.
才是Facebook想要为人们实现的理想。

194
00:09:25.980 --> 00:09:27.695
Or imagine a different conversation.
或试想另一个对话。

195
00:09:27.719 --> 00:09:31.036
Let's say you wanted to post something supercontroversial on Facebook,
比方说你想要在Facebook上 发表备受争议的言论，

196
00:09:30.806 --> 00:09:32.686
which is a really important thing to be able to do,
能这么做是很重要的，

197
00:09:32.686 --> 00:09:34.926
to talk about controversial topics.
谈论争议性话题。

198
00:09:34.926 --> 00:09:37.300
And right now, when there's that big comment box,
现在有一个巨大的评论栏，

199
00:09:37.580 --> 00:09:40.956
it's almost asking you, what key do you want to type?
它就像是在问你， 你想要输入什么东西？

200
00:09:40.980 --> 00:09:43.796
In other words, it's scheduling a little timeline of things
换句话说，它设定了你将要

201
00:09:43.820 --> 00:09:45.956
you're going to continue to do on the screen.
继续在屏幕上做的事情。

202
00:09:45.980 --> 00:09:48.956
And imagine instead that there was another button there saying,
试想如果有另外一个提问框跳出来问你，

203
00:09:48.980 --> 00:09:51.036
what would be most time well spent for you?
怎么花费时间最好？

204
00:09:50.566 --> 00:09:52.696
And you click "host a dinner."
你点击 “办个晚餐聚会”

205
00:09:52.660 --> 00:09:54.756
And right there underneath the item it said,
紧接着下面有个选项问道，

206
00:09:54.780 --> 00:09:56.476
"Who wants to RSVP for the dinner?"
“谁想要去这个晚餐？”

207
00:09:56.500 --> 00:09:59.756
And so you'd still have a conversation about something controversial,
虽然你仍想对 有争议性的话题进行讨论，

208
00:09:59.780 --> 00:10:03.516
but you'd be having it in the most empowering place on your timeline,
但你的时间花在了最好的地方，

209
00:10:03.540 --> 00:10:06.556
which would be at home that night with a bunch of a friends over
晚上在家里和一群朋友

210
00:10:06.580 --> 00:10:07.780
to talk about it.
讨论那个话题。

211
00:10:08.820 --> 00:10:11.980
So imagine we're running, like, a find and replace
想象我们正在使用“查找并替代” 功能，

212
00:10:12.820 --> 00:10:15.396
on all of the timelines that are currently steering us
把所有那些正在促使我们

213
00:10:15.420 --> 00:10:17.980
towards more and more screen time persuasively
花越来越多的时间在屏幕上的事情，

214
00:10:18.900 --> 00:10:21.436
and replacing all of those timelines
用我们在生活中的真实意愿

215
00:10:21.460 --> 00:10:23.100
with what do we want in our lives.
来逐一替换掉。

216
00:10:26.780 --> 00:10:28.260
It doesn't have to be this way.
事情本不必如此复杂。

217
00:10:30.180 --> 00:10:32.436
Instead of handicapping our attention,
与阻碍我们的注意力相反，

218
00:10:32.460 --> 00:10:35.276
imagine if we used all of this data and all of this power
试想如果我们利用所有这些数据和功能，

219
00:10:35.300 --> 00:10:36.916
and this new view of human nature
加上对人类本性的全新认识，

220
00:10:36.940 --> 00:10:39.796
to give us a superhuman ability to focus
给我们以超人的能力来集中注意力，

221
00:10:39.820 --> 00:10:43.956
and a superhuman ability to put our attention to what we cared about
来关注我们应该关心的事，

222
00:10:43.980 --> 00:10:46.596
and a superhuman ability to have the conversations
来进行民主所需要的

223
00:10:46.620 --> 00:10:48.620
that we need to have for democracy.
互动交流。

224
00:10:51.420 --> 00:10:54.100
The most complex challenges in the world
世界上最复杂的挑战

225
00:10:56.100 --> 00:10:59.220
require not just us to use our attention individually.
要求我们不仅独立的运用我们的注意力，

226
00:11:00.260 --> 00:11:03.580
They require us to use our attention and coordinate it together.
还要求我们同心协力。

227
00:11:04.260 --> 00:11:07.076
Climate change is going to require that a lot of people
气候变化需要许多人

228
00:11:07.100 --> 00:11:09.196
are being able to coordinate their attention
使用最有力的方式

229
00:11:09.220 --> 00:11:11.116
in the most empowering way together.
彼此协作。

230
00:11:11.140 --> 00:11:14.220
And imagine creating a superhuman ability to do that.
试想创造出这种超人能力会是什么情景。

231
00:11:18.820 --> 00:11:22.980
Sometimes the world's most pressing and important problems
有时世界上最紧要，最关键的问题

232
00:11:23.860 --> 00:11:27.700
are not these hypothetical future things that we could create in the future.
不是假象出来的未来事物。

233
00:11:28.380 --> 00:11:30.116
Sometimes the most pressing problems
有时最紧要的东西

234
00:11:29.856 --> 00:11:32.526
are the ones that are right underneath our noses,
恰恰是我们眼前的东西，

235
00:11:32.500 --> 00:11:35.620
the things that are already directing a billion people's thoughts.
是已经影响了上亿人思想的东西。

236
00:11:36.420 --> 00:11:39.796
And maybe instead of getting excited about the new augmented reality
也许，与其对新兴的增强现实，

237
00:11:39.820 --> 00:11:43.116
and virtual reality and these cool things that could happen,
虚拟现实等炫酷玩物感到着迷，

238
00:11:43.140 --> 00:11:46.436
which are going to be susceptible to the same race for attention,
使得它们也会成为 注意力竞争中的一员，

239
00:11:46.460 --> 00:11:48.636
if we could fix the race for attention
我们不如改进亿万人口袋里

240
00:11:48.660 --> 00:11:51.380
on the thing that's already in a billion people's pockets.
影响注意力的那个东西。

241
00:11:51.860 --> 00:11:53.436
Maybe instead of getting excited
也许，我们与其对

242
00:11:53.460 --> 00:11:57.636
about the most exciting new cool fancy education apps,
新潮炫酷的教育软件感到兴奋，

243
00:11:57.660 --> 00:12:00.556
we could fix the way kids' minds are getting manipulated
还不如想想如何拯救那些已被操纵，

244
00:12:00.580 --> 00:12:03.060
into sending empty messages back and forth.
来回发送空洞消息的孩子们。

245
00:12:03.860 --> 00:12:08.156
(Applause)
（掌声）

246
00:12:08.180 --> 00:12:09.436
Maybe instead of worrying
也许，我们与其

247
00:12:09.460 --> 00:12:13.236
about hypothetical future runaway artificial intelligences
对假象的未来中的最大主角，

248
00:12:13.260 --> 00:12:15.140
that are maximizing for one goal,
人工智能感到担心，

249
00:12:16.500 --> 00:12:19.156
we could solve the runaway artificial intelligence
不如解决当下就存在的

250
00:12:19.180 --> 00:12:21.236
that already exists right now,
人工智能问题，

251
00:12:21.260 --> 00:12:24.180
which are these newsfeeds maximizing for one thing.
就是那些争夺注意力的新闻推送。

252
00:12:25.900 --> 00:12:29.716
It's almost like instead of running away to colonize new planets,
这就好比与其逃跑和殖民新的星球，

253
00:12:29.740 --> 00:12:31.796
we could fix the one that we're already on.
我们可以解决现在所在星球的问题。

254
00:12:31.820 --> 00:12:35.940
(Applause)
（掌声）

255
00:12:39.860 --> 00:12:41.636
Solving this problem
解决这个问题

256
00:12:41.660 --> 00:12:45.460
is critical infrastructure for solving every other problem.
是解决其他问题的关键所在。

257
00:12:46.420 --> 00:12:50.436
There's nothing in your life or in our collective problems
在你生命之中或是我们共同的问题中，

258
00:12:50.460 --> 00:12:54.020
that does not require our ability to put our attention where we care about.
没有一件事不需要我们 把注意力放到我们关心的事情上去。

259
00:12:55.620 --> 00:12:56.860
At the end of our lives,
归根到底，在一生中，

260
00:12:58.060 --> 00:13:00.700
all we have is our attention and our time.
我们共同拥有的就是注意力和时间。

261
00:13:01.620 --> 00:13:03.516
What will be time well spent for ours?
时间会被我们自己充分利用吗？

262
00:13:03.540 --> 00:13:04.756
Thank you.
谢谢大家。

263
00:13:04.780 --> 00:13:07.900
(Applause)
（掌声）

264
00:13:17.580 --> 00:13:20.516
Chris Anderson: Tristan, thank you. Hey, stay up here a sec.
克里斯•安德森： Tristan，谢谢你，请留步。

265
00:13:20.540 --> 00:13:21.876
First of all, thank you.
首先，谢谢你。

266
00:13:21.900 --> 00:13:24.676
I know we asked you to do this talk on pretty short notice,
我知道我们很晚才通知你要做这次演讲，

267
00:13:24.700 --> 00:13:26.916
and you've had quite a stressful week
这周准备这个演讲，

268
00:13:26.940 --> 00:13:29.380
getting this thing together, so thank you.
压力确实很大，再次谢谢你。

269
00:13:30.500 --> 00:13:34.476
Some people listening might say, what you complain about is addiction,
有些听众可能会说， 你抱怨的不就是上瘾吗，

270
00:13:34.500 --> 00:13:37.996
and all these people doing this stuff, for them it's actually interesting.
而对于真正这么做的人来说， 他们确确实实是觉得有趣的。

271
00:13:37.710 --> 00:13:38.876
All these design decisions
所有这些设计策划

272
00:13:38.876 --> 00:13:42.486
have built user content that is fantastically interesting.
使得用户获取了十分有趣的内容。

273
00:13:42.420 --> 00:13:44.836
The world's more interesting than it ever has been.
这个世界从未如此有意思过。

274
00:13:44.860 --> 00:13:46.116
What's wrong with that?
这有什么问题吗？

275
00:13:45.890 --> 00:13:47.366
Tristan Harris: I think it's really interesting.
特里斯坦•哈里斯： 我认为这确实有趣。

276
00:13:47.366 --> 00:13:52.496
One way to see this is if you're just YouTube, for example,
换个角度思考一下 举个例子，还用YouTube，

277
00:13:52.460 --> 00:13:55.116
you want to always show the more interesting next video.
你总是想让下一个视频更有趣。

278
00:13:55.140 --> 00:13:58.156
You want to get better and better at suggesting that next video,
你想要在建议下一个视频 这件事上越做越好，

279
00:13:57.276 --> 00:13:59.716
but even if you could propose the perfect next video
但即使你可以建议一个

280
00:14:00.660 --> 00:14:02.316
that everyone would want to watch,
所有人都想要观看的完美视频，

281
00:14:01.286 --> 00:14:04.290
it would just be better and better at keeping you hooked on the screen.
其实只是在吸引人们 盯住屏幕这件事上越来越好。

282
00:14:04.290 --> 00:14:05.956
So what's missing in that equation
所以这个等式缺失的是

283
00:14:05.956 --> 00:14:08.876
is figuring out what our boundaries would be.
知道我们的极限在哪里。

284
00:14:08.876 --> 00:14:11.450
You would want YouTube to know something about, say, falling asleep.
比如，你想要让YouTube对 入睡这样的事儿有所了解。

285
00:14:11.450 --> 00:14:13.126
The CEO of Netflix recently said,
Netflix的CEO最近说过，

286
00:14:13.126 --> 00:14:17.216
"our biggest competitors are Facebook, YouTube and sleep."
“我们最大的对手是Facebook， YouTube和睡眠。”

287
00:14:17.180 --> 00:14:21.636
And so what we need to recognize is that the human architecture is limited
所以我们需要认识到 人体本身是有极限的，

288
00:14:21.660 --> 00:14:24.636
and that we have certain boundaries or dimensions of our lives
在我们的生活中有某些界限或方面

289
00:14:24.660 --> 00:14:26.636
that we want to be honored and respected,
是要得到尊重的，

290
00:14:26.660 --> 00:14:28.476
and technology could help do that.
而科技是可以帮助我们实现这些的。

291
00:14:28.500 --> 00:14:31.116
(Applause)
（掌声）

292
00:14:31.140 --> 00:14:32.836
CA: I mean, could you make the case
CA：你是否可以说明一下

293
00:14:32.860 --> 00:14:38.916
that part of the problem here is that we've got a naïve model of human nature?
我们是不是对人性的认识太天真了？

294
00:14:38.940 --> 00:14:41.676
So much of this is justified in terms of human preference,
从人类偏好来看， 这些东西是可以合理化的，

295
00:14:41.700 --> 00:14:44.316
where we've got these algorithms that do an amazing job
我们有这些极其棒的算法

296
00:14:43.786 --> 00:14:45.936
of optimizing for human preference,
为人类的偏好优化上做着贡献，

297
00:14:45.936 --> 00:14:47.466
but which preference?
但是，是什么偏好呢？

298
00:14:47.420 --> 00:14:50.916
There's the preferences of things that we really care about
有我们确实关心的

299
00:14:50.940 --> 00:14:52.316
when we think about them
偏好，

300
00:14:52.340 --> 00:14:55.396
versus the preferences of what we just instinctively click on.
也有我们只是下意识点击的偏好。

301
00:14:55.420 --> 00:15:00.076
If we could implant that more nuanced view of human nature in every design,
如果我们在每个设计中 多加一点对人类本性的关注，

302
00:15:00.100 --> 00:15:01.556
would that be a step forward?
这会不会是一种进步呢？

303
00:15:01.580 --> 00:15:03.556
TH: Absolutely. I mean, I think right now
TH：那是肯定的。我认为现在

304
00:15:03.580 --> 00:15:07.076
it's as if all of our technology is basically only asking our lizard brain
科技基本上只是利用我们的下意识，

305
00:15:06.890 --> 00:15:09.626
what's the best way to just impulsively get you to do
找到最好的方式让我们自然而然地

306
00:15:09.620 --> 00:15:11.756
the next tiniest thing with your time,
对微不足道的事上瘾，

307
00:15:11.780 --> 00:15:13.436
instead of asking you in your life
而不是问我们在生活中

308
00:15:13.460 --> 00:15:15.636
what we would be most time well spent for you?
时间要如何花费才最有意义。

309
00:15:15.660 --> 00:15:18.956
What would be the perfect timeline that might include something later,
什么才是完美的时间安排， 这可能包括后面的事情，

310
00:15:17.766 --> 00:15:21.026
would be time well spent for you here at TED in your last day here?
例如你利用在这儿的最后一天 参加TED，是很好的利用了时间吗？

311
00:15:21.026 --> 00:15:24.796
CA: So if Facebook and Google and everyone said to us first up,
CA：如果Facebook和Google 或任何一个人上来对我们说，

312
00:15:24.796 --> 00:15:28.006
"Hey, would you like us to optimize for your reflective brain
“嘿，你想要我们为你的思考进行优化，

313
00:15:28.100 --> 00:15:29.756
or your lizard brain? You choose."
还是优化你的下意识？你来选择。”

314
00:15:29.780 --> 00:15:31.860
TH: Right. That would be one way. Yes.
TH：是的。这可能是个方法。

315
00:15:34.178 --> 00:15:37.036
CA: You said persuadability, that's an interesting word to me
CA：你说到可说服性， 对我来说这个词很有趣，

316
00:15:37.060 --> 00:15:39.916
because to me there's two different types of persuadability.
因为在我看来有两种不同的可说服性。

317
00:15:39.940 --> 00:15:42.476
There's the persuadability that we're trying right now
有一种可说服性是我们现在正尝试的，

318
00:15:42.500 --> 00:15:44.676
of reason and thinking and making an argument,
关于推理，思考以及做出论证，

319
00:15:44.700 --> 00:15:47.396
but I think you're almost talking about a different kind,
但是我觉得你在谈论另一种，

320
00:15:46.456 --> 00:15:48.586
a more visceral type of persuadability,
更加内在的本能的一种可说服性，

321
00:15:48.586 --> 00:15:51.056
of being persuaded without even knowing that you're thinking.
即在不经思考下就被说服了。

322
00:15:51.056 --> 00:15:53.906
TH: Exactly. The reason I care about this problem so much is
TH：是的。我十分关注 这个问题的原因是，

323
00:15:53.906 --> 00:15:57.720
I studied at a lab called the Persuasive Technology Lab at Stanford
我曾在斯坦福的 说服性技术实验室学习，

324
00:15:57.720 --> 00:16:00.006
that taught [students how to recognize] exactly these techniques.
那里就是教学生这些技巧。

325
00:16:00.926 --> 00:16:03.916
There's conferences and workshops that teach people all these covert ways
那有专门的论坛和讨论会， 教授人们如何用隐蔽的方式

326
00:16:03.940 --> 00:16:06.916
of getting people's attention and orchestrating people's lives.
来获得人们的注意力， 从而策划人们的生活。

327
00:16:06.940 --> 00:16:09.596
And it's because most people don't know that that exists
正因为大部分人不知道它的存在，

328
00:16:08.856 --> 00:16:10.936
that this conversation is so important.
才使得这个演讲如此重要。

329
00:16:11.540 --> 00:16:15.316
CA: Tristan, you and I, we both know so many people from all these companies.
CA：Tristan，咱们都认识 许多来自这些公司的人。

330
00:16:15.340 --> 00:16:17.316
There are actually many here in the room,
他们当中许多人也在这里，

331
00:16:17.340 --> 00:16:19.817
and I don't know about you, but my experience of them
我不知道你的想法， 但是就我的经验而言，

332
00:16:19.841 --> 00:16:21.916
is that there is no shortage of good intent.
他们是心存善意的。

333
00:16:21.940 --> 00:16:24.116
People want a better world.
大家都向往更美好的世界。

334
00:16:24.140 --> 00:16:27.660
They are actually -- they really want it.
他们确实是这样想的。

335
00:16:28.140 --> 00:16:32.316
And I don't think anything you're saying is that these are evil people.
我不认为你的意思是 这些人都是坏人。

336
00:16:32.340 --> 00:16:36.036
It's a system where there's these unintended consequences
只不过是这个系统产生了不经意的结果，

337
00:16:35.970 --> 00:16:37.186
that have really got out of control --
超出了我们的控制范围——

338
00:16:37.940 --> 00:16:39.436
TH: Of this race for attention.
TH：在争夺注意力 这件事儿上，没错儿。

339
00:16:38.436 --> 00:16:41.500
It's the classic race to the bottom when you have to get attention,
当你要获得众人的注意力时， 便成了经典的厮杀，

340
00:16:41.500 --> 00:16:43.406
and it's so tense.
而且特别激烈残忍。

341
00:16:43.900 --> 00:16:46.636
The only way to get more is to go lower on the brain stem,
取得更多注意的唯一办法， 只有深入大脑，

342
00:16:46.660 --> 00:16:49.076
to go lower into outrage, to go lower into emotion,
深入愤怒，深入情感，

343
00:16:49.100 --> 00:16:50.796
to go lower into the lizard brain.
深入本性。

344
00:16:50.820 --> 00:16:54.636
CA: Well, thank you so much for helping us all get a little bit wiser about this.
CA：感谢你帮助我们 对这个问题有了更深的认识。

345
00:16:54.660 --> 00:16:57.076
Tristan Harris, thank you. TH: Thank you very much.
特里斯坦•哈里斯，谢谢你。 TH：非常感谢。

346
00:16:56.330 --> 00:16:59.340
(Applause)
（掌声）