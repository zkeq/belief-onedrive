WEBVTT

1
00:00:13.288 --> 00:00:18.510
So, on April 23 of 2013,
2013 年 4 月 23 日，

2
00:00:18.534 --> 00:00:24.048
the Associated Press put out the following tweet on Twitter.
美联社在推特上发布了 这样一条推文：

3
00:00:24.072 --> 00:00:26.469
It said, "Breaking news:
“突发新闻：

4
00:00:26.493 --> 00:00:29.064
Two explosions at the White House
白宫发生两起爆炸，

5
00:00:29.088 --> 00:00:31.421
and Barack Obama has been injured."
巴拉克·奥巴马受伤。”

6
00:00:32.032 --> 00:00:37.457
This tweet was retweeted 4,000 times in less than five minutes,
在不到五分钟的时间里， 这条推文被转发了四千次，

7
00:00:37.481 --> 00:00:39.698
and it went viral thereafter.
随后也在网络上被疯传。

8
00:00:40.580 --> 00:00:44.930
Now, this tweet wasn't real news put out by the Associated Press.
不过，这条推文并不是 美联社发布的真实新闻。

9
00:00:44.954 --> 00:00:48.287
In fact it was false news, or fake news,
事实上，这是一则不实新闻， 或者说是虚假新闻，

10
00:00:48.311 --> 00:00:51.136
that was propagated by Syrian hackers
是由入侵了美联社推特账号

11
00:00:51.160 --> 00:00:55.854
that had infiltrated the Associated Press Twitter handle.
的叙利亚黑客扩散的。

12
00:00:56.227 --> 00:01:00.116
Their purpose was to disrupt society, but they disrupted much more.
他们的目的是扰乱社会， 但他们扰乱的远不止于此。

13
00:01:00.140 --> 00:01:02.616
Because automated trading algorithms
因为自动交易算法

14
00:01:02.640 --> 00:01:06.000
immediately seized on the sentiment on this tweet,
立刻捕捉了这条推文的情感， 【注：机器学习中对主观性文本的情感分析】

15
00:01:06.024 --> 00:01:08.992
and began trading based on the potential
并且根据美国总统在这次爆炸中

16
00:01:09.016 --> 00:01:12.397
that the president of the United States had been injured or killed
受伤或丧生的可能性，

17
00:01:12.421 --> 00:01:13.621
in this explosion.
开始了交易。

18
00:01:14.008 --> 00:01:16.000
And as they started tweeting,
而当他们开始发推时，

19
00:01:16.024 --> 00:01:19.373
they immediately sent the stock market crashing,
股市迅速随之崩盘，

20
00:01:19.397 --> 00:01:24.564
wiping out 140 billion dollars in equity value in a single day.
一日之内便蒸发了 1400 亿美元的市值。

21
00:01:24.882 --> 00:01:29.358
Robert Mueller, special counsel prosecutor in the United States,
美国特别检察官罗伯特·穆勒

22
00:01:29.382 --> 00:01:33.274
issued indictments against three Russian companies
起诉了三家俄罗斯公司

23
00:01:33.298 --> 00:01:35.917
and 13 Russian individuals
以及十三个俄罗斯人，

24
00:01:35.941 --> 00:01:39.108
on a conspiracy to defraud the United States
指控他们干预 2016 年美国总统大选，

25
00:01:39.132 --> 00:01:42.912
by meddling in the 2016 presidential election.
合谋诓骗美国。

26
00:01:43.675 --> 00:01:47.239
And what this indictment tells as a story
而这次起诉讲述的

27
00:01:47.263 --> 00:01:50.405
is the story of the Internet Research Agency,
是互联网研究机构的故事，

28
00:01:50.429 --> 00:01:54.023
the shadowy arm of the Kremlin on social media.
即俄罗斯政府在社交媒体上 布下的影影绰绰的手腕。

29
00:01:54.635 --> 00:01:57.412
During the presidential election alone,
仅在总统大选期间，

30
00:01:57.436 --> 00:01:59.325
the Internet Agency's efforts
互联网机构就

31
00:01:59.349 --> 00:02:04.516
reached 126 million people on Facebook in the United States,
影响了 1.26 亿名 美国 Facebook 用户，

32
00:02:04.540 --> 00:02:07.817
issued three million individual tweets
发布了 300 万条推文，

33
00:02:07.841 --> 00:02:11.683
and 43 hours' worth of YouTube content.
以及 43 个小时的 Youtube 内容。

34
00:02:11.707 --> 00:02:13.359
All of which was fake --
这一切都是虚假的——

35
00:02:13.383 --> 00:02:19.706
misinformation designed to sow discord in the US presidential election.
通过精心设计的虚假信息， 在美国总统大选中播下不和的种子。

36
00:02:20.816 --> 00:02:23.466
A recent study by Oxford University
牛津大学最近的一项研究显示，

37
00:02:23.490 --> 00:02:26.760
showed that in the recent Swedish elections,
在近期的瑞典大选中，

38
00:02:26.784 --> 00:02:31.159
one third of all of the information spreading on social media
在社交媒体上传播 的关于大选的信息中，

39
00:02:31.183 --> 00:02:32.381
about the election
有三分之一

40
00:02:32.405 --> 00:02:34.492
was fake or misinformation.
是虚假或谬误信息。

41
00:02:34.857 --> 00:02:39.935
In addition, these types of social-media misinformation campaigns
另外，这些通过社交媒体 进行的误导活动

42
00:02:39.959 --> 00:02:44.110
can spread what has been called "genocidal propaganda,"
可以传播所谓的“种族清洗宣传”，

43
00:02:44.134 --> 00:02:47.245
for instance against the Rohingya in Burma,
例如在缅甸煽动对罗兴亚人的迫害，

44
00:02:47.269 --> 00:02:49.572
triggering mob killings in India.
或者在印度引发暴徒杀人。

45
00:02:49.596 --> 00:02:51.090
We studied fake news
我们在虚假新闻变成热点之前

46
00:02:51.114 --> 00:02:54.333
and began studying it before it was a popular term.
就开始了对虚假新闻的研究。

47
00:02:54.850 --> 00:02:59.890
And we recently published the largest-ever longitudinal study
最近，我们发表了一项 迄今最大型的关于虚假新闻

48
00:03:00.020 --> 00:03:02.200
of the spread of fake news online
在网络传播的纵向研究，

49
00:03:02.224 --> 00:03:05.428
on the cover of "Science" in March of this year.
在今年三月登上了《科学》期刊封面。

50
00:03:06.343 --> 00:03:10.504
We studied all of the verified true and false news stories
我们研究了推特上传播的所有

51
00:03:10.528 --> 00:03:12.281
that ever spread on Twitter,
核实过的真假新闻，

52
00:03:12.305 --> 00:03:16.123
from its inception in 2006 to 2017.
范围是自 2006 年推特创立到 2017 年。

53
00:03:16.432 --> 00:03:18.746
And when we studied this information,
在我们研究这些讯息时，

54
00:03:18.770 --> 00:03:21.646
we studied verified news stories
我们通过六家独立的 事实核查机构验证，

55
00:03:21.670 --> 00:03:25.588
that were verified by six independent fact-checking organizations.
以确认新闻故事的真实性。

56
00:03:25.612 --> 00:03:28.374
So we knew which stories were true
所以我们清楚哪些新闻是真的，

57
00:03:28.398 --> 00:03:30.524
and which stories were false.
哪些是假的。

58
00:03:30.548 --> 00:03:32.421
We can measure their diffusion,
我们可以测量 这些新闻的扩散程度，

59
00:03:32.445 --> 00:03:34.096
the speed of their diffusion,
扩散速度，

60
00:03:34.120 --> 00:03:36.215
the depth and breadth of their diffusion,
以及深度与广度，

61
00:03:36.239 --> 00:03:40.381
how many people become entangled in this information cascade and so on.
有多少人被卷入这个信息级联。 【注：人们加入信息更具说服力的团体】

62
00:03:40.762 --> 00:03:42.246
And what we did in this paper
我们在这篇论文中

63
00:03:42.270 --> 00:03:46.135
was we compared the spread of true news to the spread of false news.
比较了真实新闻和 虚假新闻的传播程度。

64
00:03:46.159 --> 00:03:47.842
And here's what we found.
这是我们的研究发现。

65
00:03:47.866 --> 00:03:51.845
We found that false news diffused further, faster, deeper
我们发现，在我们研究 的所有新闻类别中，

66
00:03:51.869 --> 00:03:53.675
and more broadly than the truth
虚假新闻都比真实新闻传播得

67
00:03:53.699 --> 00:03:56.702
in every category of information that we studied,
更远、更快、更深、更广，

68
00:03:56.726 --> 00:03:59.225
sometimes by an order of magnitude.
有时甚至超出一个数量级。

69
00:03:59.662 --> 00:04:03.186
And in fact, false political news was the most viral.
事实上，虚假的政治新闻 传播速度最快。

70
00:04:03.210 --> 00:04:06.357
It diffused further, faster, deeper and more broadly
它比任何其他种类的虚假新闻

71
00:04:06.381 --> 00:04:09.183
than any other type of false news.
都扩散得更远、更快、更深、更广。

72
00:04:09.207 --> 00:04:10.500
When we saw this,
我们看到这个结果时，

73
00:04:10.524 --> 00:04:13.365
we were at once worried but also curious.
我们立刻感到担忧， 但同时也很好奇。

74
00:04:13.389 --> 00:04:14.540
Why?
为什么？

75
00:04:14.564 --> 00:04:17.937
Why does false news travel so much further, faster, deeper
为什么虚假新闻比真相

76
00:04:17.961 --> 00:04:19.825
and more broadly than the truth?
传播得更远、更快、更深、更广？

77
00:04:20.159 --> 00:04:23.120
The first hypothesis that we came up with was,
我们想到的第一个假设是，

78
00:04:23.144 --> 00:04:27.936
"Well, maybe people who spread false news have more followers or follow more people,
“可能传播虚假新闻的人 有更多的关注者，或者关注了更多人，

79
00:04:27.960 --> 00:04:29.517
or tweet more often,
或者发推更频繁，

80
00:04:29.541 --> 00:04:33.667
or maybe they're more often 'verified' users of Twitter, with more credibility,
或者他们中有更多 推特的‘认证’用户，可信度更高，

81
00:04:33.691 --> 00:04:35.873
or maybe they've been on Twitter longer."
或者他们在推特上的时间更长。”

82
00:04:35.897 --> 00:04:38.195
So we checked each one of these in turn.
因此，我们挨个检验了这些假设。

83
00:04:38.511 --> 00:04:41.431
And what we found was exactly the opposite.
我们发现，结果恰恰相反。

84
00:04:41.455 --> 00:04:43.891
False-news spreaders had fewer followers,
假新闻散布者有更少关注者，

85
00:04:43.915 --> 00:04:46.169
followed fewer people, were less active,
关注的人更少，活跃度更低，

86
00:04:46.193 --> 00:04:47.653
less often "verified"
更少被“认证”，

87
00:04:47.677 --> 00:04:50.637
and had been on Twitter for a shorter period of time.
使用推特的时间更短。

88
00:04:50.661 --> 00:04:51.850
And yet,
然而，

89
00:04:51.874 --> 00:04:56.907
false news was 70 percent more likely to be retweeted than the truth,
在控制了这些和很多其他变量之后，

90
00:04:56.931 --> 00:05:00.294
controlling for all of these and many other factors.
虚假新闻比真实新闻 被转发的可能性高出了 70%。

91
00:05:00.318 --> 00:05:03.008
So we had to come up with other explanations.
我们不得不提出别的解释。

92
00:05:03.032 --> 00:05:06.499
And we devised what we called a "novelty hypothesis."
于是，我们设想了一个 “新颖性假设”。

93
00:05:06.858 --> 00:05:08.818
So if you read the literature,
如果各位对文献有所了解，

94
00:05:08.842 --> 00:05:12.596
it is well known that human attention is drawn to novelty,
会知道一个广为人知的现象是， 人类的注意力会被新颖性所吸引，

95
00:05:12.620 --> 00:05:15.139
things that are new in the environment.
也就是环境中的新事物。

96
00:05:15.163 --> 00:05:17.148
And if you read the sociology literature,
如果各位了解社会学文献的话，

97
00:05:17.172 --> 00:05:21.472
you know that we like to share novel information.
你们应该知道，我们喜欢分享 新鲜的信息。

98
00:05:21.496 --> 00:05:25.334
It makes us seem like we have access to inside information,
这使我们看上去像是 能够获得内部消息，

99
00:05:25.358 --> 00:05:29.143
and we gain in status by spreading this kind of information.
通过传播这类信息， 我们的地位可以获得提升。

100
00:05:29.612 --> 00:05:36.064
So what we did was we measured the novelty of an incoming true or false tweet,
因此我们把刚收到的真假推文

101
00:05:35.954 --> 00:05:39.933
compared to the corpus of what that individual had seen
和用户前 60 天内 在推特上看过的语库比较，

102
00:05:39.933 --> 00:05:43.119
in the 60 days prior on Twitter.
以衡量刚收到的推文的新颖度。

103
00:05:43.143 --> 00:05:45.802
But that wasn't enough, because we thought to ourselves,
但这还不够， 因为我们想到，

104
00:05:45.826 --> 00:05:50.034
"Well, maybe false news is more novel in an information-theoretic sense,
“可能在信息论的层面 虚假新闻更加新颖，

105
00:05:50.058 --> 00:05:53.316
but maybe people don't perceive it as more novel."
但也许在人们的感知里， 它并没有很新鲜。”

106
00:05:53.669 --> 00:05:57.596
So to understand people's perceptions of false news,
因此，为了理解 人们对虚假新闻的感知，

107
00:05:57.620 --> 00:06:01.310
we looked at the information and the sentiment
我们研究了对真假推文的回复中

108
00:06:01.334 --> 00:06:05.540
contained in the replies to true and false tweets.
包含的信息和情感。

109
00:06:05.842 --> 00:06:07.048
And what we found
我们发现，

110
00:06:07.072 --> 00:06:11.286
was that across a bunch of different measures of sentiment --
在多种不同的情感量表上——

111
00:06:11.310 --> 00:06:14.611
surprise, disgust, fear, sadness,
惊讶，厌恶，恐惧，悲伤，

112
00:06:14.635 --> 00:06:17.119
anticipation, joy and trust --
期待，喜悦，信任——

113
00:06:17.143 --> 00:06:23.000
false news exhibited significantly more surprise and disgust
对虚假新闻的回复里 明显表现出了

114
00:06:23.024 --> 00:06:25.830
in the replies to false tweets.
更多的惊讶和厌恶。

115
00:06:26.212 --> 00:06:30.001
And true news exhibited significantly more anticipation,
而对真实新闻的回复里，

116
00:06:30.025 --> 00:06:31.572
joy and trust
表现出的则是

117
00:06:31.596 --> 00:06:34.143
in reply to true tweets.
更多的期待、喜悦，和信任。

118
00:06:34.167 --> 00:06:37.953
The surprise corroborates our novelty hypothesis.
这个意外事件证实了 我们的新颖性假设。

119
00:06:37.977 --> 00:06:42.586
This is new and surprising, and so we're more likely to share it.
这很新鲜、很令人惊讶， 所以我们更可能把它分享出去。

120
00:06:42.912 --> 00:06:45.837
At the same time, there was congressional testimony
同时，在美国国会两院前 进行的国会作证

121
00:06:45.861 --> 00:06:48.897
in front of both houses of Congress in the United States,
提到了机器人账号（注：一种使用 自动化脚本执行大量简单任务的软件）

122
00:06:48.921 --> 00:06:52.659
looking at the role of bots in the spread of misinformation.
在传播虚假信息时的作用。

123
00:06:52.683 --> 00:06:54.037
So we looked at this too --
因此我们也对这一点进行了研究——

124
00:06:54.061 --> 00:06:57.659
we used multiple sophisticated bot-detection algorithms
我们使用多个复杂的 机器人账号探测算法，

125
00:06:57.683 --> 00:07:00.757
to find the bots in our data and to pull them out.
寻找并提取出了 我们数据中的机器人账号。

126
00:07:01.167 --> 00:07:03.826
So we pulled them out, we put them back in
我们把机器人账号移除， 再把它们放回去，

127
00:07:03.850 --> 00:07:06.969
and we compared what happens to our measurement.
并比较其对我们的测量 产生的影响。

128
00:07:06.993 --> 00:07:09.286
And what we found was that, yes indeed,
我们发现，确实，

129
00:07:09.310 --> 00:07:12.992
bots were accelerating the spread of false news online,
机器人账号加速了 虚假新闻在网络上的传播，

130
00:07:13.016 --> 00:07:15.667
but they were accelerating the spread of true news
但它们也在以大约相同的速度

131
00:07:15.691 --> 00:07:18.096
at approximately the same rate.
加速真实新闻的传播。

132
00:07:18.120 --> 00:07:20.978
Which means bots are not responsible
这意味着，机器人账号

133
00:07:21.002 --> 00:07:25.715
for the differential diffusion of truth and falsity online.
并不是造成网上虚实信息 传播差距的原因。

134
00:07:25.739 --> 00:07:28.588
We can't abdicate that responsibility,
我们不能推脱这个责任，

135
00:07:28.612 --> 00:07:32.871
because we, humans, are responsible for that spread.
因为要对这种传播负责的， 是我们人类自己。

136
00:07:34.292 --> 00:07:37.626
Now, everything that I have told you so far,
对于我们大家来说 都很不幸的是，

137
00:07:37.650 --> 00:07:39.404
unfortunately for all of us,
刚刚我告诉各位的一切

138
00:07:39.428 --> 00:07:40.689
is the good news.
都是好消息。

139
00:07:42.490 --> 00:07:46.940
The reason is because it's about to get a whole lot worse.
原因在于，形势马上要大幅恶化了。

140
00:07:47.670 --> 00:07:51.352
And two specific technologies are going to make it worse.
而两种特定的技术 会将形势变得更加糟糕。

141
00:07:52.027 --> 00:07:57.199
We are going to see the rise of a tremendous wave of synthetic media.
我们将会目睹 一大波合成媒体的剧增。

142
00:07:57.223 --> 00:08:03.254
Fake video, fake audio that is very convincing to the human eye.
虚假视频、虚假音频， 对于人类来说都能以假乱真。

143
00:08:03.278 --> 00:08:06.032
And this will powered by two technologies.
这是由两项技术支持的。

144
00:08:06.056 --> 00:08:09.889
The first of these is known as "generative adversarial networks."
其一是所谓的“生成对抗网络”。

145
00:08:09.913 --> 00:08:12.476
This is a machine-learning model with two networks:
这是一个由两个网络组成 的机器学习模型：

146
00:08:12.500 --> 00:08:14.047
a discriminator,
一个是判别网络，

147
00:08:14.071 --> 00:08:18.271
whose job it is to determine whether something is true or false,
负责分辨样本的真假；

148
00:08:18.295 --> 00:08:19.462
and a generator,
另一个是生成网络，

149
00:08:19.486 --> 00:08:22.636
whose job it is to generate synthetic media.
负责产生合成媒体。

150
00:08:22.660 --> 00:08:27.762
So the synthetic generator generates synthetic video or audio,
生成网络产生 合成视频或音频，

151
00:08:27.786 --> 00:08:32.461
and the discriminator tries to tell, "Is this real or is this fake?"
而判别网络则试图分辨， “这是真的还是假的？”

152
00:08:32.485 --> 00:08:35.359
And in fact, it is the job of the generator
事实上，生成网络的任务是

153
00:08:35.383 --> 00:08:39.818
to maximize the likelihood that it will fool the discriminator
尽可能地欺骗判别网络， 让判别网络误以为

154
00:08:39.842 --> 00:08:43.429
into thinking the synthetic video and audio that it is creating
它合成的视频和音频

155
00:08:43.453 --> 00:08:45.183
is actually true.
其实是真的。

156
00:08:45.207 --> 00:08:47.580
Imagine a machine in a hyperloop,
想象一台处于超级循环中的机器，

157
00:08:47.604 --> 00:08:50.407
trying to get better and better at fooling us.
试图变得越来越擅长欺骗我们。

158
00:08:50.934 --> 00:08:53.434
This, combined with the second technology,
第二项技术， 简而言之，

159
00:08:53.458 --> 00:08:59.180
which is essentially the democratization of artificial intelligence to the people,
就是在民众中 的人工智能的民主化，

160
00:08:59.204 --> 00:09:01.393
the ability for anyone,
即让任何人

161
00:09:01.417 --> 00:09:04.247
without any background in artificial intelligence
不需要任何人工智能或

162
00:09:04.271 --> 00:09:05.453
or machine learning,
机器学习的背景，

163
00:09:05.477 --> 00:09:09.580
to deploy these kinds of algorithms to generate synthetic media
也能调用这些算法 生成人工合成媒体。

164
00:09:09.604 --> 00:09:14.151
makes it ultimately so much easier to create videos.
这两种技术相结合， 让制作视频变得如此容易。

165
00:09:14.175 --> 00:09:18.596
The White House issued a false, doctored video
白宫曾发布过一个 虚假的、篡改过的视频，

166
00:09:18.620 --> 00:09:22.908
of a journalist interacting with an intern who was trying to take his microphone.
内容为一名记者和一个试图抢夺 他的麦克风的实习生的互动。

167
00:09:23.247 --> 00:09:25.246
They removed frames from this video
他们从视频中移除了一些帧，

168
00:09:25.270 --> 00:09:28.557
in order to make his actions seem more punchy.
让他的行动显得更有攻击性。

169
00:09:28.977 --> 00:09:32.362
And when videographers and stuntmen and women
而当摄影师和替身演员

170
00:09:32.386 --> 00:09:34.813
were interviewed about this type of technique,
被采访问及这种技术时，

171
00:09:34.837 --> 00:09:38.665
they said, "Yes, we use this in the movies all the time
他们说，“是的，我们经常 在电影中使用这种技术,

172
00:09:38.689 --> 00:09:43.452
to make our punches and kicks look more choppy and more aggressive."
让我们的出拳和踢腿动作 看上去更具打击感，更加有气势。”

173
00:09:44.088 --> 00:09:45.955
They then put out this video
他们于是发布了这个视频，

174
00:09:45.979 --> 00:09:48.479
and partly used it as justification
将其作为部分证据，

175
00:09:48.503 --> 00:09:52.502
to revoke Jim Acosta, the reporter's, press pass
试图撤销视频中的记者， 吉姆·阿考斯塔

176
00:09:51.962 --> 00:09:53.842
from the White House.
的白宫新闻通行证。

177
00:09:53.889 --> 00:09:58.698
And CNN had to sue to have that press pass reinstated.
于是 CNN 不得不提出诉讼， 要求恢复该新闻通行证。

178
00:10:00.358 --> 00:10:05.961
There are about five different paths that I can think of that we can follow
我能想到我们可以走 的五条不同道路，

179
00:10:05.985 --> 00:10:09.724
to try and address some of these very difficult problems today.
以试图解决当今我们面对 的这些异常艰难的问题。

180
00:10:10.199 --> 00:10:12.009
Each one of them has promise,
每一种措施都带来希望，

181
00:10:12.033 --> 00:10:15.032
but each one of them has its own challenges.
但每一种也有其自身的挑战。

182
00:10:15.056 --> 00:10:17.064
The first one is labeling.
第一种措施是贴上标签。

183
00:10:17.088 --> 00:10:18.445
Think about it this way:
可以这么想：

184
00:10:18.469 --> 00:10:22.080
when you go to the grocery store to buy food to consume,
当你去超市购买食品时，

185
00:10:22.104 --> 00:10:24.008
it's extensively labeled.
食品上会有详细的标签。

186
00:10:24.032 --> 00:10:26.024
You know how many calories it has,
你可以得知它有多少卡路里，

187
00:10:26.048 --> 00:10:27.849
how much fat it contains --
含有多少脂肪——

188
00:10:27.873 --> 00:10:32.151
and yet when we consume information, we have no labels whatsoever.
然而当我们摄取信息时， 我们没有任何标签。

189
00:10:32.175 --> 00:10:34.103
What is contained in this information?
这个信息中含有什么？

190
00:10:34.127 --> 00:10:35.580
Is the source credible?
其来源是否可信？

191
00:10:35.604 --> 00:10:37.921
Where is this information gathered from?
这个信息是从哪里收集的？

192
00:10:37.945 --> 00:10:39.770
We have none of that information
在我们摄取信息时，

193
00:10:39.794 --> 00:10:41.897
when we are consuming information.
我们并没有以上的任何信息。

194
00:10:41.921 --> 00:10:45.159
That is a potential avenue, but it comes with its challenges.
这是一种可能的解决办法， 但它有自身的挑战。

195
00:10:45.183 --> 00:10:51.634
For instance, who gets to decide, in society, what's true and what's false?
比如说，在社会中， 有谁能决定信息的真伪？

196
00:10:52.207 --> 00:10:53.849
Is it the governments?
是政府吗？

197
00:10:53.873 --> 00:10:55.023
Is it Facebook?
是 Facebook 吗？

198
00:10:55.421 --> 00:10:59.183
Is it an independent consortium of fact-checkers?
是由事实核查机构 组成的独立联盟吗？

199
00:10:59.207 --> 00:11:01.673
And who's checking the fact-checkers?
谁又来对事实核查机构 进行核查呢？

200
00:11:02.247 --> 00:11:05.331
Another potential avenue is incentives.
另一种可能的解决手段是奖励措施。

201
00:11:05.355 --> 00:11:07.989
We know that during the US presidential election
我们知道，在美国总统大选期间，

202
00:11:08.013 --> 00:11:11.703
there was a wave of misinformation that came from Macedonia
有一波虚假信息来源于马其顿，

203
00:11:11.727 --> 00:11:14.064
that didn't have any political motive
他们没有任何政治动机，

204
00:11:14.088 --> 00:11:16.548
but instead had an economic motive.
相反，他们有经济动机。

205
00:11:16.572 --> 00:11:18.720
And this economic motive existed,
这个经济动机之所以存在，

206
00:11:18.744 --> 00:11:22.268
because false news travels so much farther, faster
是因为虚假新闻比真相传播得

207
00:11:22.292 --> 00:11:24.302
and more deeply than the truth,
更远、更快、更深，

208
00:11:24.326 --> 00:11:29.286
and you can earn advertising dollars as you garner eyeballs and attention
你可以使用这类信息 博取眼球、吸引注意，

209
00:11:29.310 --> 00:11:31.270
with this type of information.
从而通过广告赚钱。

210
00:11:31.294 --> 00:11:35.127
But if we can depress the spread of this information,
但如果我们能抑制 这类信息的传播，

211
00:11:35.151 --> 00:11:38.048
perhaps it would reduce the economic incentive
或许就能在源头减少

212
00:11:37.968 --> 00:11:40.786
to produce it at all in the first place.
生产这类信息的经济动机。

213
00:11:40.786 --> 00:11:43.286
Third, we can think about regulation,
第三，我们可以考虑进行监管，

214
00:11:43.310 --> 00:11:45.635
and certainly, we should think about this option.
毫无疑问，我们应当考虑这个选项。

215
00:11:45.659 --> 00:11:47.270
In the United States, currently,
现在，在美国，

216
00:11:47.294 --> 00:11:52.142
we are exploring what might happen if Facebook and others are regulated.
我们在探索当 Facebook 和其它平台 受到监管时，会发生什么事情。

217
00:11:52.166 --> 00:11:55.967
While we should consider things like regulating political speech,
我们应当考虑的措施包括： 监管政治言论，

218
00:11:55.991 --> 00:11:58.499
labeling the fact that it's political speech,
对政治言论进行标签，

219
00:11:58.523 --> 00:12:02.342
making sure foreign actors can't fund political speech,
确保外国参与者无法资助政治言论，

220
00:12:02.366 --> 00:12:04.913
it also has its own dangers.
但这也有自己的风险。

221
00:12:05.342 --> 00:12:10.220
For instance, Malaysia just instituted a six-year prison sentence
举个例子，马来西亚刚刚颁布法案， 对任何散布不实消息的人

222
00:12:10.244 --> 00:12:12.978
for anyone found spreading misinformation.
处以六年监禁。

223
00:12:13.516 --> 00:12:15.595
And in authoritarian regimes,
而在独裁政权中，

224
00:12:15.619 --> 00:12:20.285
these kinds of policies can be used to suppress minority opinions
这种政策可以被利用 以压制少数群体的意见，

225
00:12:20.309 --> 00:12:23.817
and to continue to extend repression.
继续扩大压迫。

226
00:12:24.500 --> 00:12:28.043
The fourth possible option is transparency.
第四种可能的解决方法是透明度。

227
00:12:28.663 --> 00:12:32.377
We want to know how do Facebook's algorithms work.
我们想了解 Facebook  的算法是怎样运作的。

228
00:12:32.401 --> 00:12:35.281
How does the data combine with the algorithms
数据是怎样与算法结合，

229
00:12:35.305 --> 00:12:38.143
to produce the outcomes that we see?
得出我们看到的结果？

230
00:12:38.167 --> 00:12:40.516
We want them to open the kimono
我们想让他们开诚布公，

231
00:12:40.540 --> 00:12:44.754
and show us exactly the inner workings of how Facebook is working.
为我们披露 Facebook 内部 具体是如何运作的。

232
00:12:44.778 --> 00:12:47.557
And if we want to know social media's effect on society,
而如果我们想知道 社交媒体对社会的影响，

233
00:12:47.581 --> 00:12:49.667
we need scientists, researchers
我们需要科学家、研究人员

234
00:12:49.691 --> 00:12:52.834
and others to have access to this kind of information.
和其他人能够入手这种信息。

235
00:12:52.858 --> 00:12:54.405
But at the same time,
但与此同时，

236
00:12:54.429 --> 00:12:58.230
we are asking Facebook to lock everything down,
我们还要求 Facebook 锁上一切，

237
00:12:58.254 --> 00:13:00.427
to keep all of the data secure.
保证所有数据的安全。

238
00:13:00.451 --> 00:13:03.610
So, Facebook and the other social media platforms
因此，Facebook 和其他社交媒体平台

239
00:13:03.634 --> 00:13:06.768
are facing what I call a transparency paradox.
正面对我称之为的“透明性悖论”。

240
00:13:07.086 --> 00:13:09.760
We are asking them, at the same time,
我们要求他们

241
00:13:09.784 --> 00:13:14.593
to be open and transparent and, simultaneously secure.
在开放、透明的同时 保证安全。

242
00:13:14.617 --> 00:13:17.308
This is a very difficult needle to thread,
这是非常艰难的挑战，

243
00:13:17.332 --> 00:13:19.245
but they will need to thread this needle
这些公司必须直面挑战，

244
00:13:19.269 --> 00:13:23.056
if we are to achieve the promise of social technologies
才能在实现社交科技承诺的同时

245
00:13:22.719 --> 00:13:24.722
while avoiding their peril.
回避它们带来的危害。

246
00:13:24.746 --> 00:13:29.437
The final thing that we could think about is algorithms and machine learning.
我们能想到的最后一个解决手段是 算法和机器学习。

247
00:13:29.461 --> 00:13:34.738
Technology devised to root out and understand fake news, how it spreads,
有的科技被开发出来， 用于拔除和理解虚假新闻，

248
00:13:33.588 --> 00:13:37.436
and to try and dampen its flow.
了解它们的传播方式， 并试图降低其扩散。

249
00:13:37.644 --> 00:13:40.541
Humans have to be in the loop of this technology,
人类需要跟进这种科技，

250
00:13:40.565 --> 00:13:42.843
because we can never escape
因为我们无法逃避的是，

251
00:13:42.867 --> 00:13:46.905
that underlying any technological solution or approach
在任何科技解答或手段的背后

252
00:13:46.929 --> 00:13:50.976
is a fundamental ethical and philosophical question
都有一个根本的伦理与哲学问题：

253
00:13:51.000 --> 00:13:54.270
about how do we define truth and falsity,
我们如何定义真实和虚伪，

254
00:13:54.294 --> 00:13:57.474
to whom do we give the power to define truth and falsity
我们将定义真伪的权力托付于谁，

255
00:13:57.498 --> 00:13:59.958
and which opinions are legitimate,
哪些意见是合法的，

256
00:13:59.982 --> 00:14:03.688
which type of speech should be allowed and so on.
哪种言论能被允许， 诸如此类。

257
00:14:03.712 --> 00:14:06.040
Technology is not a solution for that.
科技并非对这个问题的解答,

258
00:14:06.064 --> 00:14:09.762
Ethics and philosophy is a solution for that.
伦理学和哲学才是。

259
00:14:10.770 --> 00:14:14.088
Nearly every theory of human decision making,
人类决策、人类合作和人类协调

260
00:14:13.833 --> 00:14:16.897
human cooperation and human coordination
的几乎每一个理论，

261
00:14:16.897 --> 00:14:20.571
has some sense of the truth at its core.
其核心都存在某种程度的真相。

262
00:14:21.167 --> 00:14:23.223
But with the rise of fake news,
但随着虚假新闻、

263
00:14:23.247 --> 00:14:24.690
the rise of fake video,
虚假视频、

264
00:14:24.714 --> 00:14:26.596
the rise of fake audio,
虚假音频的崛起，

265
00:14:26.620 --> 00:14:30.544
we are teetering on the brink of the end of reality,
我们正在现实终结 的边缘摇摇欲坠，

266
00:14:30.568 --> 00:14:34.457
where we cannot tell what is real from what is fake.
在这里我们无法分辨 何为真实，何为虚假。

267
00:14:34.481 --> 00:14:37.520
And that's potentially incredibly dangerous.
这有可能是极度危险的。

268
00:14:38.751 --> 00:14:42.699
We have to be vigilant in defending the truth
我们必须保持警惕，拒绝虚假信息，

269
00:14:42.723 --> 00:14:44.257
against misinformation.
捍卫真相——

270
00:14:44.739 --> 00:14:48.175
With our technologies, with our policies
通过我们的技术，我们的政策，

271
00:14:48.199 --> 00:14:50.119
and, perhaps most importantly,
以及，或许也是最重要的，

272
00:14:50.143 --> 00:14:53.357
with our own individual responsibilities,
通过我们自己的责任感、

273
00:14:53.381 --> 00:14:56.936
decisions, behaviors and actions.
决定、行为，和举动。

274
00:14:57.373 --> 00:14:58.810
Thank you very much.
谢谢大家。

275
00:14:58.834 --> 00:15:02.351
(Applause)
（掌声）