WEBVTT

1
00:00:12.820 --> 00:00:15.036
I'm going to talk about a failure of intuition
我想谈论一种我们 很多人都经历过的

2
00:00:15.060 --> 00:00:16.660
that many of us suffer from.
来自于直觉上的失误。

3
00:00:17.300 --> 00:00:20.340
It's really a failure to detect a certain kind of danger.
它让人们无法察觉到 一种特定危险的存在。

4
00:00:21.180 --> 00:00:22.916
I'm going to describe a scenario
我要向大家描述一个情景，

5
00:00:22.940 --> 00:00:26.196
that I think is both terrifying
一个我觉得既令人害怕，

6
00:00:26.220 --> 00:00:27.980
and likely to occur,
却又很可能发生的情景。

7
00:00:28.660 --> 00:00:30.316
and that's not a good combination,
这样一个组合的出现，

8
00:00:30.340 --> 00:00:31.876
as it turns out.
显然不是一个好的征兆。

9
00:00:31.900 --> 00:00:34.356
And yet rather than be scared, most of you will feel
不过，在座的大部分人都会觉得，

10
00:00:34.380 --> 00:00:36.460
that what I'm talking about is kind of cool.
我要谈论的这件事其实挺酷的。

11
00:00:37.020 --> 00:00:39.996
I'm going to describe how the gains we make
我将描述我们从人工智能中

12
00:00:40.020 --> 00:00:41.796
in artificial intelligence
获得的好处，

13
00:00:41.820 --> 00:00:43.596
could ultimately destroy us.
将怎样彻底地毁灭我们。

14
00:00:43.620 --> 00:00:47.076
And in fact, I think it's very difficult to see how they won't destroy us
事实上，想看到人工智能 最终不摧毁我们是很难的，

15
00:00:46.500 --> 00:00:48.780
or inspire us to destroy ourselves.
或者说它必将驱使我们自我毁灭。

16
00:00:49.220 --> 00:00:51.076
And yet if you're anything like me,
如果你和我有共同点，

17
00:00:51.100 --> 00:00:53.756
you'll find that it's fun to think about these things.
你会发现思考这些问题 是相当有趣的。

18
00:00:53.780 --> 00:00:57.156
And that response is part of the problem.
而这种反应就是问题的一部分。

19
00:00:57.180 --> 00:00:58.900
OK? That response should worry you.
因为这种想法应该使你感到担忧。

20
00:00:59.740 --> 00:01:02.396
And if I were to convince you in this talk
假如我想在这个演讲中让你们相信，

21
00:01:02.420 --> 00:01:05.836
that we were likely to suffer a global famine,
我们因为气候变化或者其他灾难，

22
00:01:05.860 --> 00:01:08.916
either because of climate change or some other catastrophe,
很可能会遭受全球性的饥荒，

23
00:01:08.940 --> 00:01:12.356
and that your grandchildren, or their grandchildren,
同时，你们的子孙后辈

24
00:01:12.380 --> 00:01:14.180
are very likely to live like this,
都可能在这样的饥荒中挣扎求生，

25
00:01:15.020 --> 00:01:16.220
you wouldn't think,
你们就不会觉得

26
00:01:17.260 --> 00:01:18.596
"Interesting.
“真有趣，

27
00:01:18.620 --> 00:01:19.820
I like this TED Talk."
我喜欢这个TED演讲。”

28
00:01:21.020 --> 00:01:22.540
Famine isn't fun.
因为饥荒一点都不有趣。

29
00:01:23.620 --> 00:01:26.996
Death by science fiction, on the other hand, is fun,
不过，科幻小说中的死亡 往往却引人入胜。

30
00:01:26.990 --> 00:01:30.496
and one of the things that worries me most about the development of AI at this point
所以我现在最担心的一个问题是，

31
00:01:30.496 --> 00:01:35.160
is that we seem unable to marshal an appropriate emotional response
人们对人工智能的发展将带来的危险，

32
00:01:35.140 --> 00:01:36.956
to the dangers that lie ahead.
似乎还没有形成一个正确的认识。

33
00:01:36.980 --> 00:01:40.180
I am unable to marshal this response, and I'm giving this talk.
我也同样如此，所以我想 在这个演讲中和大家一起探讨。

34
00:01:41.940 --> 00:01:44.636
It's as though we stand before two doors.
我们就像站在了两扇门前。

35
00:01:44.660 --> 00:01:45.916
Behind door number one,
在第一扇门后面，

36
00:01:45.940 --> 00:01:49.236
we stop making progress in building intelligent machines.
我们停下打造智能机器的脚步。

37
00:01:49.260 --> 00:01:53.276
Our computer hardware and software just stops getting better for some reason.
某些原因也使我们停止了 对电脑软件和硬件的升级。

38
00:01:53.300 --> 00:01:56.300
Now take a moment to consider why this might happen.
现在让我们想一下为什么会这样。

39
00:01:56.900 --> 00:02:00.556
I mean, given how valuable intelligence and automation are,
我的意思是，当我们认识到 智能和自动化不可估量的价值时，

40
00:02:00.580 --> 00:02:04.100
we will continue to improve our technology if we are at all able to.
我们总会竭尽所能的改善这些科技。

41
00:02:05.020 --> 00:02:06.687
What could stop us from doing this?
那么，什么会使我们停下脚步呢？

42
00:02:07.620 --> 00:02:09.420
A full-scale nuclear war?
一场大规模的核战争？

43
00:02:10.820 --> 00:02:12.380
A global pandemic?
一次全球性的瘟疫？

44
00:02:14.140 --> 00:02:15.460
An asteroid impact?
一个小行星撞击了地球？

45
00:02:17.460 --> 00:02:20.036
Justin Bieber becoming president of the United States?
或者是贾斯汀·比伯成为了美国总统？

46
00:02:20.060 --> 00:02:22.340
(Laughter)
（笑声）

47
00:02:24.580 --> 00:02:28.500
The point is, something would have to destroy civilization as we know it.
重点是，总有一个事物 会摧毁人类现有的文明。

48
00:02:29.180 --> 00:02:33.476
You have to imagine how bad it would have to be
你需要思考这个灾难究竟有多恐怖，

49
00:02:33.500 --> 00:02:36.836
to prevent us from making improvements in our technology
才会永久性地阻止我们

50
00:02:36.860 --> 00:02:38.076
permanently,
发展科技，

51
00:02:38.100 --> 00:02:40.116
generation after generation.
永久性的。

52
00:02:40.140 --> 00:02:42.276
Almost by definition, this is the worst thing
光想想它， 就觉得这将是人类历史上

53
00:02:41.810 --> 00:02:44.290
that's ever happened in human history.
能发生的最惨绝人寰的事了。

54
00:02:44.340 --> 00:02:45.636
So the only alternative,
那么，我们唯一剩下的选择，

55
00:02:45.660 --> 00:02:47.996
and this is what lies behind door number two,
就藏在第二扇门的后面，

56
00:02:47.960 --> 00:02:51.140
is that we continue to improve our intelligent machines
那就是我们持续 改进我们的智能机器，

57
00:02:51.180 --> 00:02:52.780
year after year after year.
永不停歇。

58
00:02:53.540 --> 00:02:57.180
At a certain point, we will build machines that are smarter than we are,
在将来的某一天，我们会 造出比我们更聪明的机器，

59
00:02:57.900 --> 00:03:00.516
and once we have machines that are smarter than we are,
一旦我们有了 比我们更聪明的机器，

60
00:03:00.540 --> 00:03:02.516
they will begin to improve themselves.
它们将进行自我改进。

61
00:03:02.540 --> 00:03:05.276
And then we risk what the mathematician IJ Good called
然后我们就会承担着 数学家IJ Good 所说的

62
00:03:05.300 --> 00:03:07.076
an "intelligence explosion,"
“智能爆炸”的风险，

63
00:03:07.100 --> 00:03:09.100
that the process could get away from us.
（科技进步的） 进程将不再受我们的控制。

64
00:03:09.940 --> 00:03:12.756
Now, this is often caricatured, as I have here,
现在我们时常会看到 这样一些讽刺漫画，

65
00:03:12.780 --> 00:03:15.996
as a fear that armies of malicious robots
我们总会担心受到一些不怀好意的

66
00:03:15.970 --> 00:03:17.260
will attack us.
机器人军队的攻击。

67
00:03:17.300 --> 00:03:19.996
But that isn't the most likely scenario.
但这不是最可能出现的事情。

68
00:03:19.450 --> 00:03:24.860
It's not that our machines will become spontaneously malevolent.
我们的机器不会自动变得邪恶。

69
00:03:24.900 --> 00:03:27.516
The concern is really that we will build machines
所以，我们唯一的顾虑就是 我们将会打造

70
00:03:27.540 --> 00:03:29.596
that are so much more competent than we are
比我们人类更有竞争力的机器。

71
00:03:29.620 --> 00:03:33.396
that the slightest divergence between their goals and our own
而一旦我们和它们的目标不一致，

72
00:03:33.420 --> 00:03:34.620
could destroy us.
我们将会被摧毁。

73
00:03:35.780 --> 00:03:37.860
Just think about how we relate to ants.
想想我们与蚂蚁的关系吧。

74
00:03:38.420 --> 00:03:40.076
We don't hate them.
我们不讨厌它们，

75
00:03:40.100 --> 00:03:42.156
We don't go out of our way to harm them.
我们不会去主动去伤害它们。

76
00:03:42.180 --> 00:03:44.556
In fact, sometimes we take pains not to harm them.
实际上，我们经常会尽量 避免伤害蚂蚁。

77
00:03:44.580 --> 00:03:46.596
We step over them on the sidewalk.
我们会选择从它们身边走过。

78
00:03:46.620 --> 00:03:48.756
But whenever their presence
但只要它们的存在

79
00:03:48.780 --> 00:03:51.276
seriously conflicts with one of our goals,
妨碍到了我们达成目标，

80
00:03:51.300 --> 00:03:53.777
let's say when constructing a building like this one,
比如说当我们在建造这样一个建筑，

81
00:03:53.801 --> 00:03:55.761
we annihilate them without a qualm.
我们会毫不手软地杀掉它们。

82
00:03:56.300 --> 00:03:59.236
The concern is that we will one day build machines
所以我们的顾虑是，终将有一天 我们打造的机器，

83
00:03:59.260 --> 00:04:01.996
that, whether they're conscious or not,
不管它们是否有意识， 它们终将会以

84
00:04:02.020 --> 00:04:04.020
could treat us with similar disregard.
我们对待蚂蚁的方式 来对待我们。

85
00:04:05.580 --> 00:04:08.340
Now, I suspect this seems far-fetched to many of you.
我想很多人会说这很遥远。

86
00:04:09.180 --> 00:04:15.516
I bet there are those of you who doubt that superintelligent AI is possible,
我打赌你们中有些人还会 怀疑超级人工智能是否可能实现，

87
00:04:15.540 --> 00:04:17.196
much less inevitable.
认为我是在小题大做。

88
00:04:17.220 --> 00:04:20.840
But then you must find something wrong with one of the following assumptions.
但是你很快会发现以下这些 假设中的某一个是有问题的。

89
00:04:20.864 --> 00:04:22.436
And there are only three of them.
下面是仅有的三种假设：

90
00:04:23.620 --> 00:04:28.339
Intelligence is a matter of information processing in physical systems.
第一，智慧可以被看做 物理系统中的信息处理过程。

91
00:04:29.140 --> 00:04:31.755
Actually, this is a little bit more than an assumption.
事实上，这不仅仅是一个假设。

92
00:04:31.779 --> 00:04:35.236
We have already built narrow intelligence into our machines,
我们已经在有些机器中 嵌入了智能系统，

93
00:04:35.260 --> 00:04:37.276
and many of these machines perform
这些机器中很多已经

94
00:04:37.300 --> 00:04:39.940
at a level of superhuman intelligence already.
有着超越普通人的智慧了。

95
00:04:40.660 --> 00:04:43.236
And we know that mere matter
而且，我们也知道任何一点小事

96
00:04:43.260 --> 00:04:45.876
can give rise to what is called "general intelligence,"
都可以引发所谓的“普遍智慧”，

97
00:04:45.900 --> 00:04:49.556
an ability to think flexibly across multiple domains,
这是一种可以在不同领域间 灵活思考的能力，

98
00:04:49.580 --> 00:04:52.716
because our brains have managed it. Right?
因为我们的大脑已经 成功做到了这些。对吧？

99
00:04:52.740 --> 00:04:56.676
I mean, there's just atoms in here,
我的意思是， 大脑里其实都是原子，

100
00:04:56.700 --> 00:05:01.196
and as long as we continue to build systems of atoms
只要我们继续建造这些原子体系，

101
00:05:01.220 --> 00:05:03.916
that display more and more intelligent behavior,
我们就能实现越来越多的智慧行为，

102
00:05:03.940 --> 00:05:06.476
we will eventually, unless we are interrupted,
我们最终将会， 当然除非我们被干扰，

103
00:05:06.500 --> 00:05:09.876
we will eventually build general intelligence
我们最终将会给我们的机器赋予

104
00:05:09.900 --> 00:05:11.196
into our machines.
广泛意义上的智能。

105
00:05:11.220 --> 00:05:14.876
It's crucial to realize that the rate of progress doesn't matter,
我们要知道这个进程的速度并不重要，

106
00:05:14.900 --> 00:05:18.076
because any progress is enough to get us into the end zone.
因为任何进程都足够 让我们走进死胡同。

107
00:05:18.100 --> 00:05:21.876
We don't need Moore's law to continue. We don't need exponential progress.
甚至不需要考虑摩尔定律， 也不需要用指数函数来衡量，

108
00:05:21.900 --> 00:05:23.500
We just need to keep going.
这一切顺其自然都会发生。

109
00:05:25.300 --> 00:05:28.220
The second assumption is that we will keep going.
第二个假设是，我们会一直创新。

110
00:05:28.820 --> 00:05:31.580
We will continue to improve our intelligent machines.
去继续改进我们的智能机器。

111
00:05:32.820 --> 00:05:37.196
And given the value of intelligence --
由于智慧的价值就是——

112
00:05:37.220 --> 00:05:40.756
I mean, intelligence is either the source of everything we value
提供我们所珍爱的事物，

113
00:05:40.780 --> 00:05:43.556
or we need it to safeguard everything we value.
或是用于保护我们所珍视的一切。

114
00:05:43.580 --> 00:05:45.836
It is our most valuable resource.
智慧就是我们最有价值的资源。

115
00:05:45.860 --> 00:05:47.396
So we want to do this.
所以我们想继续革新它。

116
00:05:47.420 --> 00:05:50.756
We have problems that we desperately need to solve.
因为我们有很多需要 迫切解决的问题。

117
00:05:50.780 --> 00:05:53.980
We want to cure diseases like Alzheimer's and cancer.
我们想要治愈像阿兹海默症 和癌症这样的疾病，

118
00:05:54.780 --> 00:05:58.716
We want to understand economic systems. We want to improve our climate science.
我们想要了解经济系统， 想要改善我们的气候科学，

119
00:05:58.740 --> 00:06:00.996
So we will do this, if we can.
所以只要可能， 我们就会将革新继续下去。

120
00:06:00.940 --> 00:06:04.306
The train is already out of the station, and there's no brake to pull.
而且革新的列车早已驶出，  车上却没有刹车。

121
00:06:05.700 --> 00:06:11.156
Finally, we don't stand on a peak of intelligence,
第三种假设是： 人类没有登上智慧的巅峰，

122
00:06:11.180 --> 00:06:12.980
or anywhere near it, likely.
甚至连接近可能都谈不上。

123
00:06:13.460 --> 00:06:15.356
And this really is the crucial insight.
这个想法十分关键。

124
00:06:15.380 --> 00:06:17.796
This is what makes our situation so precarious,
这就是为什么 我们所处的环境是很危险的，

125
00:06:17.820 --> 00:06:21.860
and this is what makes our intuitions about risk so unreliable.
这也是为什么我们对风险的 直觉是不可靠的。

126
00:06:22.940 --> 00:06:25.660
Now, just consider the smartest person who has ever lived.
现在，请大家想一下 谁是世界上最聪明的人。

127
00:06:26.460 --> 00:06:29.876
On almost everyone's shortlist here is John von Neumann.
几乎每个人的候选名单里都会 有约翰·冯·诺伊曼。

128
00:06:29.900 --> 00:06:33.236
I mean, the impression that von Neumann made on the people around him,
冯·诺伊曼留给周围人的印象

129
00:06:33.260 --> 00:06:37.316
and this included the greatest mathematicians and physicists of his time,
就是他是那个时代当中最杰出的 数学家和物理学家，

130
00:06:37.340 --> 00:06:39.276
is fairly well-documented.
这些都是完好的记录在案的。

131
00:06:39.300 --> 00:06:43.076
If only half the stories about him are half true,
即使他的故事里有一半是假的，

132
00:06:43.100 --> 00:06:44.316
there's no question
都没有人会质疑

133
00:06:44.340 --> 00:06:46.796
he's one of the smartest people who has ever lived.
他仍然是世界上最聪明的人之一。

134
00:06:46.820 --> 00:06:49.340
So consider the spectrum of intelligence.
那么，让我们来看看智慧谱线吧。

135
00:06:50.140 --> 00:06:51.569
Here we have John von Neumann.
现在我们有了约翰·冯·诺伊曼，

136
00:06:53.380 --> 00:06:54.714
And then we have you and me.
还有我们大家。

137
00:06:55.940 --> 00:06:57.236
And then we have a chicken.
另外还有一只鸡。

138
00:06:57.260 --> 00:06:59.196
(Laughter)
（笑声）

139
00:06:59.220 --> 00:07:00.436
Sorry, a chicken.
抱歉，母鸡的位置应该在这。

140
00:07:00.460 --> 00:07:01.716
(Laughter)
（笑声）

141
00:07:01.740 --> 00:07:05.476
There's no reason for me to make this talk more depressing than it needs to be.
这个演讲已经够严肃了， 开个玩笑轻松一下。

142
00:07:05.500 --> 00:07:07.100
(Laughter)
（笑声）

143
00:07:08.159 --> 00:07:11.636
It seems overwhelmingly likely, however, that the spectrum of intelligence
然而，很可能的情况是， 智慧谱线上的内容

144
00:07:11.660 --> 00:07:14.780
extends much further than we currently conceive,
已远远超出了我们的认知，

145
00:07:15.700 --> 00:07:18.916
and if we build machines that are more intelligent than we are,
如果我们建造了比 自身更聪明的机器，

146
00:07:18.940 --> 00:07:21.236
they will very likely explore this spectrum
它们将非常可能 以超乎寻常的方式

147
00:07:21.260 --> 00:07:23.116
in ways that we can't imagine,
延展这个谱线，

148
00:07:23.140 --> 00:07:25.660
and exceed us in ways that we can't imagine.
最终超越人类。

149
00:07:26.820 --> 00:07:31.156
And it's important to recognize that this is true by virtue of speed alone.
仅仅从速度方面考虑， 我们就能够意识到这一点。

150
00:07:31.180 --> 00:07:36.236
Right? So imagine if we just built a superintelligent AI
那么，现在让我们来想象一下 我们刚建好一个超级人工智能机器，

151
00:07:36.260 --> 00:07:39.716
that was no smarter than your average team of researchers
大概和斯坦福 或是麻省理工学院的研究员的

152
00:07:39.740 --> 00:07:42.036
at Stanford or MIT.
平均水平差不多吧。

153
00:07:42.060 --> 00:07:45.036
Well, electronic circuits function about a million times faster
但是，电路板要比生物系统

154
00:07:44.810 --> 00:07:46.260
than biochemical ones,
运行速度快一百万倍，

155
00:07:46.340 --> 00:07:49.476
so this machine should think about a million times faster
所以这个机器思考起来 会比那些打造它的大脑

156
00:07:49.500 --> 00:07:51.316
than the minds that built it.
快一百万倍。

157
00:07:51.340 --> 00:07:52.996
So you set it running for a week,
当你让它运行一周后，

158
00:07:52.970 --> 00:07:57.580
and it will perform 20,000 years of human-level intellectual work,
它将能呈现出相当于人类智慧在 20000年间发展出的水平，

159
00:07:58.220 --> 00:08:00.180
week after week after week.
而这个过程将周而复始。

160
00:08:01.460 --> 00:08:04.556
How could we even understand, much less constrain,
那么，我们又怎么能理解， 更不用说去制约

161
00:08:04.580 --> 00:08:06.860
a mind making this sort of progress?
一个以如此速度运行的机器呢？

162
00:08:08.660 --> 00:08:10.796
The other thing that's worrying, frankly,
坦白讲，另一件令人担心的事就是，

163
00:08:10.820 --> 00:08:15.796
is that, imagine the best case scenario.
我们考虑一下最理想的情景。

164
00:08:15.820 --> 00:08:19.996
So imagine we hit upon a design of superintelligent AI
想象我们正好做出了 一个没有任何安全隐患的

165
00:08:19.970 --> 00:08:21.390
that has no safety concerns.
超级人工智能。

166
00:08:21.420 --> 00:08:24.676
We have the perfect design the first time around.
我们有了一个前所未有的完美设计。

167
00:08:24.700 --> 00:08:26.916
It's as though we've been handed an oracle
就好像我们被赐予了一件神物，

168
00:08:26.940 --> 00:08:28.956
that behaves exactly as intended.
它能够准确的执行目标动作。

169
00:08:28.980 --> 00:08:32.700
Well, this machine would be the perfect labor-saving device.
这个机器将完美的节省人力工作。

170
00:08:33.500 --> 00:08:35.929
It can design the machine that can build the machine
它设计出的机器 能够再生产其他机器，

171
00:08:35.953 --> 00:08:37.716
that can do any physical work,
去完成所有的人力工作。

172
00:08:37.740 --> 00:08:39.196
powered by sunlight,
由太阳能供电，

173
00:08:39.220 --> 00:08:41.916
more or less for the cost of raw materials.
而成本的多少仅取决于原材料。

174
00:08:41.940 --> 00:08:45.196
So we're talking about the end of human drudgery.
那么，我们正在谈论的 就是人力劳动的终结。

175
00:08:45.220 --> 00:08:48.020
We're also talking about the end of most intellectual work.
也关乎脑力劳动的终结。

176
00:08:49.020 --> 00:08:52.076
So what would apes like ourselves do in this circumstance?
那在这种情况下， 像我们这样的"大猩猩"还能有什么用呢？

177
00:08:52.100 --> 00:08:56.180
Well, we'd be free to play Frisbee and give each other massages.
我们可以悠闲地玩飞盘， 给彼此做按摩。

178
00:08:57.660 --> 00:09:00.516
Add some LSD and some questionable wardrobe choices,
服用一些迷药， 穿一些奇装异服，

179
00:09:00.540 --> 00:09:02.716
and the whole world could be like Burning Man.
整个世界都沉浸在狂欢节之中。

180
00:09:02.740 --> 00:09:04.380
(Laughter)
（笑声）

181
00:09:06.140 --> 00:09:08.140
Now, that might sound pretty good,
那可能听起来挺棒的，

182
00:09:09.100 --> 00:09:11.476
but ask yourself what would happen
不过让我们扪心自问，

183
00:09:11.500 --> 00:09:14.236
under our current economic and political order?
在现有的经济和政治体制下， 这意味着什么？

184
00:09:14.260 --> 00:09:16.676
It seems likely that we would witness
我们很可能会目睹

185
00:09:16.700 --> 00:09:20.836
a level of wealth inequality and unemployment
前所未有的贫富差距

186
00:09:20.860 --> 00:09:22.356
that we have never seen before.
和失业率。

187
00:09:22.380 --> 00:09:24.996
Absent a willingness to immediately put this new wealth
有钱人不愿意马上把这笔新的财富

188
00:09:24.950 --> 00:09:26.500
to the service of all humanity,
贡献出来服务社会，

189
00:09:27.460 --> 00:09:31.076
a few trillionaires could grace the covers of our business magazines
这时一些千万富翁能够优雅地 登上商业杂志的封面，

190
00:09:31.100 --> 00:09:33.540
while the rest of the world would be free to starve.
而剩下的人可能都在挨饿。

191
00:09:34.140 --> 00:09:36.436
And what would the Russians or the Chinese do
如果听说硅谷里的公司

192
00:09:36.460 --> 00:09:39.076
if they heard that some company in Silicon Valley
即将造出超级人工智能，

193
00:09:39.100 --> 00:09:41.836
was about to deploy a superintelligent AI?
俄国人和中国人 会采取怎样的行动呢？

194
00:09:41.860 --> 00:09:44.716
This machine would be capable of waging war,
那个机器将能够 以一种前所未有的能力

195
00:09:44.740 --> 00:09:46.956
whether terrestrial or cyber,
去开展由领土问题和

196
00:09:46.980 --> 00:09:48.660
with unprecedented power.
网络问题引发的战争。

197
00:09:49.940 --> 00:09:51.796
This is a winner-take-all scenario.
这是一个胜者为王的世界。

198
00:09:51.820 --> 00:09:54.956
To be six months ahead of the competition here
机器世界中的半年，

199
00:09:54.980 --> 00:09:57.756
is to be 500,000 years ahead,
在现实世界至少会相当于

200
00:09:57.780 --> 00:09:59.276
at a minimum.
50万年。

201
00:09:59.300 --> 00:10:04.036
So it seems that even mere rumors of this kind of breakthrough
所以仅仅是关于这种科技突破的传闻，

202
00:10:04.060 --> 00:10:06.436
could cause our species to go berserk.
就可以让我们的种族丧失理智。

203
00:10:06.460 --> 00:10:09.356
Now, one of the most frightening things,
在我的观念里，

204
00:10:09.380 --> 00:10:12.156
in my view, at this moment,
当前最可怕的东西

205
00:10:12.180 --> 00:10:16.476
are the kinds of things that AI researchers say
正是人工智能的研究人员

206
00:10:16.500 --> 00:10:18.060
when they want to be reassuring.
安慰我们的那些话。

207
00:10:18.820 --> 00:10:22.276
And the most common reason we're told not to worry is time.
最常见的理由就是关于时间。

208
00:10:21.920 --> 00:10:24.300
This is all a long way off, don't you know.
他们会说，现在开始担心还为时尚早。

209
00:10:24.380 --> 00:10:26.820
This is probably 50 or 100 years away.
这很可能是50年或者 100年之后才需要担心的事。

210
00:10:27.540 --> 00:10:28.796
One researcher has said,
一个研究人员曾说过，

211
00:10:28.820 --> 00:10:30.396
"Worrying about AI safety
“担心人工智能的安全性

212
00:10:30.420 --> 00:10:32.700
is like worrying about overpopulation on Mars."
就好比担心火星上人口过多一样。”

213
00:10:33.936 --> 00:10:35.556
This is the Silicon Valley version
这就是硅谷版本的

214
00:10:35.580 --> 00:10:37.956
of "don't worry your pretty little head about it."
“不要杞人忧天。”

215
00:10:37.980 --> 00:10:39.316
(Laughter)
（笑声）

216
00:10:39.340 --> 00:10:41.236
No one seems to notice
似乎没有人注意到

217
00:10:41.260 --> 00:10:43.876
that referencing the time horizon
以时间作为参考系

218
00:10:43.900 --> 00:10:46.476
is a total non sequitur.
是得不出合理的结论的。

219
00:10:46.500 --> 00:10:49.756
If intelligence is just a matter of information processing,
如果说智慧只包括信息处理，

220
00:10:49.780 --> 00:10:52.436
and we continue to improve our machines,
然后我们继续改善这些机器，

221
00:10:52.460 --> 00:10:55.340
we will produce some form of superintelligence.
那么我们终将生产出超级智能。

222
00:10:56.140 --> 00:10:59.796
And we have no idea how long it will take us
但是，我们无法预估将花费多长时间

223
00:10:59.820 --> 00:11:02.220
to create the conditions to do that safely.
来创造实现这一切的安全环境。

224
00:11:04.020 --> 00:11:05.316
Let me say that again.
我再重复一遍。

225
00:11:05.340 --> 00:11:09.156
We have no idea how long it will take us
我们无法预估将花费多长时间

226
00:11:09.180 --> 00:11:11.420
to create the conditions to do that safely.
来创造实现这一切的安全环境。

227
00:11:12.740 --> 00:11:16.196
And if you haven't noticed, 50 years is not what it used to be.
你们可能没有注意过， 50年的概念已今非昔比。

228
00:11:16.220 --> 00:11:18.676
This is 50 years in months.
这是用月来衡量50年的样子。 （每个点表示一个月）

229
00:11:18.700 --> 00:11:20.540
This is how long we've had the iPhone.
红色的点是代表苹果手机出现的时间。

230
00:11:21.260 --> 00:11:23.860
This is how long "The Simpsons" has been on television.
这是《辛普森一家》（动画片） 在电视上播出以来的时间。

231
00:11:24.500 --> 00:11:26.876
Fifty years is not that much time
要做好准备面对 人类历史上前所未有的挑战，

232
00:11:26.900 --> 00:11:30.060
to meet one of the greatest challenges our species will ever face.
50年时间并不是很长。

233
00:11:31.460 --> 00:11:35.476
Once again, we seem to be failing to have an appropriate emotional response
就像我刚才说的， 我们对确定会来临的事情

234
00:11:35.500 --> 00:11:38.196
to what we have every reason to believe is coming.
做出了不合理的回应。

235
00:11:38.220 --> 00:11:42.196
The computer scientist Stuart Russell has a nice analogy here.
计算机科学家斯图尔特·罗素 给出了一个极好的类比。

236
00:11:42.220 --> 00:11:47.116
He said, imagine that we received a message from an alien civilization,
他说，想象我们从 外太空接收到一条讯息，

237
00:11:47.140 --> 00:11:48.836
which read:
上面写着：

238
00:11:48.860 --> 00:11:50.396
"People of Earth,
“地球上的人类，

239
00:11:50.420 --> 00:11:52.780
we will arrive on your planet in 50 years.
我们将在五十年后到达你们的星球，

240
00:11:53.620 --> 00:11:55.196
Get ready."
做好准备吧。”

241
00:11:55.220 --> 00:11:59.476
And now we're just counting down the months until the mothership lands?
于是我们就开始倒计时， 直到它们的“母舰”着陆吗？

242
00:11:59.500 --> 00:12:02.500
We would feel a little more urgency than we do.
在这种情况下我们会感到更紧迫。

243
00:12:04.500 --> 00:12:06.356
Another reason we're told not to worry
另外一个试图安慰我们的理由是，

244
00:12:06.380 --> 00:12:09.396
is that these machines can't help but share our values
那些机器必须 拥有和我们一样的价值观，

245
00:12:09.420 --> 00:12:12.036
because they will be literally extensions of ourselves.
因为它们将会是我们自身的延伸。

246
00:12:12.060 --> 00:12:13.876
They'll be grafted onto our brains,
它们将会被嫁接到我们的大脑上，

247
00:12:13.900 --> 00:12:16.260
and we'll essentially become their limbic systems.
我们将会成它们的边缘系统。

248
00:12:16.940 --> 00:12:18.356
Now take a moment to consider
现在我们再思考一下

249
00:12:18.380 --> 00:12:21.556
that the safest and only prudent path forward,
最安全的，也是唯一经慎重考虑后

250
00:12:21.580 --> 00:12:22.916
recommended,
推荐的发展方向，

251
00:12:22.940 --> 00:12:25.740
is to implant this technology directly into our brains.
是将这项技术直接植入我们大脑。

252
00:12:26.420 --> 00:12:29.796
Now, this may in fact be the safest and only prudent path forward,
这也许确实是最安全的， 也是唯一慎重的发展方向，

253
00:12:29.820 --> 00:12:32.876
but usually one's safety concerns about a technology
但通常在我们把它塞进脑袋之前，

254
00:12:32.900 --> 00:12:36.556
have to be pretty much worked out before you stick it inside your head.
会充分考虑这项技术的安全性。

255
00:12:36.580 --> 00:12:38.596
(Laughter)
（笑声）

256
00:12:38.620 --> 00:12:43.956
The deeper problem is that building superintelligent AI on its own
更深一层的问题是： 仅仅制造出超级人工智能机器

257
00:12:43.980 --> 00:12:45.716
seems likely to be easier
可能要比

258
00:12:45.740 --> 00:12:47.596
than building superintelligent AI
既制造超级人工智能，

259
00:12:47.620 --> 00:12:49.396
and having the completed neuroscience
又让其拥有能让 我们的思想和超级人工智能

260
00:12:49.420 --> 00:12:52.100
that allows us to seamlessly integrate our minds with it.
无缝对接的完整的 神经科学系统要简单很多。

261
00:12:52.620 --> 00:12:55.796
And given that the companies and governments doing this work
而做这些研究的公司或政府，

262
00:12:55.820 --> 00:12:59.476
are likely to perceive themselves as being in a race against all others,
很可能将彼此视作竞争对手，

263
00:12:59.500 --> 00:13:02.756
given that to win this race is to win the world,
因为赢得了比赛就意味着称霸了世界，

264
00:13:02.780 --> 00:13:05.236
provided you don't destroy it in the next moment,
前提是不在刚成功后就将其销毁，

265
00:13:05.260 --> 00:13:07.876
then it seems likely that whatever is easier to do
所以结论是：简单的选项

266
00:13:07.900 --> 00:13:09.100
will get done first.
一定会被先实现。

267
00:13:10.380 --> 00:13:13.236
Now, unfortunately, I don't have a solution to this problem,
但很遗憾， 除了建议更多人去思考这个问题，

268
00:13:13.260 --> 00:13:15.876
apart from recommending that more of us think about it.
我对此并无解决方案。

269
00:13:15.900 --> 00:13:18.276
I think we need something like a Manhattan Project
我觉得在人工智能问题上，

270
00:13:18.300 --> 00:13:20.316
on the topic of artificial intelligence.
我们需要一个“曼哈顿计划” （二战核武器研究计划），

271
00:13:20.340 --> 00:13:23.076
Not to build it, because I think we'll inevitably do that,
不是用于讨论如何制造人工智能， 因为我们一定会这么做，

272
00:13:22.980 --> 00:13:26.390
but to understand how to avoid an arms race
而是去避免军备竞赛，

273
00:13:26.460 --> 00:13:29.956
and to build it in a way that is aligned with our interests.
最终以一种有利于 我们的方式去打造它。

274
00:13:29.980 --> 00:13:32.116
When you're talking about superintelligent AI
当你在谈论一个可以自我改造的

275
00:13:32.140 --> 00:13:34.396
that can make changes to itself,
超级人工智能时，

276
00:13:34.420 --> 00:13:39.036
it seems that we only have one chance to get the initial conditions right,
我们似乎只有 一次正确搭建初始系统的机会，

277
00:13:39.060 --> 00:13:41.116
and even then we will need to absorb
而这个正确的初始系统

278
00:13:41.140 --> 00:13:44.180
the economic and political consequences of getting them right.
需要我们在经济以及政治上 做出很大的努力。

279
00:13:45.580 --> 00:13:47.636
But the moment we admit
但是当我们承认

280
00:13:47.660 --> 00:13:51.660
that information processing is the source of intelligence,
信息处理是智慧的源头，

281
00:13:52.540 --> 00:13:57.340
that some appropriate computational system is what the basis of intelligence is,
承认一些电脑系统是智能的基础，

282
00:13:58.180 --> 00:14:01.940
and we admit that we will improve these systems continuously,
承认我们会不断改善这些系统，

283
00:14:03.100 --> 00:14:07.556
and we admit that the horizon of cognition very likely far exceeds
承认我们现存的认知远没有达到极限，

284
00:14:07.580 --> 00:14:08.780
what we currently know,
将很可能被超越，

285
00:14:09.940 --> 00:14:11.156
then we have to admit
我们又必须同时承认

286
00:14:11.180 --> 00:14:13.820
that we are in the process of building some sort of god.
我们在某种意义上 正在创造一个新的“上帝”。

287
00:14:15.220 --> 00:14:16.796
Now would be a good time
现在正是思考人类是否

288
00:14:16.820 --> 00:14:18.773
to make sure it's a god we can live with.
能与这个“上帝”和睦相处的最佳时机。

289
00:14:19.940 --> 00:14:21.476
Thank you very much.
非常感谢！

290
00:14:21.500 --> 00:14:26.593
(Applause)
（掌声）