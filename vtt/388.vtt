WEBVTT

1
00:00:12.580 --> 00:00:16.420
When I was a kid, I was the quintessential nerd.
在我还是孩子的时候，我是一个典型的书呆子。

2
00:00:17.140 --> 00:00:19.316
I think some of you were, too.
我猜你们的一部分人和我一样。

3
00:00:19.340 --> 00:00:20.556
(Laughter)
（笑声）

4
00:00:20.580 --> 00:00:23.796
And you, sir, who laughed the loudest, you probably still are.
还有你，那位先生，笑得最大声，说不定你现在还是呢。

5
00:00:23.820 --> 00:00:26.076
(Laughter)
（笑声）

6
00:00:26.100 --> 00:00:29.596
I grew up in a small town in the dusty plains of north Texas,
我成长在德克萨斯州北部荒凉平原上的一个小镇。

7
00:00:29.620 --> 00:00:32.956
the son of a sheriff who was the son of a pastor.
我的爸爸是警长，爷爷是一位牧师，

8
00:00:32.980 --> 00:00:34.900
Getting into trouble was not an option.
所以自找麻烦从来不是一个好的选项。

9
00:00:35.860 --> 00:00:39.116
And so I started reading calculus books for fun.
所以我开始看有关微积分的书来打发时间。

10
00:00:39.140 --> 00:00:40.676
(Laughter)
（笑声）

11
00:00:40.700 --> 00:00:42.396
You did, too.
你也是这样。

12
00:00:42.420 --> 00:00:46.156
That led me to building a laser and a computer and model rockets,
于是我学着制作了一个激光器，一台电脑，和一个火箭模型，

13
00:00:46.180 --> 00:00:49.180
and that led me to making rocket fuel in my bedroom.
并且在自己的卧室制取火箭燃料。

14
00:00:49.780 --> 00:00:53.436
Now, in scientific terms,
现在，从科学的角度而言，

15
00:00:53.460 --> 00:00:56.716
we call this a very bad idea.
我们把这个叫做一个糟糕透顶的主意。

16
00:00:56.740 --> 00:00:57.956
(Laughter)
（笑声）

17
00:00:57.980 --> 00:01:00.156
Around that same time,
在差不多同一时间，

18
00:01:00.180 --> 00:01:03.396
Stanley Kubrick's "2001: A Space Odyssey" came to the theaters,
Stanley Kubrick的 “2001：太空漫游”上映了，

19
00:01:03.420 --> 00:01:05.620
and my life was forever changed.
我的生活从此改变。

20
00:01:06.100 --> 00:01:08.156
I loved everything about that movie,
我喜欢关于那部电影的一切，

21
00:01:08.180 --> 00:01:10.716
especially the HAL 9000.
特别是 HAL 9000。

22
00:01:10.740 --> 00:01:12.796
Now, HAL was a sentient computer
HAL是一台有情感的电脑，

23
00:01:12.820 --> 00:01:17.836
designed to guide the Discovery spacecraft from the Earth to Jupiter.
为引导“发现一号”飞船从地球前往木星而设计出来。

24
00:01:17.860 --> 00:01:19.916
HAL was also a flawed character,
HAL 也是一个有缺陷的角色，

25
00:01:19.940 --> 00:01:24.220
for in the end he chose to value the mission over human life.
因为在结尾他将任务的价值置于生命之上。

26
00:01:24.660 --> 00:01:26.756
Now, HAL was a fictional character,
HAL 是一个虚构的角色，

27
00:01:26.780 --> 00:01:29.436
but nonetheless he speaks to our fears,
但尽管如此，他挑战了我们的恐惧，

28
00:01:29.460 --> 00:01:36.580
our fears of being subjugated by some unfeeling, artificial intelligence who is indifferent to our humanity.
被一些冷漠无情的人工智能（AI）所统治的恐惧。

29
00:01:37.700 --> 00:01:40.276
I believe that such fears are unfounded.
但我相信这些恐惧只是杞人忧天。

30
00:01:40.300 --> 00:01:44.556
Indeed, we stand at a remarkable time in human history,
的确，我们正处于人类历史上

31
00:01:42.900 --> 00:01:49.540
where, driven by refusal to accept the limits of our bodies and our minds,
一个值得铭记的时间点，不甘被自身肉体和头脑所局限，

32
00:01:49.580 --> 00:01:58.700
we are building machines of exquisite, beautiful complexity and grace that will extend the human experience in ways beyond our imagining.
我们正在制造 那些可以通过我们无法想象的方式 来拓展人类体验的机器，它们精美，复杂，而且优雅。

33
00:01:59.540 --> 00:02:04.076
After a career that led me from the Air Force Academy to Space Command to now,
我在空军学院和航天司令部工作过，

34
00:02:04.100 --> 00:02:05.796
I became a systems engineer,
现在是一个系统工程师。

35
00:02:05.820 --> 00:02:11.156
and recently I was drawn into an engineering problem associated with NASA's mission to Mars.
最近我碰到了一个和NASA火星任务相关的工程问题。

36
00:02:11.180 --> 00:02:13.676
Now, in space flights to the Moon,
当前，前往月球的宇宙飞行，

37
00:02:13.700 --> 00:02:16.836
we can rely upon mission control in Houston
我们可以依靠休斯顿的指挥中心

38
00:02:16.860 --> 00:02:18.836
to watch over all aspects of a flight.
来密切关注飞行的各个方面。

39
00:02:18.860 --> 00:02:22.396
However, Mars is 200 times further away,
但是，火星相比而言多出了200倍的距离，

40
00:02:22.420 --> 00:02:28.796
and as a result it takes on average 13 minutes for a signal to travel from the Earth to Mars.
这使得一个信号从地球到火星平均要花费13分钟。

41
00:02:28.820 --> 00:02:32.220
If there's trouble, there's not enough time.
如果有了麻烦，我们并没有足够的时间来解决。

42
00:02:32.660 --> 00:02:35.156
And so a reasonable engineering solution
所以一个可靠的工程解决方案

43
00:02:35.180 --> 00:02:37.756
calls for us to put mission control
促使我们把一个指挥中枢

44
00:02:37.780 --> 00:02:40.796
inside the walls of the Orion spacecraft.
放在“猎户星”飞船之中。

45
00:02:40.820 --> 00:02:43.716
Another fascinating idea in the mission profile
在任务档案中的另一个创意

46
00:02:43.740 --> 00:02:48.516
places humanoid robots on the surface of Mars before the humans themselves arrive,
是在人类抵达火星之前，把人形机器人先一步放在火星表面，

47
00:02:48.540 --> 00:02:50.196
first to build facilities
它们可以先建造据点，

48
00:02:50.220 --> 00:02:53.580
and later to serve as collaborative members of the science team.
然后作为科学团队的合作伙伴驻扎。

49
00:02:55.220 --> 00:02:57.956
Now, as I looked at this from an engineering perspective,
当我从工程师的角度看待这个想法，

50
00:02:57.980 --> 00:03:05.756
it became very clear to me that what I needed to architect was a smart, collaborative, socially intelligent artificial intelligence.
对于建造一个聪明，懂得合作，擅长社交的人工智能的需求是十分明显的。

51
00:03:05.780 --> 00:03:10.076
In other words, I needed to build something very much like a HAL
换句话说，我需要建造一个和HAL一样，

52
00:03:10.100 --> 00:03:12.516
but without the homicidal tendencies.
但是没有谋杀倾向的机器。

53
00:03:12.540 --> 00:03:13.900
(Laughter)
（笑声）

54
00:03:14.740 --> 00:03:16.556
Let's pause for a moment.
待会儿再回到这个话题。

55
00:03:16.580 --> 00:03:20.476
Is it really possible to build an artificial intelligence like that?
真的有可能打造一个类似的人工智能吗？

56
00:03:20.500 --> 00:03:21.956
Actually, it is.
是的，当然可以。

57
00:03:21.980 --> 00:03:23.236
In many ways,
在许多方面，

58
00:03:23.260 --> 00:03:26.716
this is a hard engineering problem with elements of AI,
一个困难的工程问题来自于AI的各个零件，

59
00:03:26.740 --> 00:03:31.436
not some wet hair ball of an AI problem that needs to be engineered.
而不是什么琐碎的AI问题。

60
00:03:31.460 --> 00:03:34.116
To paraphrase Alan Turing,
借用Alan Turing的话来说，

61
00:03:34.140 --> 00:03:36.516
I'm not interested in building a sentient machine.
我没有兴趣建造一台有情感的机器。

62
00:03:36.540 --> 00:03:38.116
I'm not building a HAL.
我不是要建造HAL。

63
00:03:38.140 --> 00:03:40.556
All I'm after is a simple brain,
我只想要一个简单的大脑，

64
00:03:40.580 --> 00:03:43.700
something that offers the illusion of intelligence.
一个能让你以为它有智能的东西。

65
00:03:44.820 --> 00:03:49.476
The art and the science of computing have come a long way since HAL was onscreen,
自从HAL登上荧幕，关于编程的技术和艺术已经发展了许多，

66
00:03:49.500 --> 00:03:52.716
and I'd imagine if his inventor Dr. Chandra were here today,
我能想象如果它的发明者Chandra博士今天在这里的话，

67
00:03:52.740 --> 00:03:55.076
he'd have a whole lot of questions for us.
他会有很多的问题问我们。

68
00:03:55.100 --> 00:04:01.236
Is it really possible for us to take a system of millions upon millions of devices,
我们真的可能用一个连接了无数设备的系统，

69
00:04:01.260 --> 00:04:02.716
to read in their data streams,
通过读取数据流，

70
00:04:02.740 --> 00:04:04.996
to predict their failures and act in advance?
来预测它们的失败并提前行动吗？

71
00:04:04.960 --> 00:04:06.170
Yes.
是的。

72
00:04:06.260 --> 00:04:09.436
Can we build systems that converse with humans in natural language?
我们能建造一个和人类用语言交流的系统吗？

73
00:04:09.460 --> 00:04:10.676
Yes.
能。

74
00:04:10.700 --> 00:04:13.676
Can we build systems that recognize objects, identify emotions,
我们能够打造一个能辨识目标，鉴别情绪，

75
00:04:13.700 --> 00:04:17.076
emote themselves, play games and even read lips?
表现自身情感，打游戏，甚至读唇的系统吗？

76
00:04:17.100 --> 00:04:18.316
Yes.
可以。

77
00:04:18.340 --> 00:04:20.476
Can we build a system that sets goals,
我们可以建造一个能设定目标，

78
00:04:20.500 --> 00:04:24.116
that carries out plans against those goals and learns along the way?
通过各种达成目的的方法来学习的系统吗？

79
00:04:24.140 --> 00:04:25.356
Yes.
也可以。

80
00:04:25.380 --> 00:04:28.716
Can we build systems that have a theory of mind?
我们可以建造一个类似人脑的系统吗？

81
00:04:28.740 --> 00:04:30.236
This we are learning to do.
这是我们正在努力做的。

82
00:04:30.260 --> 00:04:33.740
Can we build systems that have an ethical and moral foundation?
我们可以建造一个有道德和感情基础的系统吗？

83
00:04:34.300 --> 00:04:36.340
This we must learn how to do.
这是我们必须要学习的。

84
00:04:37.180 --> 00:04:38.556
So let's accept for a moment
总而言之，

85
00:04:38.580 --> 00:04:43.636
that it's possible to build such an artificial intelligence for this kind of mission and others.
建造一个类似的用于这类任务的人工智能是可行的。

86
00:04:43.660 --> 00:04:46.196
The next question you must ask yourself is,
另一个你必须扪心自问的问题是，

87
00:04:46.220 --> 00:04:47.676
should we fear it?
我们应该害怕它吗？

88
00:04:47.700 --> 00:04:49.676
Now, every new technology
毕竟，每一种新技术

89
00:04:49.700 --> 00:04:52.596
brings with it some measure of trepidation.
都给我们带来某种程度的不安。

90
00:04:52.620 --> 00:04:54.316
When we first saw cars,
我们第一次看见汽车的时候，

91
00:04:54.340 --> 00:04:58.356
people lamented that we would see the destruction of the family.
人们悲叹我们会看到家庭的毁灭。

92
00:04:58.380 --> 00:05:01.076
When we first saw telephones come in,
我们第一次看见电话的时候，

93
00:05:01.100 --> 00:05:03.996
people were worried it would destroy all civil conversation.
人们担忧这会破坏所有的文明交流。

94
00:05:03.970 --> 00:05:07.930
At a point in time we saw the written word become pervasive,
在某个时间点我们看到书写文字的蔓延，

95
00:05:07.980 --> 00:05:10.476
people thought we would lose our ability to memorize.
人们认为我们会丧失记忆的能力。

96
00:05:10.500 --> 00:05:12.556
These things are all true to a degree,
这些在某个程度上是合理的，

97
00:05:12.580 --> 00:05:14.996
but it's also the case that these technologies
但也正是这些技术

98
00:05:14.980 --> 00:05:20.300
brought to us things that extended the human experience in some profound ways.
给人类的生活在某些方面带来了前所未有的体验。

99
00:05:21.660 --> 00:05:23.940
So let's take this a little further.
我们再稍稍扩展一下。

100
00:05:24.940 --> 00:05:29.676
I do not fear the creation of an AI like this,
我并不畏惧这类人工智能的诞生，

101
00:05:29.700 --> 00:05:33.516
because it will eventually embody some of our values.
因为它最终会融入我们的部分价值观。

102
00:05:33.540 --> 00:05:37.036
Consider this: building a cognitive system is fundamentally different
想想这个：建造一个认知系统

103
00:05:37.060 --> 00:05:40.356
than building a traditional software-intensive system of the past.
与建造一个以传统的软件为主的系统有着本质的不同。

104
00:05:40.380 --> 00:05:42.836
We don't program them. We teach them.
我们不编译它们。我们教导它们。

105
00:05:42.860 --> 00:05:45.516
In order to teach a system how to recognize flowers,
为了教会系统如何识别花朵，

106
00:05:45.540 --> 00:05:48.556
I show it thousands of flowers of the kinds I like.
我给它看了上千种我喜欢的花。

107
00:05:48.580 --> 00:05:50.836
In order to teach a system how to play a game --
为了教会系统怎么打游戏——

108
00:05:50.860 --> 00:05:52.820
Well, I would. You would, too.
当然，我会。你们也会。

109
00:05:54.420 --> 00:05:56.460
I like flowers. Come on.
我喜欢花。这没什么。

110
00:05:57.260 --> 00:06:00.116
To teach a system how to play a game like Go,
为了教会系统如何玩像围棋一样的游戏，

111
00:06:00.140 --> 00:06:02.196
I'd have it play thousands of games of Go,
我玩了许多围棋的游戏，

112
00:06:02.220 --> 00:06:03.876
but in the process I also teach it
但是在这个过程中

113
00:06:03.900 --> 00:06:06.316
how to discern a good game from a bad game.
我也教会它如何分别差游戏和好游戏。

114
00:06:06.340 --> 00:06:10.036
If I want to create an artificially intelligent legal assistant,
如果我想要一个人工智能法律助手，

115
00:06:10.060 --> 00:06:11.836
I will teach it some corpus of law
我会给它一些法律文集，

116
00:06:11.860 --> 00:06:14.716
but at the same time I am fusing with it
但同时我会将怜悯和正义

117
00:06:14.740 --> 00:06:17.620
the sense of mercy and justice that is part of that law.
也是法律的一部分这种观点融入其中。

118
00:06:18.380 --> 00:06:21.356
In scientific terms, this is what we call ground truth,
用一个术语来解释，就是我们所说的真相，

119
00:06:21.380 --> 00:06:23.396
and here's the important point:
而关键在于：

120
00:06:23.420 --> 00:06:24.876
in producing these machines,
为了制造这些机器，

121
00:06:24.900 --> 00:06:28.316
we are therefore teaching them a sense of our values.
我们正教给它们我们的价值观。

122
00:06:28.340 --> 00:06:31.476
To that end, I trust an artificial intelligence
正因如此，我相信一个人工智能

123
00:06:31.500 --> 00:06:35.140
the same, if not more, as a human who is well-trained.
绝不逊于一个经过良好训练的人类。

124
00:06:35.900 --> 00:06:37.116
But, you may ask,
但是，你或许会问，

125
00:06:37.140 --> 00:06:39.756
what about rogue agents,
要是流氓组织，

126
00:06:39.780 --> 00:06:43.116
some well-funded nongovernment organization?
和资金充沛的无政府组织也在利用它们呢？

127
00:06:43.140 --> 00:06:46.956
I do not fear an artificial intelligence in the hand of a lone wolf.
我并不害怕独狼掌控的人工智能。

128
00:06:46.980 --> 00:06:51.516
Clearly, we cannot protect ourselves against all random acts of violence,
很明显，我们无法从随机的暴力行为中保护自己，

129
00:06:51.540 --> 00:06:53.676
but the reality is such a system
但是现实是，制造这样一个系统

130
00:06:53.700 --> 00:06:56.796
requires substantial training and subtle training
超越了个人所拥有资源的极限，

131
00:06:56.820 --> 00:06:59.116
far beyond the resources of an individual.
因为它需要踏实细致的训练和培养。

132
00:06:59.140 --> 00:07:00.356
And furthermore,
还有，

133
00:07:00.380 --> 00:07:03.636
it's far more than just injecting an internet virus to the world,
这远比向世界散播一个网络病毒，

134
00:07:03.660 --> 00:07:06.756
where you push a button, all of a sudden it's in a million places
比如你按下一个按钮，瞬间全世界都被感染，

135
00:07:06.780 --> 00:07:09.236
and laptops start blowing up all over the place.
并且在各处的笔记本电脑中开始爆发来的复杂。

136
00:07:09.260 --> 00:07:12.076
Now, these kinds of substances are much larger,
这类东西正在越来越强大，

137
00:07:12.100 --> 00:07:13.815
and we'll certainly see them coming.
我们必然会看到它们的来临。

138
00:07:14.340 --> 00:07:19.380
Do I fear that such an artificial intelligence might threaten all of humanity?
我会害怕一个有可能威胁所有人类的人工智能吗？

139
00:07:20.100 --> 00:07:24.476
If you look at movies such as "The Matrix," "Metropolis,"
如果你看过《黑客帝国》，《大都会》，

140
00:07:24.500 --> 00:07:27.676
“The Terminator,” shows such as “Westworld,”
《终结者》，或者 《西部世界》这类电视剧，

141
00:07:27.700 --> 00:07:29.836
they all speak of this kind of fear.
它们都在表达这种恐惧。

142
00:07:29.860 --> 00:07:34.156
Indeed, in the book "Superintelligence" by the philosopher Nick Bostrom,
的确，在哲学家Nick Bostrom 写的《超级智能》中，

143
00:07:34.180 --> 00:07:35.716
he picks up on this theme
他选择了这个主题，

144
00:07:35.740 --> 00:07:39.756
and observes that a superintelligence might not only be dangerous,
并且观察到超级智能不仅仅危险，

145
00:07:39.780 --> 00:07:43.636
it could represent an existential threat to all of humanity.
它还对所有人类的存在造成了威胁。

146
00:07:43.660 --> 00:07:45.876
Dr. Bostrom's basic argument
Bostrom博士的基础观点认为，

147
00:07:45.900 --> 00:07:51.916
is that such systems will eventually have such an insatiable thirst for information
这样的系统迟早会产生对信息的无止境渴求，

148
00:07:51.940 --> 00:07:54.836
that they will perhaps learn how to learn
也许它们会开始学习，

149
00:07:54.860 --> 00:07:59.796
and eventually discover that they may have goals that are contrary to human needs.
并且最终发现它们的目的和人类的需求背道而驰。

150
00:07:59.820 --> 00:08:01.676
Dr. Bostrom has a number of followers.
Bostrom博士有一群粉丝。

151
00:08:01.700 --> 00:08:06.020
He is supported by people such as Elon Musk and Stephen Hawking.
Elon Musk和Stephen Hawking也支持他。

152
00:08:06.700 --> 00:08:11.996
With all due respect to these brilliant minds,
虽然要向这些伟大的头脑致以崇高的敬意，

153
00:08:11.960 --> 00:08:14.270
I believe that they are fundamentally wrong.
但我还是相信他们从一开始就错了。

154
00:08:14.300 --> 00:08:17.476
Now, there are a lot of pieces of Dr. Bostrom's argument to unpack,
Bostrom博士的观点有很多地方可以细细体会，

155
00:08:17.500 --> 00:08:19.636
and I don't have time to unpack them all,
但现在我没有时间一一解读，

156
00:08:19.660 --> 00:08:22.356
but very briefly, consider this:
简要而言，请考虑这句话：

157
00:08:22.380 --> 00:08:26.116
super knowing is very different than super doing.
全知并非全能。

158
00:08:26.140 --> 00:08:28.036
HAL was a threat to the Discovery crew
HAL成为了对发现一号成员的威胁，

159
00:08:28.060 --> 00:08:32.476
only insofar as HAL commanded all aspects of the Discovery.
只是因为它控制了发现一号的各个方面。

160
00:08:32.500 --> 00:08:34.996
So it would have to be with a superintelligence.
正因如此它才需要是一个人工智能。

161
00:08:34.990 --> 00:08:37.470
It would have to have dominion over all of our world.
它需要对于我们世界的完全掌控。

162
00:08:37.540 --> 00:08:40.356
This is the stuff of Skynet from the movie “The Terminator”
这就是《终结者》中的天网，

163
00:08:40.380 --> 00:08:42.236
in which we had a superintelligence
一个控制了人们意志，

164
00:08:42.260 --> 00:08:47.516
that commanded human will, that directed every device that was in every corner of the world.
控制了世界各处各个机器的超级智能。

165
00:08:47.540 --> 00:08:48.996
Practically speaking,
说实在的，

166
00:08:48.990 --> 00:08:51.100
it ain't gonna happen.
这完全是杞人忧天。

167
00:08:51.140 --> 00:08:54.196
We are not building AIs that control the weather,
我们不是在制造可以控制天气，

168
00:08:54.220 --> 00:08:55.556
that direct the tides,
引导潮水，

169
00:08:55.580 --> 00:08:58.956
that command us capricious, chaotic humans.
指挥我们这些多变，混乱的人类的人工智能。

170
00:08:58.980 --> 00:09:02.876
And furthermore, if such an artificial intelligence existed,
另外，即使这类人工智能存在，

171
00:09:02.900 --> 00:09:05.836
it would have to compete with human economies,
它需要和人类的经济竞争，

172
00:09:05.860 --> 00:09:08.380
and thereby compete for resources with us.
进而和我们拥有的资源竞争。

173
00:09:09.020 --> 00:09:10.236
And in the end --
最后——

174
00:09:10.260 --> 00:09:11.500
don't tell Siri this --
不要告诉Siri——

175
00:09:12.260 --> 00:09:13.636
we can always unplug them.
我们可以随时拔掉电源。

176
00:09:13.660 --> 00:09:15.780
(Laughter)
（笑声）

177
00:09:17.180 --> 00:09:22.156
We are on an incredible journey of coevolution with our machines.
我们正处于和机器共同演化的奇妙旅途之中。

178
00:09:22.180 --> 00:09:27.236
The humans we are today are not the humans we will be then.
未来的人类将与今天的人类大相径庭。

179
00:09:27.260 --> 00:09:30.396
To worry now about the rise of a superintelligence
当前对人工智能崛起的担忧，

180
00:09:30.420 --> 00:09:33.476
is in many ways a dangerous distraction
从各方面来说都是一个危险的错误指引，

181
00:09:33.500 --> 00:09:35.836
because the rise of computing itself
因为电脑的崛起

182
00:09:35.860 --> 00:09:40.540
brings to us a number of human and societal issues to which we must now attend.
带给了我们许多必须参与的关乎人类和社会的问题。

183
00:09:41.180 --> 00:09:43.996
How shall I best organize society
我应该如何管理社会

184
00:09:43.950 --> 00:09:46.376
when the need for human labor diminishes?
来应对人类劳工需求量的降低？

185
00:09:46.380 --> 00:09:50.196
How can I bring understanding and education throughout the globe
我应该怎样在进行全球化交流和教育的同时，

186
00:09:50.220 --> 00:09:51.996
and still respect our differences?
依旧尊重彼此的不同？

187
00:09:51.990 --> 00:09:56.356
How might I extend and enhance human life through cognitive healthcare?
我应该如何通过可认知医疗来延长并强化人类的生命？

188
00:09:56.300 --> 00:10:00.940
How might I use computing to help take us to the stars?
我应该如何通过计算机来帮助我们前往其他星球？

189
00:10:01.580 --> 00:10:03.620
And that's the exciting thing.
这些都很令人兴奋。

190
00:10:04.220 --> 00:10:08.116
The opportunities to use computing to advance the human experience
通过计算机来升级人类体验的机会

191
00:10:08.140 --> 00:10:09.556
are within our reach,
就在我们手中，

192
00:10:09.580 --> 00:10:11.436
here and now,
就在此时此刻，

193
00:10:11.460 --> 00:10:13.140
and we are just beginning.
我们的旅途才刚刚开始。

194
00:10:14.100 --> 00:10:15.316
Thank you very much.
谢谢大家。

195
00:10:15.340 --> 00:10:19.626
(Applause)
（掌声）